{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Photoredox_Yield_Iterative: GGH Method Benchmark\n",
    "\n",
    "Apply the Gradient-Guided Hypothesis (GGH) iterative method to the Photoredox Yield dataset.\n",
    "\n",
    "**Dataset**: Photoredox yield prediction (Merck 2021, 1649 reactions)\n",
    "- 4 hypothesis classes for photocatalyst moles\n",
    "- Target: UPLCMS yield\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "data_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: ../data/photoredox_yield/photo_redox_merck2021_1649reactions.csv\n",
      "Input variables: ['aryl_halides', 'photocalysts', 'piperidines_moles']\n",
      "Target variables: ['uplcms']\n",
      "Missing variables: ['photocalysts_moles']\n",
      "Hypothesis values: [0.02, 0.05, 0.5, 5.0]\n",
      "Number of hypothesis classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Data configuration for Photoredox Yield dataset\n",
    "data_path = '../data/photoredox_yield/photo_redox_merck2021_1649reactions.csv'\n",
    "results_path = \"../saved_results/Photoredox Yield Iterative\"\n",
    "\n",
    "# Input, target, and missing variables\n",
    "inpt_vars = ['aryl_halides', 'photocalysts', 'piperidines_moles']\n",
    "target_vars = ['uplcms']\n",
    "miss_vars = ['photocalysts_moles']\n",
    "\n",
    "# Hypothesis values for photocatalyst moles (4 classes)\n",
    "hypothesis = [[0.02, 0.05, 0.50, 5.0]]\n",
    "\n",
    "# Simulation parameters\n",
    "partial_perc = 0.30  # 10% partial data\n",
    "rand_state = 42\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "\n",
    "# Create results directory if needed\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Input variables: {inpt_vars}\")\n",
    "print(f\"Target variables: {target_vars}\")\n",
    "print(f\"Missing variables: {miss_vars}\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")\n",
    "print(f\"Number of hypothesis classes: {len(hypothesis[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_defs_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wine_cell_4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wine_cell_5",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "wine_cell_6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wine_cell_7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "wine_cell_8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "wine_cell_9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypothesisDataset defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that includes global IDs for tracking.\"\"\"\n",
    "    def __init__(self, DO):\n",
    "        # Input features = inpt_vars + hypothesis column\n",
    "        input_cols = DO.inpt_vars + [f'{DO.miss_vars[0]}_hypothesis']\n",
    "        self.inputs = torch.tensor(\n",
    "            DO.df_train_hypothesis[input_cols].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.targets = torch.tensor(\n",
    "            DO.df_train_hypothesis[DO.target_vars].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.global_ids = torch.arange(len(self.inputs))\n",
    "        self.input_cols = input_cols\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.global_ids[idx]\n",
    "\n",
    "print(\"HypothesisDataset defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wine_cell_10",
   "metadata": {},
   "source": [
    "## Adaptive Context Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wine_cell_11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wine_cell_12",
   "metadata": {},
   "source": [
    "## Combined Loss + Gradient Scoring (for Iteration 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wine_cell_13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scoring utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_combined_score(loss, gradient, features, class_id, anchor_data, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Combine loss and gradient signals for scoring.\n",
    "    \n",
    "    For a truth-biased model:\n",
    "    - Lower loss = more likely correct (aligned with truth)\n",
    "    - Gradient similarity to correct anchor = more likely correct\n",
    "    \n",
    "    Final score = (1 - loss_weight) * gradient_score + loss_weight * (-normalized_loss)\n",
    "    Higher score = more likely correct\n",
    "    \"\"\"\n",
    "    # Gradient score (same as adaptive)\n",
    "    grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "    \n",
    "    # Loss score: lower loss = higher score\n",
    "    # We'll normalize this later when we have all losses\n",
    "    loss_score = -loss  # Negative because lower loss is better\n",
    "    \n",
    "    return {\n",
    "        'grad_score': grad_score,\n",
    "        'loss_score': loss_score,\n",
    "        'raw_loss': loss\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_and_combine_scores(all_scores, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Normalize loss scores per class and combine with gradient scores.\n",
    "    \n",
    "    Returns combined scores where higher = more likely correct.\n",
    "    \"\"\"\n",
    "    # Group by class\n",
    "    class_losses = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        if class_id not in class_losses:\n",
    "            class_losses[class_id] = []\n",
    "        class_losses[class_id].append(scores['raw_loss'])\n",
    "    \n",
    "    # Compute per-class mean and std for loss normalization\n",
    "    class_stats = {}\n",
    "    for class_id, losses in class_losses.items():\n",
    "        class_stats[class_id] = {\n",
    "            'mean': np.mean(losses),\n",
    "            'std': np.std(losses) + 1e-8\n",
    "        }\n",
    "    \n",
    "    # Normalize and combine\n",
    "    combined_scores = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        stats = class_stats[class_id]\n",
    "        \n",
    "        # Z-score normalize loss (then negate: lower loss = higher score)\n",
    "        normalized_loss_score = -(scores['raw_loss'] - stats['mean']) / stats['std']\n",
    "        \n",
    "        # Combine: weighted average of gradient and loss scores\n",
    "        combined = (1 - loss_weight) * scores['grad_score'] + loss_weight * normalized_loss_score\n",
    "        \n",
    "        combined_scores[sample_idx] = {\n",
    "            'gid': gid,\n",
    "            'combined_score': combined,\n",
    "            'grad_score': scores['grad_score'],\n",
    "            'loss_score': normalized_loss_score,\n",
    "            'raw_loss': scores['raw_loss'],\n",
    "            'class_id': class_id,\n",
    "            'is_correct': scores['is_correct']\n",
    "        }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "print(\"Combined scoring utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wine_cell_14",
   "metadata": {},
   "source": [
    "## Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wine_cell_15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def analyze_threshold_precision(all_selections, title=\"Precision Analysis\", verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze precision at different thresholds.\n",
    "    \n",
    "    all_selections: list of (score, is_correct, sample_idx) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    if not all_selections:\n",
    "        print(\"No selections to analyze\")\n",
    "        return None, None\n",
    "    \n",
    "    # Compute precision at different percentiles\n",
    "    results = []\n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        n_include = max(1, int(len(all_selections) * pct / 100))\n",
    "        top_selections = all_selections[:n_include]\n",
    "        n_correct = sum(1 for _, is_correct, _ in top_selections if is_correct)\n",
    "        precision = n_correct / n_include\n",
    "        results.append({\n",
    "            'percentile': pct,\n",
    "            'n_samples': n_include,\n",
    "            'n_correct': n_correct,\n",
    "            'precision': precision\n",
    "        })\n",
    "    \n",
    "    # Compute precision in score bins\n",
    "    scores = [s[0] for s in all_selections]\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    n_bins = 10\n",
    "    bin_results = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_low = min_score + (max_score - min_score) * i / n_bins\n",
    "        bin_high = min_score + (max_score - min_score) * (i + 1) / n_bins\n",
    "        bin_selections = [(s, c) for s, c, _ in all_selections if bin_low <= s < bin_high]\n",
    "        if bin_selections:\n",
    "            bin_correct = sum(1 for _, c in bin_selections if c)\n",
    "            bin_results.append({\n",
    "                'bin': f'{bin_low:.2f}-{bin_high:.2f}',\n",
    "                'n_samples': len(bin_selections),\n",
    "                'precision': bin_correct / len(bin_selections)\n",
    "            })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(title)\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\nPrecision by Top Percentile (highest scores first):\")\n",
    "        print(\"-\" * 50)\n",
    "        for r in results:\n",
    "            print(f\"Top {r['percentile']:>3}%: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "        \n",
    "        if bin_results:\n",
    "            print(\"\\nPrecision by Score Bin:\")\n",
    "            print(\"-\" * 50)\n",
    "            for r in bin_results:\n",
    "                print(f\"Score {r['bin']}: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "    \n",
    "    return results, bin_results\n",
    "\n",
    "\n",
    "def select_hypotheses_adaptive(trainer, DO, anchor_data=None):\n",
    "    \"\"\"\n",
    "    Select best hypothesis per sample using adaptive context.\n",
    "    Returns list of (score, is_correct, sample_idx) sorted by score descending.\n",
    "    \"\"\"\n",
    "    if anchor_data is None:\n",
    "        anchor_data = compute_anchor_data(trainer, DO)\n",
    "    \n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    partial_sample_indices = anchor_data['partial_sample_indices']\n",
    "    blacklisted_gids = anchor_data['blacklisted_gids']\n",
    "    \n",
    "    all_selections = []\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_is_correct = False\n",
    "        best_gid = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return all_selections, anchor_data\n",
    "\n",
    "print(\"Analysis utilities loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "anchor_data_with_loss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions with loss context defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data_with_loss(analysis, DO, loss_data):\n",
    "    \"\"\"\n",
    "    Compute anchors that include loss in the enriched representation.\n",
    "    \n",
    "    For each class:\n",
    "    - Compute gradient-only anchors (same as before)\n",
    "    - Compute enriched anchors: gradient + features + normalized_loss\n",
    "    - Decide which to use based on gradient anchor similarity\n",
    "    \"\"\"\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Storage\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    feature_norm_params = {}\n",
    "    loss_norm_params = {}\n",
    "    \n",
    "    # Get gradient scale for normalization\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect data for correct hypotheses\n",
    "        correct_grads, correct_features, correct_losses = [], [], []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                correct_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                correct_losses.append(loss_data[gid])\n",
    "        \n",
    "        # Collect data for incorrect hypotheses\n",
    "        incorrect_grads, incorrect_features, incorrect_losses = [], [], []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                incorrect_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                incorrect_losses.append(loss_data[gid])\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "        \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Convert to arrays\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        correct_losses = np.array(correct_losses, dtype=np.float64)\n",
    "        incorrect_losses = np.array(incorrect_losses, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Normalize losses\n",
    "        all_losses = np.concatenate([correct_losses, incorrect_losses])\n",
    "        loss_mean = np.mean(all_losses)\n",
    "        loss_std = np.std(all_losses) + 1e-8\n",
    "        loss_norm_params[class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "        \n",
    "        # Negate loss: lower loss = higher value (more likely correct)\n",
    "        correct_losses_norm = -((correct_losses - loss_mean) / loss_std) * grad_scale\n",
    "        incorrect_losses_norm = -((incorrect_losses - loss_mean) / loss_std) * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm, correct_losses_norm.reshape(-1, 1)])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm, incorrect_losses_norm.reshape(-1, 1)])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'loss_norm_params': loss_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method with loss included in enriched context.\n",
    "    \n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features + loss) for classes with poor gradient separation\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Normalize features\n",
    "        feat_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if feat_params:\n",
    "            features_norm = (features - feat_params['mean']) / feat_params['std'] * feat_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        \n",
    "        # Normalize loss (negated: lower loss = higher value)\n",
    "        loss_params = anchor_data['loss_norm_params'].get(class_id)\n",
    "        if loss_params:\n",
    "            loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "        else:\n",
    "            loss_norm = -loss\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "print(\"Functions with loss context defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "data_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHOTOREDOX YIELD DATA SUMMARY\n",
      "============================================================\n",
      "Dataset: ../data/photoredox_yield/photo_redox_merck2021_1649reactions.csv\n",
      "Total samples: 1187\n",
      "Hypotheses per sample: 4\n",
      "Total hypotheses: 4748\n",
      "Partial data: 356 hypotheses (7.5%)\n",
      "Missing variable: ['photocalysts_moles']\n",
      "Hypothesis values: [0.02, 0.05, 0.5, 5.0]\n",
      "Lack partial coverage: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataOperator and verify data loads correctly\n",
    "DO = DataOperator(\n",
    "    data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "    partial_perc, rand_state, device='cpu',\n",
    "    data_split={\"train\": 0.72, \"val\": 0.88}\n",
    ")\n",
    "DO.problem_type = 'regression'\n",
    "\n",
    "# Print data summary\n",
    "hyp_per_sample = DO.num_hyp_comb\n",
    "n_total_hyp = len(DO.df_train_hypothesis)\n",
    "n_samples = n_total_hyp // hyp_per_sample\n",
    "\n",
    "partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "    (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "    (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "].index.tolist())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHOTOREDOX YIELD DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {data_path}\")\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Hypotheses per sample: {hyp_per_sample}\")\n",
    "print(f\"Total hypotheses: {n_total_hyp}\")\n",
    "print(f\"Partial data: {len(partial_correct_gids)} hypotheses ({len(partial_correct_gids)/n_total_hyp*100:.1f}%)\")\n",
    "print(f\"Missing variable: {miss_vars}\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")\n",
    "print(f\"Lack partial coverage: {DO.lack_partial_coverage}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "comparison_study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON STUDY: GGH vs Partial-Only\n",
      "================================================================================\n",
      "Data splits: 72% train pool, 16% val, 12% test\n",
      "  - Partial trains on: ~2.5% (partial data only, 100% correct)\n",
      "  - GGH trains on: ~2.5% partial + ~15% selected (~73% correct)\n",
      "Number of runs: 15\n",
      "Training epochs: 200 (with validation-based epoch selection)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 28.9%, class dist: {0: 0, 1: 61, 2: 116, 3: 72}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 24.9%, class dist: {0: 0, 1: 153, 2: 0, 3: 96}\n",
      "GGH selection: 249 hypotheses at 24.9% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=8, val_loss=0.0209, test_loss=0.0239, test_mae=0.0759, R2=0.5785\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0220, test_loss=0.0263, test_mae=0.1085, R2=0.5359\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=+0.0326, R2=+0.0426\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 31.3%, class dist: {0: 0, 1: 7, 2: 146, 3: 96}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 20.5%, class dist: {0: 72, 1: 80, 2: 30, 3: 67}\n",
      "GGH selection: 249 hypotheses at 20.5% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=17, val_loss=0.0158, test_loss=0.0171, test_mae=0.0733, R2=0.7290\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=54, val_loss=0.0152, test_loss=0.0212, test_mae=0.0777, R2=0.6631\n",
      "\n",
      ">>> Improvement: Loss=+0.0042, MAE=+0.0044, R2=+0.0659\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 30.9%, class dist: {0: 0, 1: 125, 2: 0, 3: 124}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 20.5%, class dist: {0: 61, 1: 42, 2: 31, 3: 115}\n",
      "GGH selection: 249 hypotheses at 20.5% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=51, val_loss=0.0206, test_loss=0.0226, test_mae=0.0859, R2=0.6266\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0179, test_loss=0.0236, test_mae=0.0898, R2=0.6111\n",
      "\n",
      ">>> Improvement: Loss=+0.0009, MAE=+0.0038, R2=+0.0154\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 26.9%, class dist: {0: 10, 1: 15, 2: 9, 3: 215}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 21.7%, class dist: {0: 84, 1: 51, 2: 73, 3: 41}\n",
      "GGH selection: 249 hypotheses at 21.7% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=11, val_loss=0.0152, test_loss=0.0229, test_mae=0.0842, R2=0.5943\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=16, val_loss=0.0161, test_loss=0.0253, test_mae=0.0893, R2=0.5510\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=+0.0051, R2=+0.0433\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 27.7%, class dist: {0: 85, 1: 0, 2: 71, 3: 93}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 20.1%, class dist: {0: 50, 1: 102, 2: 67, 3: 30}\n",
      "GGH selection: 249 hypotheses at 20.1% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=12, val_loss=0.0169, test_loss=0.0212, test_mae=0.0745, R2=0.5689\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0160, test_loss=0.0202, test_mae=0.0782, R2=0.5903\n",
      "\n",
      ">>> Improvement: Loss=-0.0011, MAE=+0.0037, R2=-0.0214\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 27.7%, class dist: {0: 17, 1: 12, 2: 5, 3: 215}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 20.5%, class dist: {0: 72, 1: 100, 2: 12, 3: 65}\n",
      "GGH selection: 249 hypotheses at 20.5% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=14, val_loss=0.0200, test_loss=0.0200, test_mae=0.0781, R2=0.5908\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=14, val_loss=0.0208, test_loss=0.0222, test_mae=0.0856, R2=0.5466\n",
      "\n",
      ">>> Improvement: Loss=+0.0022, MAE=+0.0075, R2=+0.0442\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 26.5%, class dist: {0: 83, 1: 53, 2: 0, 3: 113}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 23.7%, class dist: {0: 7, 1: 36, 2: 6, 3: 200}\n",
      "GGH selection: 249 hypotheses at 23.7% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=18, val_loss=0.0190, test_loss=0.0255, test_mae=0.0879, R2=0.5606\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=40, val_loss=0.0186, test_loss=0.0250, test_mae=0.0936, R2=0.5681\n",
      "\n",
      ">>> Improvement: Loss=-0.0004, MAE=+0.0057, R2=-0.0075\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=742)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 21.7%, class dist: {0: 6, 1: 214, 2: 20, 3: 9}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 27.3%, class dist: {0: 42, 1: 49, 2: 57, 3: 101}\n",
      "GGH selection: 249 hypotheses at 27.3% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=32, val_loss=0.0217, test_loss=0.0122, test_mae=0.0569, R2=0.7168\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=48, val_loss=0.0289, test_loss=0.0151, test_mae=0.0597, R2=0.6504\n",
      "\n",
      ">>> Improvement: Loss=+0.0029, MAE=+0.0029, R2=+0.0663\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=842)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 28.1%, class dist: {0: 39, 1: 34, 2: 80, 3: 96}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 22.9%, class dist: {0: 48, 1: 59, 2: 79, 3: 63}\n",
      "GGH selection: 249 hypotheses at 22.9% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0132, test_loss=0.0142, test_mae=0.0670, R2=0.6120\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=22, val_loss=0.0142, test_loss=0.0188, test_mae=0.0684, R2=0.4872\n",
      "\n",
      ">>> Improvement: Loss=+0.0046, MAE=+0.0014, R2=+0.1248\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=942)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 33.7%, class dist: {0: 23, 1: 80, 2: 92, 3: 54}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 24.5%, class dist: {0: 244, 1: 5, 2: 0, 3: 0}\n",
      "GGH selection: 249 hypotheses at 24.5% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=5, val_loss=0.0144, test_loss=0.0177, test_mae=0.0726, R2=0.6173\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=6, val_loss=0.0180, test_loss=0.0197, test_mae=0.0798, R2=0.5738\n",
      "\n",
      ">>> Improvement: Loss=+0.0020, MAE=+0.0071, R2=+0.0435\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=1042)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 26.5%, class dist: {0: 51, 1: 107, 2: 27, 3: 64}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 22.5%, class dist: {0: 46, 1: 50, 2: 30, 3: 123}\n",
      "GGH selection: 249 hypotheses at 22.5% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=15, val_loss=0.0168, test_loss=0.0152, test_mae=0.0665, R2=0.6325\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=18, val_loss=0.0167, test_loss=0.0167, test_mae=0.0753, R2=0.5963\n",
      "\n",
      ">>> Improvement: Loss=+0.0015, MAE=+0.0088, R2=+0.0362\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=1142)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 31.7%, class dist: {0: 3, 1: 7, 2: 112, 3: 127}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 19.7%, class dist: {0: 7, 1: 112, 2: 55, 3: 75}\n",
      "GGH selection: 249 hypotheses at 19.7% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=14, val_loss=0.0204, test_loss=0.0122, test_mae=0.0571, R2=0.6715\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=13, val_loss=0.0211, test_loss=0.0161, test_mae=0.0726, R2=0.5661\n",
      "\n",
      ">>> Improvement: Loss=+0.0039, MAE=+0.0155, R2=+0.1054\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=1242)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 33.3%, class dist: {0: 1, 1: 63, 2: 98, 3: 87}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 21.7%, class dist: {0: 87, 1: 69, 2: 26, 3: 67}\n",
      "GGH selection: 249 hypotheses at 21.7% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=65, val_loss=0.0151, test_loss=0.0168, test_mae=0.0681, R2=0.6738\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0200, test_loss=0.0149, test_mae=0.0692, R2=0.7118\n",
      "\n",
      ">>> Improvement: Loss=-0.0020, MAE=+0.0011, R2=-0.0380\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=1342)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 30.9%, class dist: {0: 91, 1: 112, 2: 0, 3: 46}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 25.3%, class dist: {0: 17, 1: 14, 2: 104, 3: 114}\n",
      "GGH selection: 249 hypotheses at 25.3% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=8, val_loss=0.0185, test_loss=0.0204, test_mae=0.0700, R2=0.5982\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=13, val_loss=0.0224, test_loss=0.0197, test_mae=0.0864, R2=0.6126\n",
      "\n",
      ">>> Improvement: Loss=-0.0007, MAE=+0.0164, R2=-0.0144\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=1442)\n",
      "============================================================\n",
      "Total: 1187 samples, 4748 hypotheses\n",
      "Partial: 356 hypotheses (7.5%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1424 hypotheses from 356 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|| 5/5 [00:03<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2328 hypotheses from 582 samples\n",
      "  Iter1 top 30% precision: 26.5%, class dist: {0: 60, 1: 92, 2: 12, 3: 85}\n",
      "  Iter3 expansion: 582 scored, top 249 precision: 20.9%, class dist: {0: 94, 1: 131, 2: 19, 3: 5}\n",
      "GGH selection: 249 hypotheses at 20.9% precision\n",
      "GGH trains on: 605 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=128, val_loss=0.0138, test_loss=0.0178, test_mae=0.0755, R2=0.6169\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 356 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=25, val_loss=0.0130, test_loss=0.0202, test_mae=0.0795, R2=0.5644\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=+0.0040, R2=+0.0525\n",
      "\n",
      "================================================================================\n",
      "COMPARISON STUDY RESULTS\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   GGH Prec   GGH Loss     Partial Loss    Loss     GGH R2     Part R2     R2      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     24.9      % 0.0239       0.0263            +0.0024 0.5785     0.5359        +0.0426\n",
      "2     20.5      % 0.0171       0.0212            +0.0042 0.7290     0.6631        +0.0659\n",
      "3     20.5      % 0.0226       0.0236            +0.0009 0.6266     0.6111        +0.0154\n",
      "4     21.7      % 0.0229       0.0253            +0.0024 0.5943     0.5510        +0.0433\n",
      "5     20.1      % 0.0212       0.0202            -0.0011 0.5689     0.5903        -0.0214\n",
      "6     20.5      % 0.0200       0.0222            +0.0022 0.5908     0.5466        +0.0442\n",
      "7     23.7      % 0.0255       0.0250            -0.0004 0.5606     0.5681        -0.0075\n",
      "8     27.3      % 0.0122       0.0151            +0.0029 0.7168     0.6504        +0.0663\n",
      "9     22.9      % 0.0142       0.0188            +0.0046 0.6120     0.4872        +0.1248\n",
      "10    24.5      % 0.0177       0.0197            +0.0020 0.6173     0.5738        +0.0435\n",
      "11    22.5      % 0.0152       0.0167            +0.0015 0.6325     0.5963        +0.0362\n",
      "12    19.7      % 0.0122       0.0161            +0.0039 0.6715     0.5661        +0.1054\n",
      "13    21.7      % 0.0168       0.0149            -0.0020 0.6738     0.7118        -0.0380\n",
      "14    25.3      % 0.0204       0.0197            -0.0007 0.5982     0.6126        -0.0144\n",
      "15    20.9      % 0.0178       0.0202            +0.0024 0.6169     0.5644        +0.0525\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "GGH Selection Precision: 22.4%  2.2%\n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH:     0.0187  0.0040\n",
      "  Partial: 0.0203  0.0036\n",
      "\n",
      "Test MAE:\n",
      "  GGH:     0.0729  0.0089\n",
      "  Partial: 0.0809  0.0115\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH:     0.6258  0.0493\n",
      "  Partial: 0.5886  0.0539\n",
      "\n",
      "Statistical Tests (paired t-test):\n",
      "  Loss: t=-3.301, p=0.0052 *\n",
      "  MAE:  t=-3.801, p=0.0019 *\n",
      "  R2:   t=3.192, p=0.0065 *\n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (Loss): 11/15 (73.3%)\n",
      "  GGH wins (MAE):  15/15 (100.0%)\n",
      "  GGH wins (R2):   11/15 (73.3%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "GGH significantly OUTPERFORMS Partial-only (p=0.0052)\n",
      "Average improvement: 0.0017 MSE\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON STUDY: GGH vs Partial-Only (15 Random States)\n",
    "# =============================================================================\n",
    "# Compare:\n",
    "#   - GGH: Single-pass pruning (~73% precision) + partial (~2.5%)\n",
    "#   - Partial: Partial data only (~2.5%)\n",
    "# Both: 200 epochs, validation-based epoch selection, test evaluation\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train model with validation-based epoch selection.\n",
    "    Returns best model state, best epoch, and validation losses.\n",
    "    \"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, val_losses, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        # Calculate R2 score\n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def run_ggh_single_pass(DO, rand_state, iter1_epochs=60, iter2_epochs=30, top_percentile=30):\n",
    "    \"\"\"\n",
    "    Run GGH iterative method to get high-precision selection.\n",
    "    \n",
    "    Iteration 1: Train unbiased model (60 epochs, track last 5) -> top 30% has ~68% precision\n",
    "    Iteration 2: Train biased model on top 30% + partial (30 epochs)\n",
    "    Iteration 3: Score REMAINING 70% with biased model -> ~73%+ precision\n",
    "    \n",
    "    Returns selected_gids, precision, and partial_correct_gids.\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    # Get data parameters\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared_features = len(DO.inpt_vars)\n",
    "    n_hypothesis_features = len(DO.miss_vars)\n",
    "    output_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Get blacklisted gids (partial incorrect)\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    # Create dataloader with global_ids\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Train unbiased model (60 epochs, track last 5) ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=0.01)\n",
    "    \n",
    "    # Train without tracking for first iter1_epochs - 5 epochs\n",
    "    for epoch in range(iter1_epochs - 5):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # Track last 5 epochs for analysis\n",
    "    for epoch in range(iter1_epochs - 5, iter1_epochs):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # === Get top 30% using Adaptive Context Selection ===\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    all_selections, _ = select_hypotheses_adaptive(trainer_unbiased, DO, anchor_data)\n",
    "    \n",
    "    # Sort and get top 30%\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * top_percentile / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    \n",
    "    # Track sample_to_gid for all samples\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # Calculate iter1 precision on top 30%\n",
    "    iter1_correct = sum(1 for s in top_selections if s[1])\n",
    "    iter1_precision = iter1_correct / len(top_selections) * 100 if top_selections else 0\n",
    "    \n",
    "    # Track iter1 class distribution\n",
    "    iter1_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_selections:\n",
    "        gid = s[3]\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        iter1_class_counts[class_id] += 1\n",
    "    \n",
    "    # === ITERATION 2: Train biased model on top 30% + partial ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, partial_weight=2.0, lr=0.01)\n",
    "    \n",
    "    for epoch in range(iter2_epochs):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # === ITERATION 3: Score REMAINING 70% with biased model ===\n",
    "    all_sample_indices = set(range(n_samples))\n",
    "    remaining_sample_indices = all_sample_indices - top_sample_indices - partial_sample_indices\n",
    "    \n",
    "    # First, score partial data to build anchors\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=5)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Build loss data for partial samples\n",
    "    partial_loss_data = {gid: partial_analysis[gid]['avg_loss'] \n",
    "                        for gid in partial_analysis if partial_analysis[gid]['avg_loss'] is not None}\n",
    "    \n",
    "    # Compute anchors from partial data\n",
    "    anchor_data_biased = compute_anchor_data_with_loss(partial_analysis, DO, partial_loss_data)\n",
    "    \n",
    "    # Score REMAINING 70%\n",
    "    scorer = RemainingDataScorer(DO, model_biased, remaining_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=5)\n",
    "    analysis = scorer.get_analysis()\n",
    "    \n",
    "    # Score each hypothesis in remaining samples and select best per sample\n",
    "    remaining_scored = []\n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        start_gid = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_gid = None\n",
    "        best_is_correct = False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start_gid + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            loss = analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, DO.inpt_vars].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_adaptive_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            remaining_scored.append({\n",
    "                'sample_idx': sample_idx,\n",
    "                'gid': best_gid,\n",
    "                'score': best_score,\n",
    "                'is_correct': best_is_correct\n",
    "            })\n",
    "    \n",
    "    # Sort remaining by score and take top selections\n",
    "    remaining_scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Take top 30% of remaining (or same number as iter1 top selection)\n",
    "    n_take_from_remaining = min(len(remaining_scored), n_top)\n",
    "    top_remaining = remaining_scored[:n_take_from_remaining]\n",
    "    \n",
    "    # Calculate precision on top of remaining\n",
    "    iter3_correct = sum(1 for s in top_remaining if s['is_correct'])\n",
    "    iter3_precision = iter3_correct / len(top_remaining) * 100 if top_remaining else 0\n",
    "    \n",
    "    # Final selection: top 30% from iter1 + top from iter3\n",
    "    # For training, we use iter1 top 30% (which has ~68% precision) \n",
    "    # OR use iter3 top selections (which has ~73%+ precision)\n",
    "    # Based on original method, we use iter3 selections for the final training\n",
    "    selected_gids = set(s['gid'] for s in top_remaining)\n",
    "    \n",
    "    # Track iter3 class distribution\n",
    "    iter3_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_remaining:\n",
    "        class_id = DO.df_train_hypothesis.iloc[s['gid']]['hyp_class_id']\n",
    "        iter3_class_counts[class_id] += 1\n",
    "    \n",
    "    # Report both precisions\n",
    "    print(f\"  Iter1 top {top_percentile}% precision: {iter1_precision:.1f}%, class dist: {iter1_class_counts}\")\n",
    "    print(f\"  Iter3 expansion: {len(remaining_scored)} scored, top {n_take_from_remaining} precision: {iter3_precision:.1f}%, class dist: {iter3_class_counts}\")\n",
    "    \n",
    "    return selected_gids, iter3_precision, partial_correct_gids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON STUDY: GGH vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Data splits: 72% train pool, 16% val, 12% test\")\n",
    "print(f\"  - Partial trains on: ~2.5% (partial data only, 100% correct)\")\n",
    "print(f\"  - GGH trains on: ~2.5% partial + ~15% selected (~73% correct)\")\n",
    "print(f\"Number of runs: 15\")\n",
    "print(f\"Training epochs: 200 (with validation-based epoch selection)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_runs = 15\n",
    "n_epochs = 200\n",
    "results = []\n",
    "\n",
    "for run_idx in range(n_runs):\n",
    "    run_rand_state = 42 + run_idx * 100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{n_runs} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # === Setup DataOperator with proper splits ===\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get dimensions\n",
    "    hyp_per_sample = DO_run.num_hyp_comb\n",
    "    n_shared_features = len(DO_run.inpt_vars)\n",
    "    n_hypothesis_features = len(DO_run.miss_vars)\n",
    "    output_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # Get partial gids\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_total_hyp = len(DO_run.df_train_hypothesis)\n",
    "    n_samples = n_total_hyp // hyp_per_sample\n",
    "    print(f\"Total: {n_samples} samples, {n_total_hyp} hypotheses\")\n",
    "    print(f\"Partial: {len(partial_gids)} hypotheses ({len(partial_gids)/n_total_hyp*100:.1f}%)\")\n",
    "    \n",
    "    # === Method 1: GGH ===\n",
    "    print(\"\\n--- GGH Method ---\")\n",
    "    ggh_selected_gids, ggh_precision, _ = run_ggh_single_pass(DO_run, run_rand_state)\n",
    "    print(f\"GGH selection: {len(ggh_selected_gids)} hypotheses at {ggh_precision:.1f}% precision\")\n",
    "    print(f\"GGH trains on: {len(ggh_selected_gids) + len(partial_gids)} hypotheses (selection + partial)\")\n",
    "    \n",
    "    # Train GGH model\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(\"Training GGH model (200 epochs)...\")\n",
    "    model_ggh, ggh_best_epoch, ggh_val_losses, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=2.0, lr=0.01, n_epochs=n_epochs\n",
    "    )\n",
    "    \n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Method 2: Partial Only ===\n",
    "    print(\"\\n--- Partial Only ---\")\n",
    "    print(f\"Partial trains on: {len(partial_gids)} hypotheses (partial only)\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(\"Training Partial model (200 epochs)...\")\n",
    "    model_partial, partial_best_epoch, partial_val_losses, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(),  # No GGH selections, only partial\n",
    "        partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=0.01, n_epochs=n_epochs\n",
    "    )\n",
    "    \n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # === Record results ===\n",
    "    results.append({\n",
    "        'run': run_idx + 1,\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_n_selected': len(ggh_selected_gids),\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'ggh_val_loss': ggh_best_val_loss,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'partial_val_loss': partial_best_val_loss,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON STUDY RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'GGH Prec':<10} {'GGH Loss':<12} {'Partial Loss':<14} {' Loss':<10} {'GGH R2':<10} {'Part R2':<10} {' R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for r in results:\n",
    "    print(f\"{r['run']:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<14.4f} {r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_maes = [r['ggh_test_mae'] for r in results]\n",
    "partial_maes = [r['partial_test_mae'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\nGGH Selection Precision: {np.mean(ggh_precisions):.1f}%  {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f}  {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f}  {np.std(partial_losses):.4f}\")\n",
    "\n",
    "print(f\"\\nTest MAE:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_maes):.4f}  {np.std(ggh_maes):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_maes):.4f}  {np.std(partial_maes):.4f}\")\n",
    "\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f}  {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f}  {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests (paired t-test)\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_mae, p_value_mae = stats.ttest_rel(ggh_maes, partial_maes)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  MAE:  t={t_stat_mae:.3f}, p={p_value_mae:.4f} {'*' if p_value_mae < 0.05 else ''}\")\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_mae = sum(1 for r in results if r['ggh_test_mae'] < r['partial_test_mae'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{n_runs} ({n_ggh_wins_loss/n_runs*100:.1f}%)\")\n",
    "print(f\"  GGH wins (MAE):  {n_ggh_wins_mae}/{n_runs} ({n_ggh_wins_mae/n_runs*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{n_runs} ({n_ggh_wins_r2/n_runs*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(n_runs)\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(n_runs)])\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax2 = axes[1]\n",
    "bp = ax2.boxplot([ggh_losses, partial_losses], labels=['GGH', 'Partial'])\n",
    "ax2.set_ylabel('Test Loss (MSE)')\n",
    "ax2.set_title('Test Loss Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: GGH Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, n_runs+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('GGH Selection Precision (%)')\n",
    "ax3.set_title('GGH Hypothesis Precision')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/comparison_study_15runs.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement = np.mean([r['improvement_loss'] for r in results])\n",
    "if avg_improvement > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH significantly OUTPERFORMS Partial-only (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average improvement: {avg_improvement:.4f} MSE\")\n",
    "elif avg_improvement < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH (p={p_value_loss:.4f})\")\n",
    "    print(f\"GGH is worse by: {-avg_improvement:.4f} MSE\")\n",
    "else:\n",
    "    print(f\"No significant difference between methods (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average difference: {avg_improvement:+.4f} MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6aa3da-c5cc-4345-b3c8-2a8e2dfafcee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
