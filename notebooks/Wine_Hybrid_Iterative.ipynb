{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine_Hybrid_Iterative: Iterative Truth-Biased Training\n",
    "\n",
    "This notebook implements an iterative approach to improve hypothesis selection precision:\n",
    "\n",
    "**Iteration 1: Unbiased Signal Extraction (same as Wine_Hybrid Phase 1)**\n",
    "- Train on ALL hypotheses equally (no selection)\n",
    "- Use Adaptive Context Selection to score hypotheses\n",
    "- Top 30% has ~68% precision (vs 33% random baseline)\n",
    "\n",
    "**Iteration 2: Biased Training**\n",
    "- Train a NEW model on:\n",
    "  - Top 30% highest-scoring samples from Iteration 1\n",
    "  - Partial data with upweighting (~25% of effective training)\n",
    "- This creates a \"truth-biased\" model (trained on >70% correct data)\n",
    "\n",
    "**Iteration 3: Apply Biased Model to Remaining Data**\n",
    "- Keep Iteration 2 model frozen\n",
    "- Compute gradients AND losses on remaining 70% of data\n",
    "- Theory: Truth-biased model should produce better separation signals\n",
    "\n",
    "**Key Insight**: Once the model is biased toward correct hypotheses, loss becomes a useful signal for distinguishing correct vs incorrect hypotheses on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 0\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypothesisDataset defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that includes global IDs for tracking.\"\"\"\n",
    "    def __init__(self, DO):\n",
    "        # Input features = inpt_vars + hypothesis column\n",
    "        input_cols = DO.inpt_vars + [f'{DO.miss_vars[0]}_hypothesis']\n",
    "        self.inputs = torch.tensor(\n",
    "            DO.df_train_hypothesis[input_cols].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.targets = torch.tensor(\n",
    "            DO.df_train_hypothesis[DO.target_vars].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.global_ids = torch.arange(len(self.inputs))\n",
    "        self.input_cols = input_cols\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.global_ids[idx]\n",
    "\n",
    "print(\"HypothesisDataset defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive_header",
   "metadata": {},
   "source": [
    "## Adaptive Context Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined_scoring_header",
   "metadata": {},
   "source": [
    "## Combined Loss + Gradient Scoring (for Iteration 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined_scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scoring utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_combined_score(loss, gradient, features, class_id, anchor_data, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Combine loss and gradient signals for scoring.\n",
    "    \n",
    "    For a truth-biased model:\n",
    "    - Lower loss = more likely correct (aligned with truth)\n",
    "    - Gradient similarity to correct anchor = more likely correct\n",
    "    \n",
    "    Final score = (1 - loss_weight) * gradient_score + loss_weight * (-normalized_loss)\n",
    "    Higher score = more likely correct\n",
    "    \"\"\"\n",
    "    # Gradient score (same as adaptive)\n",
    "    grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "    \n",
    "    # Loss score: lower loss = higher score\n",
    "    # We'll normalize this later when we have all losses\n",
    "    loss_score = -loss  # Negative because lower loss is better\n",
    "    \n",
    "    return {\n",
    "        'grad_score': grad_score,\n",
    "        'loss_score': loss_score,\n",
    "        'raw_loss': loss\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_and_combine_scores(all_scores, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Normalize loss scores per class and combine with gradient scores.\n",
    "    \n",
    "    Returns combined scores where higher = more likely correct.\n",
    "    \"\"\"\n",
    "    # Group by class\n",
    "    class_losses = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        if class_id not in class_losses:\n",
    "            class_losses[class_id] = []\n",
    "        class_losses[class_id].append(scores['raw_loss'])\n",
    "    \n",
    "    # Compute per-class mean and std for loss normalization\n",
    "    class_stats = {}\n",
    "    for class_id, losses in class_losses.items():\n",
    "        class_stats[class_id] = {\n",
    "            'mean': np.mean(losses),\n",
    "            'std': np.std(losses) + 1e-8\n",
    "        }\n",
    "    \n",
    "    # Normalize and combine\n",
    "    combined_scores = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        stats = class_stats[class_id]\n",
    "        \n",
    "        # Z-score normalize loss (then negate: lower loss = higher score)\n",
    "        normalized_loss_score = -(scores['raw_loss'] - stats['mean']) / stats['std']\n",
    "        \n",
    "        # Combine: weighted average of gradient and loss scores\n",
    "        combined = (1 - loss_weight) * scores['grad_score'] + loss_weight * normalized_loss_score\n",
    "        \n",
    "        combined_scores[sample_idx] = {\n",
    "            'gid': gid,\n",
    "            'combined_score': combined,\n",
    "            'grad_score': scores['grad_score'],\n",
    "            'loss_score': normalized_loss_score,\n",
    "            'raw_loss': scores['raw_loss'],\n",
    "            'class_id': class_id,\n",
    "            'is_correct': scores['is_correct']\n",
    "        }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "print(\"Combined scoring utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_header",
   "metadata": {},
   "source": [
    "## Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "analysis_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def analyze_threshold_precision(all_selections, title=\"Precision Analysis\", verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze precision at different thresholds.\n",
    "    \n",
    "    all_selections: list of (score, is_correct, sample_idx) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    if not all_selections:\n",
    "        print(\"No selections to analyze\")\n",
    "        return None, None\n",
    "    \n",
    "    # Compute precision at different percentiles\n",
    "    results = []\n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        n_include = max(1, int(len(all_selections) * pct / 100))\n",
    "        top_selections = all_selections[:n_include]\n",
    "        n_correct = sum(1 for _, is_correct, _ in top_selections if is_correct)\n",
    "        precision = n_correct / n_include\n",
    "        results.append({\n",
    "            'percentile': pct,\n",
    "            'n_samples': n_include,\n",
    "            'n_correct': n_correct,\n",
    "            'precision': precision\n",
    "        })\n",
    "    \n",
    "    # Compute precision in score bins\n",
    "    scores = [s[0] for s in all_selections]\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    n_bins = 10\n",
    "    bin_results = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_low = min_score + (max_score - min_score) * i / n_bins\n",
    "        bin_high = min_score + (max_score - min_score) * (i + 1) / n_bins\n",
    "        bin_selections = [(s, c) for s, c, _ in all_selections if bin_low <= s < bin_high]\n",
    "        if bin_selections:\n",
    "            bin_correct = sum(1 for _, c in bin_selections if c)\n",
    "            bin_results.append({\n",
    "                'bin': f'{bin_low:.2f}-{bin_high:.2f}',\n",
    "                'n_samples': len(bin_selections),\n",
    "                'precision': bin_correct / len(bin_selections)\n",
    "            })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(title)\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\nPrecision by Top Percentile (highest scores first):\")\n",
    "        print(\"-\" * 50)\n",
    "        for r in results:\n",
    "            print(f\"Top {r['percentile']:>3}%: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "        \n",
    "        if bin_results:\n",
    "            print(\"\\nPrecision by Score Bin:\")\n",
    "            print(\"-\" * 50)\n",
    "            for r in bin_results:\n",
    "                print(f\"Score {r['bin']}: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "    \n",
    "    return results, bin_results\n",
    "\n",
    "\n",
    "def select_hypotheses_adaptive(trainer, DO, anchor_data=None):\n",
    "    \"\"\"\n",
    "    Select best hypothesis per sample using adaptive context.\n",
    "    Returns list of (score, is_correct, sample_idx) sorted by score descending.\n",
    "    \"\"\"\n",
    "    if anchor_data is None:\n",
    "        anchor_data = compute_anchor_data(trainer, DO)\n",
    "    \n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    partial_sample_indices = anchor_data['partial_sample_indices']\n",
    "    blacklisted_gids = anchor_data['blacklisted_gids']\n",
    "    \n",
    "    all_selections = []\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_is_correct = False\n",
    "        best_gid = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return all_selections, anchor_data\n",
    "\n",
    "print(\"Analysis utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration1_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 1: Unbiased Training\n",
    "\n",
    "Train on ALL hypotheses equally. No selection = no feedback loop bias.\n",
    "Use Adaptive Context Selection to score hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iteration1_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack partial coverage: False\n",
      "Number of training hypotheses: 3453\n",
      "Hypotheses per sample: 3\n",
      "Number of samples: 1151\n",
      "Partial data samples: 28\n"
     ]
    }
   ],
   "source": [
    "# Initialize data\n",
    "set_to_deterministic(rand_state)\n",
    "\n",
    "DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "                  partial_perc, rand_state, device='cpu')\n",
    "DO.problem_type = 'regression'\n",
    "\n",
    "print(f\"Lack partial coverage: {DO.lack_partial_coverage}\")\n",
    "print(f\"Number of training hypotheses: {len(DO.df_train_hypothesis)}\")\n",
    "print(f\"Hypotheses per sample: {DO.num_hyp_comb}\")\n",
    "print(f\"Number of samples: {len(DO.df_train_hypothesis) // DO.num_hyp_comb}\")\n",
    "\n",
    "# Count partial data\n",
    "partial_correct_gids = DO.df_train_hypothesis[\n",
    "    (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "    (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "].index.tolist()\n",
    "print(f\"Partial data samples: {len(partial_correct_gids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iteration1_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input structure:\n",
      "  Total input size: 4\n",
      "  Shared features: 3\n",
      "  Hypothesis feature: 1\n",
      "\n",
      "Model created: HypothesisAmplifyingModel\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create dataloader\n",
    "    dataset = HypothesisDataset(DO)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Check input structure\n",
    "    input_size = dataset.inputs.shape[1]\n",
    "    n_shared_features = len(inpt_vars)\n",
    "    n_hypothesis_features = 1\n",
    "    \n",
    "    print(f\"\\nInput structure:\")\n",
    "    print(f\"  Total input size: {input_size}\")\n",
    "    print(f\"  Shared features: {n_shared_features}\")\n",
    "    print(f\"  Hypothesis feature: {n_hypothesis_features}\")\n",
    "    \n",
    "    # Create model\n",
    "    model_iter1 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel created: HypothesisAmplifyingModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "iteration1_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 1: Unbiased Training\n",
      "======================================================================\n",
      "Training on ALL hypotheses equally for 60 epochs\n",
      "Tracking gradients in last 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [00:00<00:01, 29.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60: Loss = 0.0226 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 43/60 [00:01<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/60: Loss = 0.0214 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:07<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60: Loss = 0.0214 (tracking)\n",
      "\n",
      "Iteration 1 complete. Final loss: 0.0214\n",
      "Tracked 3453 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Iteration 1\n",
    "if not DO.lack_partial_coverage:\n",
    "    trainer_iter1 = UnbiasedTrainer(DO, model_iter1, lr=lr)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 1: Unbiased Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training on ALL hypotheses equally for {iter1_epochs} epochs\")\n",
    "    print(f\"Tracking gradients in last {iter1_analysis_epochs} epochs\")\n",
    "    \n",
    "    iter1_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter1_epochs)):\n",
    "        # Track data in last N epochs\n",
    "        track = epoch >= (iter1_epochs - iter1_analysis_epochs)\n",
    "        \n",
    "        loss = trainer_iter1.train_epoch(dataloader, epoch, track_data=track)\n",
    "        iter1_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            status = \"(tracking)\" if track else \"\"\n",
    "            print(f\"Epoch {epoch+1}/{iter1_epochs}: Loss = {loss:.4f} {status}\")\n",
    "    \n",
    "    print(f\"\\nIteration 1 complete. Final loss: {iter1_losses[-1]:.4f}\")\n",
    "    print(f\"Tracked {len(trainer_iter1.loss_history)} hypotheses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "iteration1_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Selection Analysis (Adaptive Context)\n",
      "======================================================================\n",
      "Per-class method selection:\n",
      "  Class 0: grad_sim=-0.985 (good) -> GRADIENT-ONLY\n",
      "  Class 1: grad_sim=+0.956 (poor) -> ENRICHED (enriched_sim=-0.396)\n",
      "  Class 2: grad_sim=+0.825 (poor) -> ENRICHED (enriched_sim=-0.304)\n",
      "======================================================================\n",
      "ITERATION 1: Precision by Threshold\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:  112 samples, precision=58.9%\n",
      "Top  20%:  224 samples, precision=67.4%\n",
      "Top  30%:  336 samples, precision=67.6%\n",
      "Top  40%:  449 samples, precision=64.8%\n",
      "Top  50%:  561 samples, precision=59.9%\n",
      "Top  60%:  673 samples, precision=54.2%\n",
      "Top  70%:  786 samples, precision=52.0%\n",
      "Top  80%:  898 samples, precision=51.8%\n",
      "Top  90%: 1010 samples, precision=50.5%\n",
      "Top 100%: 1123 samples, precision=48.4%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -0.85--0.56:    8 samples, precision=37.5%\n",
      "Score -0.56--0.28:   24 samples, precision=20.8%\n",
      "Score -0.28-0.00:   35 samples, precision=25.7%\n",
      "Score 0.00-0.29:   40 samples, precision=32.5%\n",
      "Score 0.29-0.57:   56 samples, precision=32.1%\n",
      "Score 0.57-0.86:   98 samples, precision=50.0%\n",
      "Score 0.86-1.14:  121 samples, precision=48.8%\n",
      "Score 1.14-1.42:  108 samples, precision=30.6%\n",
      "Score 1.42-1.71:   32 samples, precision=28.1%\n",
      "Score 1.71-1.99:  600 samples, precision=57.5%\n"
     ]
    }
   ],
   "source": [
    "# Analyze Iteration 1 results\n",
    "if not DO.lack_partial_coverage:\n",
    "    # Get selections with adaptive context\n",
    "    all_selections_iter1, anchor_data_iter1 = select_hypotheses_adaptive(trainer_iter1, DO)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 1: Selection Analysis (Adaptive Context)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print_adaptive_method_summary(anchor_data_iter1, hyp_per_sample)\n",
    "    \n",
    "    # Convert to format for analysis\n",
    "    selections_for_analysis = [(s[0], s[1], s[2]) for s in all_selections_iter1]\n",
    "    results_iter1, _ = analyze_threshold_precision(\n",
    "        selections_for_analysis, \n",
    "        title=\"ITERATION 1: Precision by Threshold\"\n",
    "    )\n",
    "    \n",
    "    # Store precision for comparison\n",
    "    iter1_precision = {r['percentile']: r['precision'] for r in results_iter1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "iteration1_select_top",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Top 30% Selection\n",
      "======================================================================\n",
      "Selected 336 samples (top 30%)\n",
      "Correct: 227 (67.6% precision)\n",
      "Remaining samples: 787\n"
     ]
    }
   ],
   "source": [
    "# Select top N% for Iteration 2\n",
    "if not DO.lack_partial_coverage:\n",
    "    n_total = len(all_selections_iter1)\n",
    "    n_top = max(1, int(n_total * top_percentile / 100))\n",
    "    \n",
    "    top_selections = all_selections_iter1[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    top_gids = set(s[3] for s in top_selections)\n",
    "    \n",
    "    # Remaining samples (not in top N%)\n",
    "    remaining_sample_indices = set(s[2] for s in all_selections_iter1[n_top:])\n",
    "    \n",
    "    # Count correct in top selection\n",
    "    n_correct_top = sum(1 for s in top_selections if s[1])\n",
    "    precision_top = n_correct_top / n_top\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ITERATION 1: Top {top_percentile}% Selection\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Selected {n_top} samples (top {top_percentile}%)\")\n",
    "    print(f\"Correct: {n_correct_top} ({precision_top*100:.1f}% precision)\")\n",
    "    print(f\"Remaining samples: {len(remaining_sample_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration2_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 2: Biased Training\n",
    "\n",
    "Train a NEW model on:\n",
    "- Top 30% from Iteration 1 (high precision selections)\n",
    "- Partial data with upweighting (~25% of effective training)\n",
    "\n",
    "This creates a \"truth-biased\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "iteration2_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 2: Biased Training Setup\n",
      "======================================================================\n",
      "Training data:\n",
      "  Selected (top 30%): 336 samples\n",
      "  Partial data: 28 samples\n",
      "  Partial weight: 4.00x\n",
      "  Effective partial: 112.0 (25.0% of training)\n",
      "  Effective total: 448.0\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Get partial data GIDs (correct hypotheses from partial data)\n",
    "    partial_gids = set(anchor_data_iter1['partial_correct_gids'])\n",
    "    n_partial = len(partial_gids)\n",
    "    n_selected = len(top_gids)\n",
    "    \n",
    "    # Calculate weight for partial data\n",
    "    # Target: partial should be ~25% of effective training\n",
    "    # partial_weight * n_partial / (partial_weight * n_partial + n_selected) = 0.25\n",
    "    # Solving: partial_weight = 0.25 * n_selected / (0.75 * n_partial)\n",
    "    partial_weight = (partial_target_ratio * n_selected) / ((1 - partial_target_ratio) * n_partial)\n",
    "    partial_weight = max(1.0, partial_weight)  # At least weight 1\n",
    "    \n",
    "    effective_partial = n_partial * partial_weight\n",
    "    effective_total = effective_partial + n_selected\n",
    "    actual_partial_ratio = effective_partial / effective_total\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 2: Biased Training Setup\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training data:\")\n",
    "    print(f\"  Selected (top {top_percentile}%): {n_selected} samples\")\n",
    "    print(f\"  Partial data: {n_partial} samples\")\n",
    "    print(f\"  Partial weight: {partial_weight:.2f}x\")\n",
    "    print(f\"  Effective partial: {effective_partial:.1f} ({actual_partial_ratio*100:.1f}% of training)\")\n",
    "    print(f\"  Effective total: {effective_total:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "iteration2_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 2: Training Biased Model\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:00<00:00, 24.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: Loss = 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 24.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2 complete. Final loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create new model for Iteration 2\n",
    "    set_to_deterministic(rand_state + 1)  # Different seed for variety\n",
    "    \n",
    "    model_iter2 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    # Create biased trainer\n",
    "    trainer_iter2 = BiasedTrainer(\n",
    "        DO, model_iter2, \n",
    "        selected_gids=top_gids,\n",
    "        partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight,\n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 2: Training Biased Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    iter2_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter2_epochs)):\n",
    "        loss = trainer_iter2.train_epoch(dataloader, epoch, track_data=False)\n",
    "        iter2_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{iter2_epochs}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nIteration 2 complete. Final loss: {iter2_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration3_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 3: Score Remaining Data with Biased Model\n",
    "\n",
    "Use the truth-biased model from Iteration 2 to score the remaining 70% of data.\n",
    "Compute BOTH loss and gradient signals, then combine them.\n",
    "\n",
    "**Key insight**: Since the model is biased toward truth:\n",
    "- Correct hypotheses should have LOWER loss\n",
    "- Incorrect hypotheses should have HIGHER loss\n",
    "- This makes loss a useful discriminative signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "iteration3_compute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 3: Scoring Remaining Data\n",
      "======================================================================\n",
      "Scoring 787 remaining samples with biased model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 3: Scoring Remaining Data\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Scoring {len(remaining_sample_indices)} remaining samples with biased model\")\n",
    "    \n",
    "    # Create scorer\n",
    "    scorer = RemainingDataScorer(DO, model_iter2, remaining_sample_indices)\n",
    "    \n",
    "    # Score remaining data\n",
    "    scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    \n",
    "    # Get analysis\n",
    "    analysis_iter3 = scorer.get_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "iteration3_anchors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing anchors from biased model on partial data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 18.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n",
      "\n",
      "Biased model anchor similarities:\n",
      "  Class 0: grad_sim = +1.000\n",
      "  Class 1: grad_sim = +0.996\n",
      "  Class 2: grad_sim = -0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Compute new anchors using the biased model's view of partial data\n",
    "    # We need to score partial data with the biased model too\n",
    "    \n",
    "    print(\"\\nComputing anchors from biased model on partial data...\")\n",
    "    \n",
    "    # Score partial data with biased model\n",
    "    partial_sample_indices = anchor_data_iter1['partial_sample_indices']\n",
    "    partial_scorer = RemainingDataScorer(DO, model_iter2, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    \n",
    "    # Build anchor data from partial scores\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Create anchor data structure similar to compute_anchor_data\n",
    "    anchor_data_iter3 = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_similarity_grad': {},\n",
    "        'use_enriched': {},  # For now, use gradient-only for simplicity\n",
    "        'input_cols': inpt_vars,\n",
    "        'partial_correct_gids': anchor_data_iter1['partial_correct_gids'],\n",
    "        'blacklisted_gids': anchor_data_iter1['blacklisted_gids'],\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "    }\n",
    "    \n",
    "    # Compute anchors per class\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads = []\n",
    "        incorrect_grads = []\n",
    "        \n",
    "        for gid in anchor_data_iter1['partial_correct_gids']:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "        \n",
    "        for gid in anchor_data_iter1['blacklisted_gids']:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            anchor_data_iter3['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_iter3['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            # Compute similarity\n",
    "            sim = float(np.dot(\n",
    "                anchor_data_iter3['anchor_correct_grad'][class_id],\n",
    "                anchor_data_iter3['anchor_incorrect_grad'][class_id]\n",
    "            ) / (\n",
    "                np.linalg.norm(anchor_data_iter3['anchor_correct_grad'][class_id]) * \n",
    "                np.linalg.norm(anchor_data_iter3['anchor_incorrect_grad'][class_id]) + 1e-8\n",
    "            ))\n",
    "            anchor_data_iter3['anchor_similarity_grad'][class_id] = sim\n",
    "            anchor_data_iter3['use_enriched'][class_id] = False  # Gradient-only for now\n",
    "    \n",
    "    print(\"\\nBiased model anchor similarities:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        sim = anchor_data_iter3['anchor_similarity_grad'].get(class_id, None)\n",
    "        if sim is not None:\n",
    "            print(f\"  Class {class_id}: grad_sim = {sim:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "iteration3_combined_scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 3: Combined Loss + Gradient Scoring\n",
      "======================================================================\n",
      "Collected scores for 787 samples\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 3: Combined Loss + Gradient Scoring\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Collect scores for each sample in remaining data\n",
    "    all_scores_iter3 = {}  # sample_idx -> (best_gid, scores_dict)\n",
    "    \n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_combined = -np.inf\n",
    "        best_gid = None\n",
    "        best_scores = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid not in analysis_iter3:\n",
    "                continue\n",
    "            if analysis_iter3[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis_iter3[gid]['avg_gradient']\n",
    "            loss = analysis_iter3[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars].values.astype(np.float64)\n",
    "            is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "            \n",
    "            # Compute gradient score\n",
    "            grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data_iter3)\n",
    "            \n",
    "            scores = {\n",
    "                'grad_score': grad_score,\n",
    "                'raw_loss': loss,\n",
    "                'class_id': class_id,\n",
    "                'is_correct': is_correct\n",
    "            }\n",
    "            \n",
    "            # Store for later normalization\n",
    "            if best_gid is None or grad_score > best_combined:\n",
    "                best_combined = grad_score\n",
    "                best_gid = gid\n",
    "                best_scores = scores\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            all_scores_iter3[sample_idx] = (best_gid, best_scores)\n",
    "    \n",
    "    print(f\"Collected scores for {len(all_scores_iter3)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iteration3_normalize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 3: Combined Score Precision (loss_weight=0.5)\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:   78 samples, precision=57.7%\n",
      "Top  20%:  157 samples, precision=56.7%\n",
      "Top  30%:  236 samples, precision=56.8%\n",
      "Top  40%:  314 samples, precision=53.5%\n",
      "Top  50%:  393 samples, precision=51.1%\n",
      "Top  60%:  472 samples, precision=47.9%\n",
      "Top  70%:  550 samples, precision=46.2%\n",
      "Top  80%:  629 samples, precision=44.5%\n",
      "Top  90%:  708 samples, precision=42.5%\n",
      "Top 100%:  787 samples, precision=41.7%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -3.95--3.42:    3 samples, precision=0.0%\n",
      "Score -3.42--2.89:    2 samples, precision=0.0%\n",
      "Score -2.89--2.35:    1 samples, precision=100.0%\n",
      "Score -2.35--1.82:    1 samples, precision=0.0%\n",
      "Score -1.82--1.29:    1 samples, precision=0.0%\n",
      "Score -1.29--0.76:    2 samples, precision=100.0%\n",
      "Score -0.76--0.22:  199 samples, precision=31.7%\n",
      "Score -0.22-0.31:  359 samples, precision=39.6%\n",
      "Score 0.31-0.84:   94 samples, precision=48.9%\n",
      "Score 0.84-1.37:  124 samples, precision=59.7%\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Normalize and combine scores\n",
    "    loss_weight = 0.5  # Equal weight to loss and gradient\n",
    "    \n",
    "    combined_scores = normalize_and_combine_scores(all_scores_iter3, loss_weight=loss_weight)\n",
    "    \n",
    "    # Create selection list sorted by combined score\n",
    "    all_selections_iter3 = [\n",
    "        (scores['combined_score'], scores['is_correct'], sample_idx)\n",
    "        for sample_idx, scores in combined_scores.items()\n",
    "    ]\n",
    "    all_selections_iter3.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Analyze precision\n",
    "    results_iter3_combined, _ = analyze_threshold_precision(\n",
    "        all_selections_iter3,\n",
    "        title=f\"ITERATION 3: Combined Score Precision (loss_weight={loss_weight})\"\n",
    "    )\n",
    "    \n",
    "    iter3_precision_combined = {r['percentile']: r['precision'] for r in results_iter3_combined}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "iteration3_gradient_only",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 3: Gradient-Only Precision (biased model)\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:   78 samples, precision=59.0%\n",
      "Top  20%:  157 samples, precision=60.5%\n",
      "Top  30%:  236 samples, precision=52.1%\n",
      "Top  40%:  314 samples, precision=49.4%\n",
      "Top  50%:  393 samples, precision=46.6%\n",
      "Top  60%:  472 samples, precision=45.6%\n",
      "Top  70%:  550 samples, precision=43.8%\n",
      "Top  80%:  629 samples, precision=43.2%\n",
      "Top  90%:  708 samples, precision=42.5%\n",
      "Top 100%:  787 samples, precision=41.7%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -0.00-0.20:  644 samples, precision=37.3%\n",
      "Score 1.78-1.98:  142 samples, precision=61.3%\n",
      "======================================================================\n",
      "ITERATION 3: Loss-Only Precision (biased model)\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:   78 samples, precision=52.6%\n",
      "Top  20%:  157 samples, precision=51.6%\n",
      "Top  30%:  236 samples, precision=50.0%\n",
      "Top  40%:  314 samples, precision=47.8%\n",
      "Top  50%:  393 samples, precision=49.4%\n",
      "Top  60%:  472 samples, precision=47.2%\n",
      "Top  70%:  550 samples, precision=46.2%\n",
      "Top  80%:  629 samples, precision=44.4%\n",
      "Top  90%:  708 samples, precision=42.4%\n",
      "Top 100%:  787 samples, precision=41.7%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -7.91--6.97:    3 samples, precision=0.0%\n",
      "Score -6.97--6.03:    2 samples, precision=0.0%\n",
      "Score -6.03--5.10:    1 samples, precision=100.0%\n",
      "Score -5.10--4.16:    1 samples, precision=0.0%\n",
      "Score -4.16--3.22:    4 samples, precision=75.0%\n",
      "Score -3.22--2.28:   10 samples, precision=70.0%\n",
      "Score -2.28--1.35:    4 samples, precision=50.0%\n",
      "Score -1.35--0.41:  197 samples, precision=28.9%\n",
      "Score -0.41-0.53:  386 samples, precision=44.0%\n",
      "Score 0.53-1.47:  178 samples, precision=49.4%\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Also analyze gradient-only and loss-only for comparison\n",
    "    \n",
    "    # Gradient-only\n",
    "    all_selections_iter3_grad = [\n",
    "        (scores['grad_score'], scores['is_correct'], sample_idx)\n",
    "        for sample_idx, scores in combined_scores.items()\n",
    "    ]\n",
    "    all_selections_iter3_grad.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    results_iter3_grad, _ = analyze_threshold_precision(\n",
    "        all_selections_iter3_grad,\n",
    "        title=\"ITERATION 3: Gradient-Only Precision (biased model)\"\n",
    "    )\n",
    "    iter3_precision_grad = {r['percentile']: r['precision'] for r in results_iter3_grad}\n",
    "    \n",
    "    # Loss-only\n",
    "    all_selections_iter3_loss = [\n",
    "        (scores['loss_score'], scores['is_correct'], sample_idx)\n",
    "        for sample_idx, scores in combined_scores.items()\n",
    "    ]\n",
    "    all_selections_iter3_loss.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    results_iter3_loss, _ = analyze_threshold_precision(\n",
    "        all_selections_iter3_loss,\n",
    "        title=\"ITERATION 3: Loss-Only Precision (biased model)\"\n",
    "    )\n",
    "    iter3_precision_loss = {r['percentile']: r['precision'] for r in results_iter3_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparison: Iteration 1 vs Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: Precision Improvement\n",
      "======================================================================\n",
      "\n",
      "Note: Iteration 3 scores the REMAINING 70% (not in top 30% of Iter 1)\n",
      "These are the 'harder' samples that Iteration 1 was less confident about.\n",
      "\n",
      "======================================================================\n",
      "ITERATION 1: Precision on Remaining 70% (for comparison)\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:   78 samples, precision=64.1%\n",
      "Top  20%:  157 samples, precision=55.4%\n",
      "Top  30%:  236 samples, precision=47.5%\n",
      "Top  40%:  314 samples, precision=41.4%\n",
      "Top  50%:  393 samples, precision=39.9%\n",
      "Top  60%:  472 samples, precision=41.1%\n",
      "Top  70%:  550 samples, precision=41.8%\n",
      "Top  80%:  629 samples, precision=42.9%\n",
      "Top  90%:  708 samples, precision=41.7%\n",
      "Top 100%:  787 samples, precision=40.2%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -0.85--0.56:    8 samples, precision=37.5%\n",
      "Score -0.56--0.28:   24 samples, precision=20.8%\n",
      "Score -0.28-0.00:   35 samples, precision=25.7%\n",
      "Score 0.00-0.28:   40 samples, precision=32.5%\n",
      "Score 0.28-0.57:   54 samples, precision=33.3%\n",
      "Score 0.57-0.85:   96 samples, precision=49.0%\n",
      "Score 0.85-1.13:  122 samples, precision=49.2%\n",
      "Score 1.13-1.42:  108 samples, precision=30.6%\n",
      "Score 1.42-1.70:   35 samples, precision=28.6%\n",
      "Score 1.70-1.98:  264 samples, precision=44.3%\n",
      "\n",
      "======================================================================\n",
      "PRECISION COMPARISON TABLE (on remaining 70% samples)\n",
      "======================================================================\n",
      "Percentile   Iter1 (unbiased)   Iter3 Grad      Iter3 Loss      Iter3 Combined \n",
      "---------------------------------------------------------------------------\n",
      "Top  10%        64.1%             59.0%          52.6%          57.7%\n",
      "Top  20%        55.4%             60.5%          51.6%          56.7% *\n",
      "Top  30%        47.5%             52.1%          50.0%          56.8% *\n",
      "Top  40%        41.4%             49.4%          47.8%          53.5% *\n",
      "Top  50%        39.9%             46.6%          49.4%          51.1% *\n",
      "Top  60%        41.1%             45.6%          47.2%          47.9% *\n",
      "Top  70%        41.8%             43.8%          46.2%          46.2% *\n",
      "Top  80%        42.9%             43.2%          44.4%          44.5%\n",
      "Top  90%        41.7%             42.5%          42.4%          42.5%\n",
      "Top 100%        40.2%             41.7%          41.7%          41.7%\n",
      "\n",
      "* = >2% improvement over Iteration 1\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPARISON: Precision Improvement\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: Iteration 3 scores the REMAINING 70% (not in top 30% of Iter 1)\")\n",
    "    print(\"These are the 'harder' samples that Iteration 1 was less confident about.\")\n",
    "    print()\n",
    "    \n",
    "    # For fair comparison, we need Iteration 1's precision on the SAME remaining samples\n",
    "    # Get Iteration 1 scores for remaining samples only\n",
    "    iter1_remaining = [s for s in all_selections_iter1[n_top:]]  # Already sorted, these are the remaining\n",
    "    iter1_remaining_for_analysis = [(s[0], s[1], s[2]) for s in iter1_remaining]\n",
    "    \n",
    "    results_iter1_remaining, _ = analyze_threshold_precision(\n",
    "        iter1_remaining_for_analysis,\n",
    "        title=\"ITERATION 1: Precision on Remaining 70% (for comparison)\",\n",
    "        verbose=True\n",
    "    )\n",
    "    iter1_remaining_precision = {r['percentile']: r['precision'] for r in results_iter1_remaining}\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PRECISION COMPARISON TABLE (on remaining 70% samples)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Percentile':<12} {'Iter1 (unbiased)':<18} {'Iter3 Grad':<15} {'Iter3 Loss':<15} {'Iter3 Combined':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for pct in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        p1 = iter1_remaining_precision.get(pct, 0) * 100\n",
    "        p3g = iter3_precision_grad.get(pct, 0) * 100\n",
    "        p3l = iter3_precision_loss.get(pct, 0) * 100\n",
    "        p3c = iter3_precision_combined.get(pct, 0) * 100\n",
    "        \n",
    "        # Highlight improvement\n",
    "        best = max(p3g, p3l, p3c)\n",
    "        improvement = best - p1\n",
    "        marker = \" *\" if improvement > 2 else \"\"\n",
    "        \n",
    "        print(f\"Top {pct:>3}%      {p1:>6.1f}%           {p3g:>6.1f}%        {p3l:>6.1f}%        {p3c:>6.1f}%{marker}\")\n",
    "    \n",
    "    print(\"\\n* = >2% improvement over Iteration 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "s6crbibdrp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGREEMENT ANALYSIS: When Iter1 AND Iter3 select same hypothesis\n",
      "======================================================================\n",
      "\n",
      "Total remaining samples: 787\n",
      "\n",
      "AGREED (same hypothesis):\n",
      "  Count: 294 (37.4%)\n",
      "  Correct: 144\n",
      "  PRECISION: 49.0%\n",
      "\n",
      "DISAGREED (different hypothesis):\n",
      "  Count: 493 (62.6%)\n",
      "  Iter1 correct: 172 (34.9%)\n",
      "  Iter3 correct: 184 (37.3%)\n"
     ]
    }
   ],
   "source": [
    "# Agreement analysis: both methods select same hypothesis\n",
    "if not DO.lack_partial_coverage:\n",
    "    # Build lookup for Iter1: sample_idx -> selected gid\n",
    "    iter1_gid_lookup = {s[2]: s[3] for s in iter1_remaining}  # sample_idx -> gid\n",
    "    \n",
    "    # Build lookup for Iter3: sample_idx -> selected gid\n",
    "    iter3_gid_lookup = {sample_idx: scores['gid'] for sample_idx, scores in combined_scores.items()}\n",
    "    \n",
    "    # Find samples where both methods agree\n",
    "    agreed_samples = []\n",
    "    disagreed_samples = []\n",
    "    \n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        iter1_gid = iter1_gid_lookup.get(sample_idx)\n",
    "        iter3_gid = iter3_gid_lookup.get(sample_idx)\n",
    "        \n",
    "        if iter1_gid is not None and iter3_gid is not None:\n",
    "            if iter1_gid == iter3_gid:\n",
    "                # Both methods selected same hypothesis\n",
    "                is_correct = DO.df_train_hypothesis.iloc[iter1_gid]['correct_hypothesis']\n",
    "                agreed_samples.append((sample_idx, iter1_gid, is_correct))\n",
    "            else:\n",
    "                # Methods disagree\n",
    "                iter1_correct = DO.df_train_hypothesis.iloc[iter1_gid]['correct_hypothesis']\n",
    "                iter3_correct = DO.df_train_hypothesis.iloc[iter3_gid]['correct_hypothesis']\n",
    "                disagreed_samples.append((sample_idx, iter1_gid, iter3_gid, iter1_correct, iter3_correct))\n",
    "    \n",
    "    # Calculate precision\n",
    "    n_agreed = len(agreed_samples)\n",
    "    n_agreed_correct = sum(1 for s in agreed_samples if s[2])\n",
    "    agreed_precision = n_agreed_correct / n_agreed * 100 if n_agreed > 0 else 0\n",
    "    \n",
    "    n_disagreed = len(disagreed_samples)\n",
    "    n_iter1_correct_disagree = sum(1 for s in disagreed_samples if s[3])\n",
    "    n_iter3_correct_disagree = sum(1 for s in disagreed_samples if s[4])\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"AGREEMENT ANALYSIS: When Iter1 AND Iter3 select same hypothesis\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal remaining samples: {len(remaining_sample_indices)}\")\n",
    "    print(f\"\\nAGREED (same hypothesis):\")\n",
    "    print(f\"  Count: {n_agreed} ({n_agreed/len(remaining_sample_indices)*100:.1f}%)\")\n",
    "    print(f\"  Correct: {n_agreed_correct}\")\n",
    "    print(f\"  PRECISION: {agreed_precision:.1f}%\")\n",
    "    print(f\"\\nDISAGREED (different hypothesis):\")\n",
    "    print(f\"  Count: {n_disagreed} ({n_disagreed/len(remaining_sample_indices)*100:.1f}%)\")\n",
    "    print(f\"  Iter1 correct: {n_iter1_correct_disagree} ({n_iter1_correct_disagree/n_disagreed*100:.1f}%)\")\n",
    "    print(f\"  Iter3 correct: {n_iter3_correct_disagree} ({n_iter3_correct_disagree/n_disagreed*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "iem3y35roi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIDENCE-RANKED AGREEMENT: High confidence agreement = better precision?\n",
      "======================================================================\n",
      "\n",
      "Total agreed samples: 294\n",
      "Overall agreement precision: 49.0%\n",
      "\n",
      "Precision by confidence percentile (among agreed samples):\n",
      "--------------------------------------------------\n",
      "Top  10% confidence:   29 samples, precision=51.7%\n",
      "Top  20% confidence:   58 samples, precision=53.4%\n",
      "Top  30% confidence:   88 samples, precision=53.4%\n",
      "Top  40% confidence:  117 samples, precision=59.0%\n",
      "Top  50% confidence:  147 samples, precision=55.8%\n",
      "Top  60% confidence:  176 samples, precision=53.4%\n",
      "Top  70% confidence:  205 samples, precision=49.8%\n",
      "Top  80% confidence:  235 samples, precision=46.8%\n",
      "Top  90% confidence:  264 samples, precision=47.7%\n",
      "Top 100% confidence:  294 samples, precision=49.0%\n"
     ]
    }
   ],
   "source": [
    "# Confidence-ranked agreement analysis\n",
    "if not DO.lack_partial_coverage and n_agreed > 0:\n",
    "    # For agreed samples, compute combined confidence score\n",
    "    # Need to get Iter1 scores for the agreed samples\n",
    "    iter1_score_lookup = {s[2]: s[0] for s in iter1_remaining}  # sample_idx -> score\n",
    "\n",
    "    # Compute combined score for each agreed sample\n",
    "    agreed_with_scores = []\n",
    "    for sample_idx, gid, is_correct in agreed_samples:\n",
    "        iter1_score = iter1_score_lookup.get(sample_idx, 0)\n",
    "        iter3_score = combined_scores[sample_idx]['combined_score']\n",
    "        combined_conf = iter1_score + iter3_score\n",
    "        agreed_with_scores.append((combined_conf, is_correct, sample_idx))\n",
    "\n",
    "    # Sort by combined confidence (descending)\n",
    "    agreed_with_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Analyze precision at different percentiles\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONFIDENCE-RANKED AGREEMENT: High confidence agreement = better precision?\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal agreed samples: {n_agreed}\")\n",
    "    print(f\"Overall agreement precision: {agreed_precision:.1f}%\")\n",
    "    print(f\"\\nPrecision by confidence percentile (among agreed samples):\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for pct in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "        n_include = max(1, int(len(agreed_with_scores) * pct / 100))\n",
    "        top = agreed_with_scores[:n_include]\n",
    "        n_correct = sum(1 for s in top if s[1])\n",
    "        prec = n_correct / n_include * 100\n",
    "        print(f\"Top {pct:>3}% confidence: {n_include:>4} samples, precision={prec:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tsq1hztx7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\n",
      "======================================================================\n",
      "\n",
      "Scoring Iter1's top 30% (336 samples) with biased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Scored 336 samples\n",
      "\n",
      "Original top 30%: 336 samples, 67.6% precision\n",
      "\n",
      "Precision after REMOVING lowest-confidence samples:\n",
      "------------------------------------------------------------\n",
      "Remove     Remaining    Correct    Precision    Change    \n",
      "------------------------------------------------------------\n",
      "  0%       336          227          67.6%        +0.0pp\n",
      " 10%       303          210          69.3%        +1.7pp\n",
      " 20%       269          194          72.1%        +4.6pp\n",
      " 30%       236          173          73.3%        +5.7pp **\n",
      " 40%       202          148          73.3%        +5.7pp **\n",
      " 50%       168          125          74.4%        +6.8pp **\n",
      "\n",
      "** = >5pp improvement over original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRUNING APPROACH: Use biased model to remove likely-wrong samples from Iter1's top 30%\n",
    "# =============================================================================\n",
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Score Iter1's top 30% samples with the biased model\n",
    "    print(f\"\\nScoring Iter1's top {top_percentile}% ({n_top} samples) with biased model...\")\n",
    "    \n",
    "    # Create scorer for top samples\n",
    "    top_scorer = RemainingDataScorer(DO, model_iter2, top_sample_indices)\n",
    "    top_scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    top_analysis = top_scorer.get_analysis()\n",
    "    \n",
    "    # Also need to score partial data with biased model to get anchors (reuse from before)\n",
    "    # anchor_data_iter3 was already computed\n",
    "    \n",
    "    # Compute combined scores for each top sample\n",
    "    top_scores = []\n",
    "    for sample_idx in top_sample_indices:\n",
    "        start = sample_idx * hyp_per_sample\n",
    "        \n",
    "        # Find the gid that Iter1 selected for this sample\n",
    "        iter1_selected_gid = None\n",
    "        for s in top_selections:\n",
    "            if s[2] == sample_idx:\n",
    "                iter1_selected_gid = s[3]\n",
    "                break\n",
    "        \n",
    "        if iter1_selected_gid is None or iter1_selected_gid not in top_analysis:\n",
    "            continue\n",
    "        \n",
    "        gid = iter1_selected_gid\n",
    "        if top_analysis[gid]['avg_gradient'] is None:\n",
    "            continue\n",
    "            \n",
    "        gradient = top_analysis[gid]['avg_gradient']\n",
    "        loss = top_analysis[gid]['avg_loss']\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        features = DO.df_train_hypothesis.loc[gid, inpt_vars].values.astype(np.float64)\n",
    "        is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        # Compute gradient score using biased model anchors\n",
    "        grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data_iter3)\n",
    "        \n",
    "        # Use negative loss as score (lower loss = higher score)\n",
    "        loss_score = -loss\n",
    "        \n",
    "        # Combined score (can tune weights)\n",
    "        combined = 0.5 * grad_score + 0.5 * loss_score\n",
    "        \n",
    "        top_scores.append({\n",
    "            'sample_idx': sample_idx,\n",
    "            'gid': gid,\n",
    "            'is_correct': is_correct,\n",
    "            'grad_score': grad_score,\n",
    "            'loss': loss,\n",
    "            'combined_score': combined\n",
    "        })\n",
    "    \n",
    "    print(f\"Scored {len(top_scores)} samples\")\n",
    "    \n",
    "    # Sort by combined score (ascending - lowest scores are most likely wrong)\n",
    "    top_scores_sorted = sorted(top_scores, key=lambda x: x['combined_score'])\n",
    "    \n",
    "    # Analyze precision after removing bottom N%\n",
    "    print(f\"\\nOriginal top {top_percentile}%: {n_top} samples, {precision_top*100:.1f}% precision\")\n",
    "    print(f\"\\nPrecision after REMOVING lowest-confidence samples:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Remove':<10} {'Remaining':<12} {'Correct':<10} {'Precision':<12} {'Change':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for remove_pct in [0, 10, 20, 30, 40, 50]:\n",
    "        n_remove = int(len(top_scores_sorted) * remove_pct / 100)\n",
    "        remaining = top_scores_sorted[n_remove:]  # Remove lowest scores\n",
    "        \n",
    "        n_remaining = len(remaining)\n",
    "        n_correct = sum(1 for s in remaining if s['is_correct'])\n",
    "        prec = n_correct / n_remaining * 100 if n_remaining > 0 else 0\n",
    "        change = prec - precision_top * 100\n",
    "        \n",
    "        marker = \" **\" if change > 5 else \"\"\n",
    "        print(f\"{remove_pct:>3}%       {n_remaining:<12} {n_correct:<10} {prec:>6.1f}%      {change:>+6.1f}pp{marker}\")\n",
    "    \n",
    "    print(\"\\n** = >5pp improvement over original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    # Plot 1: Precision comparison\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(percentiles, [iter1_remaining_precision.get(p, 0)*100 for p in percentiles], \n",
    "             'b-o', label='Iter1 (unbiased)', linewidth=2)\n",
    "    ax1.plot(percentiles, [iter3_precision_grad.get(p, 0)*100 for p in percentiles], \n",
    "             'g--s', label='Iter3 Gradient', linewidth=2)\n",
    "    ax1.plot(percentiles, [iter3_precision_loss.get(p, 0)*100 for p in percentiles], \n",
    "             'r--^', label='Iter3 Loss', linewidth=2)\n",
    "    ax1.plot(percentiles, [iter3_precision_combined.get(p, 0)*100 for p in percentiles], \n",
    "             'm-*', label='Iter3 Combined', linewidth=2, markersize=10)\n",
    "    ax1.axhline(y=100/hyp_per_sample, color='gray', linestyle=':', label='Random baseline')\n",
    "    ax1.set_xlabel('Top Percentile')\n",
    "    ax1.set_ylabel('Precision (%)')\n",
    "    ax1.set_title('Precision on Remaining 70% Samples')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Improvement over Iteration 1\n",
    "    ax2 = axes[1]\n",
    "    improvement_grad = [iter3_precision_grad.get(p, 0)*100 - iter1_remaining_precision.get(p, 0)*100 for p in percentiles]\n",
    "    improvement_loss = [iter3_precision_loss.get(p, 0)*100 - iter1_remaining_precision.get(p, 0)*100 for p in percentiles]\n",
    "    improvement_combined = [iter3_precision_combined.get(p, 0)*100 - iter1_remaining_precision.get(p, 0)*100 for p in percentiles]\n",
    "    \n",
    "    ax2.bar([p-2 for p in percentiles], improvement_grad, width=2, label='Gradient', alpha=0.7)\n",
    "    ax2.bar([p for p in percentiles], improvement_loss, width=2, label='Loss', alpha=0.7)\n",
    "    ax2.bar([p+2 for p in percentiles], improvement_combined, width=2, label='Combined', alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('Top Percentile')\n",
    "    ax2.set_ylabel('Precision Improvement (pp)')\n",
    "    ax2.set_title('Improvement over Iteration 1')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/precision_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPlot saved to {results_path}/precision_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATIVE APPROACH SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nIteration 1: Unbiased Training\")\n",
    "    print(f\"  - Trained on ALL hypotheses for {iter1_epochs} epochs\")\n",
    "    print(f\"  - Top {top_percentile}% precision: {iter1_precision.get(top_percentile, 0)*100:.1f}%\")\n",
    "    print(f\"  - Overall precision: {iter1_precision.get(100, 0)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nIteration 2: Biased Training\")\n",
    "    print(f\"  - Trained on top {top_percentile}% ({n_top} samples) + partial ({n_partial} samples)\")\n",
    "    print(f\"  - Partial weight: {partial_weight:.2f}x\")\n",
    "    print(f\"  - Training set was ~{precision_top*100:.1f}% correct (top selections) + 100% correct (partial)\")\n",
    "    \n",
    "    print(f\"\\nIteration 3: Score Remaining Data\")\n",
    "    print(f\"  - Scored {len(remaining_sample_indices)} remaining samples with biased model\")\n",
    "    print(f\"  - Best method at top 30%:\")\n",
    "    \n",
    "    p1_30 = iter1_remaining_precision.get(30, 0) * 100\n",
    "    p3g_30 = iter3_precision_grad.get(30, 0) * 100\n",
    "    p3l_30 = iter3_precision_loss.get(30, 0) * 100\n",
    "    p3c_30 = iter3_precision_combined.get(30, 0) * 100\n",
    "    \n",
    "    print(f\"    Iter1 unbiased: {p1_30:.1f}%\")\n",
    "    print(f\"    Iter3 gradient: {p3g_30:.1f}% ({p3g_30 - p1_30:+.1f}pp)\")\n",
    "    print(f\"    Iter3 loss:     {p3l_30:.1f}% ({p3l_30 - p1_30:+.1f}pp)\")\n",
    "    print(f\"    Iter3 combined: {p3c_30:.1f}% ({p3c_30 - p1_30:+.1f}pp)\")\n",
    "    \n",
    "    best_improvement = max(p3g_30, p3l_30, p3c_30) - p1_30\n",
    "    if best_improvement > 0:\n",
    "        print(f\"\\n  --> IMPROVEMENT: +{best_improvement:.1f} percentage points at top 30%\")\n",
    "    else:\n",
    "        print(f\"\\n  --> No improvement at top 30% (need to investigate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If Iteration 3 shows improvement:\n",
    "1. **Continue iterating**: Add high-confidence Iter3 selections to training, retrain, repeat\n",
    "2. **Tune loss_weight**: Try different weightings of loss vs gradient\n",
    "3. **Final GGH**: Use the improved selections to feed into full GGH training and measure R2\n",
    "\n",
    "If Iteration 3 shows no improvement:\n",
    "1. **Investigate**: Check if biased model is truly biased (loss distribution on partial data)\n",
    "2. **Try different percentiles**: Maybe top 20% or top 40% works better\n",
    "3. **Alternative scoring**: Try other combinations of signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
