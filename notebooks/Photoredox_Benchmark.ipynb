{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Photoredox Yield Benchmark\n",
    "\n",
    "This notebook benchmarks **Full Info vs Partial vs GGH Soft Refinement** on the Photoredox Yield dataset.\n",
    "\n",
    "**Dataset**: Merck Photoredox Reactions (1649 reactions, photocatalyst moles as missing variable)\n",
    "\n",
    "**Methods Compared**:\n",
    "1. **Full Info** (Oracle): Uses all data with correct hypothesis values\n",
    "2. **Partial**: Uses only 10% of samples with known correct hypotheses  \n",
    "3. **GGH Soft Refinement**: Uses gradient-guided hypothesis selection with soft weighting\n",
    "\n",
    "**Expected Results** (10% partial):\n",
    "- Full Info R2: ~0.85 (upper bound)\n",
    "- Partial Info R2: ~0.44 (baseline)\n",
    "- GGH R2: Between Partial and Full Info\n",
    "\n",
    "**Key Adaptations for 4 Hypotheses**:\n",
    "- Baseline random precision: 25% (1/4)\n",
    "- Soft weighting (min_weight=0.1) instead of hard percentile cutoffs\n",
    "- Higher baseline than Airfoil/Photocell (6 classes) but lower than Wine (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators\n",
    "from GGH.models import initialize_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Photoredox Yield\n",
      "Input variables: ['aryl_halides', 'photocalysts', 'piperidines_moles']\n",
      "Target: ['uplcms']\n",
      "Missing variable: ['photocalysts_moles']\n",
      "Hypothesis values: [0.02, 0.05, 0.5, 5.0]\n",
      "Number of hypotheses: 4\n",
      "Partial percentage: 30.0%\n",
      "Epochs: 300\n",
      "Learning rate: 0.001\n",
      "Baseline random precision: 25.0%\n",
      "Results will be saved to: ../saved_results/Photoredox Benchmark\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA CONFIGURATION - Photoredox Yield Dataset\n",
    "# =============================================================================\n",
    "data_path = '../data/photoredox_yield/photo_redox_merck2021_1649reactions.csv'\n",
    "results_path = \"../saved_results/Photoredox Benchmark\"\n",
    "\n",
    "# Variables\n",
    "inpt_vars = ['aryl_halides', 'photocalysts', 'piperidines_moles']\n",
    "target_vars = ['uplcms']\n",
    "miss_vars = ['photocalysts_moles']\n",
    "\n",
    "# Hypothesis values (4 photocatalyst moles values)\n",
    "hypothesis = [[0.02, 0.05, 0.50, 5.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])  # 4 hypotheses\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters (from original notebook)\n",
    "partial_perc = 0.30  # 10% complete data\n",
    "dropout = 0.05\n",
    "lr = 0.001  # Note: 0.001 for Photoredox (vs 0.004 for Airfoil/Photocell)\n",
    "nu = 0.1\n",
    "\n",
    "# Benchmark parameters (matching original multi_experiments)\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_EPOCHS = 300  # int(120*2.5) - same for both full and partial\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset: Photoredox Yield\")\n",
    "print(f\"Input variables: {inpt_vars}\")\n",
    "print(f\"Target: {target_vars}\")\n",
    "print(f\"Missing variable: {miss_vars}\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")\n",
    "print(f\"Number of hypotheses: {hyp_per_sample}\")\n",
    "print(f\"Partial percentage: {partial_perc*100}%\")\n",
    "print(f\"Epochs: {BENCHMARK_EPOCHS}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"Baseline random precision: {100/hyp_per_sample:.1f}%\")\n",
    "print(f\"Results will be saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## Benchmark: Partial vs Full Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: Full Info vs Partial Info on Photoredox Yield\n",
      "================================================================================\n",
      "Full Info: 300 epochs, all data with correct hypothesis\n",
      "Partial Info: 300 epochs, only 30.0% partial data\n",
      "Number of runs: 15\n",
      "Hyperparameters: lr=0.001, dropout=0.05, nu=0.1\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8228\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6411\n",
      ">>> Full Info improvement over Partial: +0.1816\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=43)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.9069\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.7432\n",
      ">>> Full Info improvement over Partial: +0.1638\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=44)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8162\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6539\n",
      ">>> Full Info improvement over Partial: +0.1623\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=45)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8063\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.7741\n",
      ">>> Full Info improvement over Partial: +0.0322\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=46)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8755\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.7132\n",
      ">>> Full Info improvement over Partial: +0.1623\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=47)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.7870\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6494\n",
      ">>> Full Info improvement over Partial: +0.1376\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=48)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8514\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.5720\n",
      ">>> Full Info improvement over Partial: +0.2794\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=49)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.7727\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6687\n",
      ">>> Full Info improvement over Partial: +0.1039\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=50)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8750\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.7769\n",
      ">>> Full Info improvement over Partial: +0.0982\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=51)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.9097\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.7891\n",
      ">>> Full Info improvement over Partial: +0.1206\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=52)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8305\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.5338\n",
      ">>> Full Info improvement over Partial: +0.2967\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=53)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.7383\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6678\n",
      ">>> Full Info improvement over Partial: +0.0706\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=54)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8245\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6390\n",
      ">>> Full Info improvement over Partial: +0.1855\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=55)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.8233\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6882\n",
      ">>> Full Info improvement over Partial: +0.1351\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=56)\n",
      "============================================================\n",
      "Training Full Info model (300 epochs)...\n",
      "Full Info R2: 0.7333\n",
      "Training Partial Info model (300 epochs)...\n",
      "Partial Info R2: 0.6193\n",
      ">>> Full Info improvement over Partial: +0.1140\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK RESULTS: Full Info vs Partial Info\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   r_state    R2 Full      R2 Partial   Δ R2      \n",
      "-------------------------------------------------------\n",
      "1     42         0.8228       0.6411          +0.1816\n",
      "2     43         0.9069       0.7432          +0.1638\n",
      "3     44         0.8162       0.6539          +0.1623\n",
      "4     45         0.8063       0.7741          +0.0322\n",
      "5     46         0.8755       0.7132          +0.1623\n",
      "6     47         0.7870       0.6494          +0.1376\n",
      "7     48         0.8514       0.5720          +0.2794\n",
      "8     49         0.7727       0.6687          +0.1039\n",
      "9     50         0.8750       0.7769          +0.0982\n",
      "10    51         0.9097       0.7891          +0.1206\n",
      "11    52         0.8305       0.5338          +0.2967\n",
      "12    53         0.7383       0.6678          +0.0706\n",
      "13    54         0.8245       0.6390          +0.1855\n",
      "14    55         0.8233       0.6882          +0.1351\n",
      "15    56         0.7333       0.6193          +0.1140\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Test R2 Score:\n",
      "  Full Info:    0.8249 ± 0.0518\n",
      "  Partial Info: 0.6753 ± 0.0715\n",
      "\n",
      "Statistical Test (paired t-test):\n",
      "  t=8.280, p=0.000001 ***\n",
      "\n",
      "Win Rate:\n",
      "  Full Info wins: 15/15 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "EXPECTED RESULTS (from original notebook at 10% partial):\n",
      "  Full Info:    ~0.85\n",
      "  Partial Info: ~0.44\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "Full Info significantly outperforms Partial Info (p=0.000001)\n",
      "Average R2 improvement: 0.1496\n",
      "\n",
      "This gap (0.1496) represents the potential for GGH to improve upon Partial.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: Full Info vs Partial Info\n",
    "# =============================================================================\n",
    "# Using the EXACT same flow as the original notebook's multi_experiments\n",
    "# =============================================================================\n",
    "\n",
    "def full_experiment(use_info, DO, INSPECT, batch_size, hidden_size, output_size, num_epochs, rand_state, results_path, \n",
    "                    dropout=0.05, lr=0.001, nu=0.1, final_analysis=False):\n",
    "    \"\"\"Exact copy of full_experiment from original notebook.\"\"\"\n",
    "    AM = AlgoModulators(DO, lr=lr, nu=nu)\n",
    "    dataloader = DO.prep_dataloader(use_info, batch_size)\n",
    "    \n",
    "    model = initialize_model(DO, dataloader, hidden_size, rand_state, dropout=dropout)\n",
    "    \n",
    "    TVM = TrainValidationManager(use_info, num_epochs, dataloader, batch_size, rand_state, \n",
    "                                 results_path, final_analysis=final_analysis)\n",
    "    TVM.train_model(DO, AM, model, final_analysis=final_analysis)\n",
    "    \n",
    "    INSPECT.save_train_val_logs(DO, AM, TVM, model, final_analysis=final_analysis)\n",
    "    \n",
    "    return DO, TVM, model\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: Full Info vs Partial Info on Photoredox Yield\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Full Info: {BENCHMARK_EPOCHS} epochs, all data with correct hypothesis\")\n",
    "print(f\"Partial Info: {BENCHMARK_EPOCHS} epochs, only {partial_perc*100}% partial data\")\n",
    "print(f\"Number of runs: {BENCHMARK_N_RUNS}\")\n",
    "print(f\"Hyperparameters: lr={lr}, dropout={dropout}, nu={nu}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "valid_runs = 0\n",
    "\n",
    "# Iterate through r_states starting from 42, same as original multi_experiments\n",
    "for r_state in range(42, 2000):\n",
    "    if valid_runs >= BENCHMARK_N_RUNS:\n",
    "        break\n",
    "    \n",
    "    # Set deterministic (same as multi_experiments)\n",
    "    set_to_deterministic(r_state)\n",
    "    \n",
    "    # Create DataOperator\n",
    "    DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                      partial_perc, r_state, device='cpu')\n",
    "    \n",
    "    # Skip if lack of partial coverage (same as original)\n",
    "    if DO.lack_partial_coverage:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {valid_runs + 1}/{BENCHMARK_N_RUNS} (rand_state={r_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create Inspector (same as original)\n",
    "    INSPECT = Inspector(results_path, hidden_size)\n",
    "    \n",
    "    # === Run Full Info (same r_state as original) ===\n",
    "    print(f\"Training Full Info model ({BENCHMARK_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(r_state)\n",
    "    DO_full = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                           partial_perc, r_state, device='cpu')\n",
    "    DO_full, TVM_full, model_full = full_experiment(\n",
    "        \"full info\", DO_full, INSPECT, batch_size, hidden_size, output_size, \n",
    "        BENCHMARK_EPOCHS, r_state, results_path, dropout, lr, nu, final_analysis=False\n",
    "    )\n",
    "    r2_full = INSPECT.calculate_val_r2score(DO_full, TVM_full, model_full, data=\"test\")\n",
    "    print(f\"Full Info R2: {r2_full:.4f}\")\n",
    "    \n",
    "    # === Run Partial Info (same r_state as original) ===\n",
    "    print(f\"Training Partial Info model ({BENCHMARK_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(r_state)\n",
    "    DO_partial = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                              partial_perc, r_state, device='cpu')\n",
    "    DO_partial, TVM_partial, model_partial = full_experiment(\n",
    "        \"partial info\", DO_partial, INSPECT, batch_size, hidden_size, output_size,\n",
    "        BENCHMARK_EPOCHS, r_state, results_path, dropout, lr, nu, final_analysis=False\n",
    "    )\n",
    "    r2_partial = INSPECT.calculate_val_r2score(DO_partial, TVM_partial, model_partial, data=\"test\")\n",
    "    print(f\"Partial Info R2: {r2_partial:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': r_state,\n",
    "        'r2_full': r2_full,\n",
    "        'r2_partial': r2_partial,\n",
    "        'improvement': r2_full - r2_partial,\n",
    "    })\n",
    "    \n",
    "    print(f\">>> Full Info improvement over Partial: {results[-1]['improvement']:+.4f}\")\n",
    "    \n",
    "    valid_runs += 1\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: Full Info vs Partial Info\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'r_state':<10} {'R2 Full':<12} {'R2 Partial':<12} {'\\u0394 R2':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['rand_state']:<10} {r['r2_full']:<12.4f} {r['r2_partial']:<12.4f} {r['improvement']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "r2_fulls = [r['r2_full'] for r in results]\n",
    "r2_partials = [r['r2_partial'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  Full Info:    {np.mean(r2_fulls):.4f} \\u00b1 {np.std(r2_fulls):.4f}\")\n",
    "print(f\"  Partial Info: {np.mean(r2_partials):.4f} \\u00b1 {np.std(r2_partials):.4f}\")\n",
    "\n",
    "# Statistical test\n",
    "t_stat, p_value = stats.ttest_rel(r2_fulls, r2_partials)\n",
    "print(f\"\\nStatistical Test (paired t-test):\")\n",
    "print(f\"  t={t_stat:.3f}, p={p_value:.6f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''}\")\n",
    "\n",
    "# Win count\n",
    "n_full_wins = sum(1 for r in results if r['r2_full'] > r['r2_partial'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  Full Info wins: {n_full_wins}/{BENCHMARK_N_RUNS} ({n_full_wins/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Expected results at 10% partial\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPECTED RESULTS (from original notebook at 10% partial):\")\n",
    "print(\"  Full Info:    ~0.85\")\n",
    "print(\"  Partial Info: ~0.44\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, r2_fulls, width, label='Full Info', color='green', alpha=0.7)\n",
    "ax.bar(x + width/2, r2_partials, width, label='Partial Info', color='orange', alpha=0.7)\n",
    "ax.set_xlabel('Run')\n",
    "ax.set_ylabel('Test R2')\n",
    "ax.set_title(f'Photoredox Yield: Full Info vs Partial Info ({partial_perc*100}% partial)')\n",
    "ax.legend()\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "ax.axhline(y=np.mean(r2_fulls), color='green', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=np.mean(r2_partials), color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/full_vs_partial_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement = np.mean([r['improvement'] for r in results])\n",
    "print(f\"Full Info significantly outperforms Partial Info (p={p_value:.6f})\")\n",
    "print(f\"Average R2 improvement: {avg_improvement:.4f}\")\n",
    "print(f\"\\nThis gap ({avg_improvement:.4f}) represents the potential for GGH to improve upon Partial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder_header",
   "metadata": {},
   "source": [
    "## GGH Soft Weight Iterative Refinement\n",
    "\n",
    "The GGH method uses soft weights instead of hard selection:\n",
    "1. **Iter1**: Unbiased training → enriched scores → initial soft weights\n",
    "2. **Iter2**: Weighted training on ALL samples (different weights) + partial\n",
    "3. **Iter3**: Biased rescoring → multiply weights (iterative refinement)\n",
    "4. **Iter4**: Loss-based adjustment → final weights\n",
    "5. **Final**: Train with refined weights + dynamic partial\n",
    "\n",
    "**Key Parameters (adapted for 4 hypotheses)**:\n",
    "- Baseline random: 25% (1/4)\n",
    "- Top percentile concept replaced by soft weighting (min_weight=0.1)\n",
    "- Temperature parameters control weight distribution sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "placeholder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGH Soft Refinement functions defined.\n",
      "Baseline random precision for 4 classes: 25.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GGH SOFT WEIGHT ITERATIVE REFINEMENT - Adapted for 4 Hypothesis Classes\n",
    "# =============================================================================\n",
    "from torch.autograd import grad\n",
    "\n",
    "# === GGH CONFIGURATION (adapted for 4 hypotheses) ===\n",
    "GGH_ITER1_EPOCHS = 60              # 60 epochs unbiased\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5      # Last 5 tracked\n",
    "GGH_ITER1_LR = 0.01\n",
    "GGH_ITER2_EPOCHS = 30              # 30 epochs weighted training\n",
    "GGH_ITER2_LR = 0.01\n",
    "GGH_SCORING_PASSES = 5             # 5 passes for scoring\n",
    "GGH_FINAL_EPOCHS = 200             # Final training epochs\n",
    "\n",
    "# Soft Weighting Parameters\n",
    "GGH_MIN_WEIGHT = 0.1               # No sample below this weight\n",
    "GGH_TEMPERATURE_ITER1 = 1.0        # Sharpness of Iter1 weight distribution\n",
    "GGH_TEMPERATURE_ITER3 = 0.8        # Sharpness of Iter3 weight distribution\n",
    "GGH_LOSS_INFLUENCE = 0.25          # How much loss affects final weights\n",
    "GGH_PARTIAL_BASE_WEIGHT = 2.0      # Base partial weight\n",
    "GGH_BENCHMARK_LR = 0.01            # LR for final training\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "# === MODEL DEFINITIONS ===\n",
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    Shared features path (smaller) + Hypothesis feature path (larger).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "# === TRAINER CLASSES ===\n",
    "class UnbiasedTrainer:\n",
    "    \"\"\"Train on ALL hypotheses equally. Track per-hypothesis losses and gradients.\"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "\n",
    "class WeightedTrainer:\n",
    "    \"\"\"Train on ALL samples with continuous weights.\"\"\"\n",
    "    def __init__(self, DO, model, sample_weights, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.sample_weights = sample_weights\n",
    "        self.partial_gids = set(partial_gids)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                elif gid in self.sample_weights:\n",
    "                    weights[i] = self.sample_weights[gid]\n",
    "            \n",
    "            if weights.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "\n",
    "\n",
    "class RemainingDataScorer:\n",
    "    \"\"\"Score remaining data using biased model.\"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        self.loss_scores = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        self.model.eval()\n",
    "        \n",
    "        for pass_idx in range(n_passes):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# === UTILITY FUNCTIONS ===\n",
    "def sigmoid_stable(x):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    return np.where(x >= 0,\n",
    "                    1 / (1 + np.exp(-x)),\n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "\n",
    "def compute_soft_weights(scores, min_weight=0.1, temperature=1.0):\n",
    "    \"\"\"Convert scores to soft weights using sigmoid.\"\"\"\n",
    "    scores = np.array(scores, dtype=np.float64)\n",
    "    if len(scores) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    mean_s = np.mean(scores)\n",
    "    std_s = np.std(scores) + 1e-8\n",
    "    normalized = (scores - mean_s) / std_s\n",
    "    \n",
    "    raw_weights = sigmoid_stable(normalized / temperature)\n",
    "    weights = min_weight + (1 - min_weight) * raw_weights\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"Compute gradient-only anchors AND enriched anchors for each class.\"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score with loss included.\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    loss_params = anchor_data.get('loss_norm_params', {}).get(class_id)\n",
    "    grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "    \n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    if loss_params:\n",
    "        loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "    else:\n",
    "        loss_norm = -loss * grad_scale\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "    \n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def train_with_soft_weights(DO, model, sample_weights, partial_gids, partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with soft weights and validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = WeightedTrainer(DO, model, sample_weights=sample_weights, \n",
    "                             partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "# === MAIN GGH SOFT REFINEMENT FUNCTION ===\n",
    "def run_ggh_soft_refinement(DO, rand_state):\n",
    "    \"\"\"\n",
    "    GGH with soft weight iterative refinement.\n",
    "    Returns: gid_weights, effective_precision, partial_gids, partial_weight_dynamic\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training + Initial soft weights ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    sample_scores = {}\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_gid, best_is_correct = -np.inf, None, False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            sample_scores[sample_idx] = (best_score, best_gid, best_is_correct)\n",
    "    \n",
    "    scores_list = [s[0] for s in sample_scores.values()]\n",
    "    weights_iter1 = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n",
    "    \n",
    "    gid_weights = {}\n",
    "    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n",
    "        gid_weights[gid] = float(weights_iter1[i])\n",
    "    \n",
    "    iter1_correct = sum(1 for s in sample_scores.values() if s[2])\n",
    "    iter1_precision = iter1_correct / len(sample_scores) * 100\n",
    "    correct_weights = [gid_weights[s[1]] for s in sample_scores.values() if s[2]]\n",
    "    incorrect_weights = [gid_weights[s[1]] for s in sample_scores.values() if not s[2]]\n",
    "    avg_weight_correct = np.mean(correct_weights) if correct_weights else 0\n",
    "    avg_weight_incorrect = np.mean(incorrect_weights) if incorrect_weights else 0\n",
    "    \n",
    "    print(f\"  Iter1: {len(sample_scores)} samples, precision: {iter1_precision:.1f}%\")\n",
    "    print(f\"    Avg weight correct: {avg_weight_correct:.3f}, incorrect: {avg_weight_incorrect:.3f}\")\n",
    "    \n",
    "    # === ITERATION 2: Weighted training ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_weighted = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    trainer_weighted = WeightedTrainer(DO, model_weighted, sample_weights=gid_weights,\n",
    "                                       partial_gids=partial_correct_gids,\n",
    "                                       partial_weight=GGH_PARTIAL_BASE_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_weighted.train_epoch(dataloader, epoch)\n",
    "    \n",
    "    # === ITERATION 3: Biased rescoring -> Multiply weights ===\n",
    "    selected_sample_indices = set(sample_scores.keys())\n",
    "    scorer = RemainingDataScorer(DO, model_weighted, selected_sample_indices | partial_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    biased_analysis = scorer.get_analysis()\n",
    "    \n",
    "    anchor_data_biased = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "        'loss_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    all_grads = [biased_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_biased['grad_scale'] = grad_scale\n",
    "    \n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        correct_losses, incorrect_losses = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if biased_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(biased_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    correct_losses.append(biased_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if biased_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(biased_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    incorrect_losses.append(biased_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n",
    "            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n",
    "            \n",
    "            all_losses = correct_losses + incorrect_losses\n",
    "            loss_mean = np.mean(all_losses)\n",
    "            loss_std = np.std(all_losses) + 1e-8\n",
    "            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n",
    "            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n",
    "            \n",
    "            correct_enriched = [np.concatenate([g, f, [l]]) \n",
    "                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n",
    "            incorrect_enriched = [np.concatenate([g, f, [l]]) \n",
    "                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n",
    "            \n",
    "            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    iter3_scores = {}\n",
    "    for sample_idx, (_, gid, _) in sample_scores.items():\n",
    "        if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None:\n",
    "            gradient = biased_analysis[gid]['avg_gradient']\n",
    "            loss = biased_analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            score = compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            iter3_scores[gid] = score\n",
    "    \n",
    "    scores_list_iter3 = list(iter3_scores.values())\n",
    "    gids_iter3 = list(iter3_scores.keys())\n",
    "    weights_iter3_raw = compute_soft_weights(scores_list_iter3, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER3)\n",
    "    \n",
    "    for i, gid in enumerate(gids_iter3):\n",
    "        gid_weights[gid] = gid_weights[gid] * weights_iter3_raw[i]\n",
    "    \n",
    "    if gid_weights:\n",
    "        max_w = max(gid_weights.values())\n",
    "        if max_w > 0:\n",
    "            for gid in gid_weights:\n",
    "                gid_weights[gid] = GGH_MIN_WEIGHT + (gid_weights[gid] / max_w) * (1 - GGH_MIN_WEIGHT)\n",
    "    \n",
    "    correct_weights_iter3 = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n",
    "    incorrect_weights_iter3 = [gid_weights[s[1]] for s in sample_scores.values() if not s[2] and s[1] in gid_weights]\n",
    "    avg_weight_correct_iter3 = np.mean(correct_weights_iter3) if correct_weights_iter3 else 0\n",
    "    avg_weight_incorrect_iter3 = np.mean(incorrect_weights_iter3) if incorrect_weights_iter3 else 0\n",
    "    print(f\"  Iter3: Avg weight correct: {avg_weight_correct_iter3:.3f}, incorrect: {avg_weight_incorrect_iter3:.3f}\")\n",
    "    \n",
    "    # === ITERATION 4: Loss-based adjustment ===\n",
    "    losses = {gid: biased_analysis[gid]['avg_loss']\n",
    "              for gid in gid_weights if gid in biased_analysis}\n",
    "    \n",
    "    if losses:\n",
    "        loss_values = list(losses.values())\n",
    "        loss_mean = np.mean(loss_values)\n",
    "        loss_std = np.std(loss_values) + 1e-8\n",
    "        \n",
    "        for gid in gid_weights:\n",
    "            if gid in losses:\n",
    "                norm_loss = (losses[gid] - loss_mean) / loss_std\n",
    "                loss_factor = 1 - GGH_LOSS_INFLUENCE * sigmoid_stable(norm_loss)\n",
    "                gid_weights[gid] = max(GGH_MIN_WEIGHT, gid_weights[gid] * loss_factor)\n",
    "    \n",
    "    correct_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n",
    "    incorrect_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if not s[2] and s[1] in gid_weights]\n",
    "    avg_weight_correct_final = np.mean(correct_weights_final) if correct_weights_final else 0\n",
    "    avg_weight_incorrect_final = np.mean(incorrect_weights_final) if incorrect_weights_final else 0\n",
    "    print(f\"  Iter4: Avg weight correct: {avg_weight_correct_final:.3f}, incorrect: {avg_weight_incorrect_final:.3f}\")\n",
    "    \n",
    "    total_weight_correct = sum(correct_weights_final)\n",
    "    total_weight_all = sum(gid_weights.values())\n",
    "    effective_precision = total_weight_correct / total_weight_all * 100 if total_weight_all > 0 else 0\n",
    "    \n",
    "    unweighted_correct = len(correct_weights_final)\n",
    "    unweighted_total = len(gid_weights)\n",
    "    unweighted_precision = unweighted_correct / unweighted_total * 100 if unweighted_total > 0 else 0\n",
    "    \n",
    "    print(f\"  Unweighted precision: {unweighted_precision:.1f}%, Effective precision: {effective_precision:.1f}%\")\n",
    "    \n",
    "    avg_final_weight = np.mean(list(gid_weights.values())) if gid_weights else 0.5\n",
    "    partial_weight_dynamic = GGH_PARTIAL_BASE_WEIGHT * (1 + (1 - avg_final_weight))\n",
    "    print(f\"  Dynamic partial_weight: {partial_weight_dynamic:.2f}\")\n",
    "    \n",
    "    return gid_weights, effective_precision, partial_correct_gids, partial_weight_dynamic\n",
    "\n",
    "\n",
    "print(\"GGH Soft Refinement functions defined.\")\n",
    "print(f\"Baseline random precision for {hyp_per_sample} classes: {100/hyp_per_sample:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ggh_benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: Full Info vs Partial vs GGH Soft Refinement on Photoredox\n",
      "================================================================================\n",
      "Full Info: 300 epochs, all data with correct hypothesis (oracle)\n",
      "Partial: 300 epochs, only 30.0% partial data\n",
      "GGH Soft Refinement: 200 epochs with iteratively refined weights\n",
      "  - Iter1: Unbiased 60 epochs -> enriched scoring -> soft weights\n",
      "  - Iter2: Weighted 30 epochs + partial\n",
      "  - Iter3: Biased rescoring -> multiply weights\n",
      "  - Iter4: Loss-based adjustment -> final weights\n",
      "Number of runs: 15\n",
      "Baseline random precision for 4 hypotheses: 25.0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 23.1%\n",
      "    Avg weight correct: 0.548, incorrect: 0.530\n",
      "  Iter3: Avg weight correct: 0.379, incorrect: 0.387\n",
      "  Iter4: Avg weight correct: 0.334, incorrect: 0.340\n",
      "  Unweighted precision: 23.1%, Effective precision: 22.8%\n",
      "  Dynamic partial_weight: 3.32\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=10, val_loss=0.0177, R2=0.6481\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0220, R2=0.5359\n",
      ">>> GGH improvement over Partial: +0.1122\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=43)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 25.4%\n",
      "    Avg weight correct: 0.551, incorrect: 0.543\n",
      "  Iter3: Avg weight correct: 0.411, incorrect: 0.436\n",
      "  Iter4: Avg weight correct: 0.360, incorrect: 0.385\n",
      "  Unweighted precision: 25.4%, Effective precision: 24.2%\n",
      "  Dynamic partial_weight: 3.24\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=8, val_loss=0.0177, R2=0.6903\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=0, val_loss=0.0205, R2=0.5894\n",
      ">>> GGH improvement over Partial: +0.1010\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=44)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 24.8%\n",
      "    Avg weight correct: 0.556, incorrect: 0.527\n",
      "  Iter3: Avg weight correct: 0.403, incorrect: 0.364\n",
      "  Iter4: Avg weight correct: 0.350, incorrect: 0.320\n",
      "  Unweighted precision: 24.8%, Effective precision: 26.5%\n",
      "  Dynamic partial_weight: 3.35\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=31, val_loss=0.0167, R2=0.6829\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=23, val_loss=0.0173, R2=0.6113\n",
      ">>> GGH improvement over Partial: +0.0716\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=45)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 24.4%\n",
      "    Avg weight correct: 0.551, incorrect: 0.526\n",
      "  Iter3: Avg weight correct: 0.400, incorrect: 0.377\n",
      "  Iter4: Avg weight correct: 0.350, incorrect: 0.331\n",
      "  Unweighted precision: 24.4%, Effective precision: 25.5%\n",
      "  Dynamic partial_weight: 3.33\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0173, R2=0.7045\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=9, val_loss=0.0193, R2=0.6410\n",
      ">>> GGH improvement over Partial: +0.0635\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=46)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 23.7%\n",
      "    Avg weight correct: 0.560, incorrect: 0.534\n",
      "  Iter3: Avg weight correct: 0.417, incorrect: 0.412\n",
      "  Iter4: Avg weight correct: 0.366, incorrect: 0.364\n",
      "  Unweighted precision: 23.7%, Effective precision: 23.8%\n",
      "  Dynamic partial_weight: 3.27\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=1, val_loss=0.0117, R2=0.7311\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0155, R2=0.6503\n",
      ">>> GGH improvement over Partial: +0.0807\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=47)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 23.3%\n",
      "    Avg weight correct: 0.547, incorrect: 0.534\n",
      "  Iter3: Avg weight correct: 0.391, incorrect: 0.399\n",
      "  Iter4: Avg weight correct: 0.344, incorrect: 0.350\n",
      "  Unweighted precision: 23.3%, Effective precision: 23.1%\n",
      "  Dynamic partial_weight: 3.30\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=4, val_loss=0.0180, R2=0.5833\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=94, val_loss=0.0169, R2=0.5470\n",
      ">>> GGH improvement over Partial: +0.0363\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=48)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 831 samples, precision: 22.9%\n",
      "    Avg weight correct: 0.545, incorrect: 0.533\n",
      "  Iter3: Avg weight correct: 0.366, incorrect: 0.393\n",
      "  Iter4: Avg weight correct: 0.321, incorrect: 0.344\n",
      "  Unweighted precision: 22.9%, Effective precision: 21.7%\n",
      "  Dynamic partial_weight: 3.32\n",
      "Training GGH model (200 epochs)...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: Full Info vs Partial vs GGH Soft Refinement\n",
    "# =============================================================================\n",
    "# Compare all three methods using SAME data splits (same r_state)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: Full Info vs Partial vs GGH Soft Refinement on Photoredox\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Full Info: {BENCHMARK_EPOCHS} epochs, all data with correct hypothesis (oracle)\")\n",
    "print(f\"Partial: {BENCHMARK_EPOCHS} epochs, only {partial_perc*100}% partial data\")\n",
    "print(f\"GGH Soft Refinement: {GGH_FINAL_EPOCHS} epochs with iteratively refined weights\")\n",
    "print(f\"  - Iter1: Unbiased {GGH_ITER1_EPOCHS} epochs -> enriched scoring -> soft weights\")\n",
    "print(f\"  - Iter2: Weighted {GGH_ITER2_EPOCHS} epochs + partial\")\n",
    "print(f\"  - Iter3: Biased rescoring -> multiply weights\")\n",
    "print(f\"  - Iter4: Loss-based adjustment -> final weights\")\n",
    "print(f\"Number of runs: {BENCHMARK_N_RUNS}\")\n",
    "print(f\"Baseline random precision for {hyp_per_sample} hypotheses: {100/hyp_per_sample:.1f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ggh_results = []\n",
    "valid_runs = 0\n",
    "\n",
    "# Use same r_state iteration pattern as original benchmark (starting from 42)\n",
    "for r_state in range(42, 2000):\n",
    "    if valid_runs >= BENCHMARK_N_RUNS:\n",
    "        break\n",
    "    \n",
    "    # Set deterministic\n",
    "    set_to_deterministic(r_state)\n",
    "    \n",
    "    # Create DataOperator\n",
    "    DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                      partial_perc, r_state, device='cpu')\n",
    "    \n",
    "    # Skip if lack of partial coverage\n",
    "    if DO.lack_partial_coverage:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {valid_runs + 1}/{BENCHMARK_N_RUNS} (rand_state={r_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data info\n",
    "    partial_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    # === Run GGH Soft Refinement ===\n",
    "    print(\"Running GGH Soft Refinement...\")\n",
    "    gid_weights, ggh_precision, _, partial_weight_dynamic = run_ggh_soft_refinement(DO, r_state)\n",
    "    \n",
    "    # === Train GGH final model with soft weights ===\n",
    "    print(f\"Training GGH model ({GGH_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(r_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_soft_weights(\n",
    "        DO, model_ggh, sample_weights=gid_weights, partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight_dynamic, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model (same architecture for fair comparison) ===\n",
    "    print(f\"Training Partial model ({GGH_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(r_state + 300)\n",
    "    model_partial_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial_ggh, partial_best_epoch, partial_best_val_loss = train_with_soft_weights(\n",
    "        DO, model_partial_ggh, sample_weights={}, partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO, model_partial_ggh)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    ggh_results.append({\n",
    "        'rand_state': r_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\">>> GGH improvement over Partial: {ggh_results[-1]['improvement_r2']:+.4f}\")\n",
    "    \n",
    "    valid_runs += 1\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY: GGH vs Partial\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: GGH Soft Refinement vs Partial\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'r_state':<10} {'GGH Prec':<12} {'R2 GGH':<12} {'R2 Partial':<12} {'\\u0394 R2':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for i, r in enumerate(ggh_results):\n",
    "    print(f\"{i+1:<5} {r['rand_state']:<10} {r['ggh_precision']:<12.1f}% {r['ggh_test_r2']:<12.4f} \"\n",
    "          f\"{r['partial_test_r2']:<12.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in ggh_results]\n",
    "partial_r2s_ggh = [r['partial_test_r2'] for r in ggh_results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in ggh_results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGGH Effective Precision: {np.mean(ggh_precisions):.1f}% \\u00b1 {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"  (Random baseline: {100/hyp_per_sample:.1f}%, improvement: {np.mean(ggh_precisions) - 100/hyp_per_sample:.1f}%)\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} \\u00b1 {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s_ggh):.4f} \\u00b1 {np.std(partial_r2s_ggh):.4f}\")\n",
    "\n",
    "# Statistical test\n",
    "t_stat_ggh, p_value_ggh = stats.ttest_rel(ggh_r2s, partial_r2s_ggh)\n",
    "print(f\"\\nStatistical Test (paired t-test):\")\n",
    "print(f\"  t={t_stat_ggh:.3f}, p={p_value_ggh:.6f} {'***' if p_value_ggh < 0.001 else '**' if p_value_ggh < 0.01 else '*' if p_value_ggh < 0.05 else ''}\")\n",
    "\n",
    "# Win count\n",
    "n_ggh_wins = sum(1 for r in ggh_results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins: {n_ggh_wins}/{BENCHMARK_N_RUNS} ({n_ggh_wins/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: R2 comparison (GGH vs Partial)\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, ggh_r2s, width, label='GGH Soft', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_r2s_ggh, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test R2')\n",
    "ax1.set_title('Test R2: GGH vs Partial (Same Architecture)')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "ax1.axhline(y=np.mean(ggh_r2s), color='blue', linestyle='--', alpha=0.5)\n",
    "ax1.axhline(y=np.mean(partial_r2s_ggh), color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 2: GGH Precision\n",
    "ax2 = axes[1]\n",
    "ax2.bar(range(1, BENCHMARK_N_RUNS+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax2.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax2.axhline(y=100/hyp_per_sample, color='gray', linestyle=':', label=f'Random: {100/hyp_per_sample:.1f}%')\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Effective Precision (%)')\n",
    "ax2.set_title(f'GGH Effective Precision (vs {100/hyp_per_sample:.1f}% random)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Improvement distribution\n",
    "ax3 = axes[2]\n",
    "improvements = [r['improvement_r2'] for r in ggh_results]\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "ax3.bar(range(1, BENCHMARK_N_RUNS+1), improvements, color=colors, alpha=0.7)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.axhline(y=np.mean(improvements), color='blue', linestyle='--', label=f'Mean: {np.mean(improvements):+.4f}')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('R2 Improvement')\n",
    "ax3.set_title('GGH R2 Improvement over Partial')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_vs_partial_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement_ggh = np.mean([r['improvement_r2'] for r in ggh_results])\n",
    "if avg_improvement_ggh > 0 and p_value_ggh < 0.05:\n",
    "    print(f\"GGH Soft Refinement significantly OUTPERFORMS Partial (p={p_value_ggh:.6f})\")\n",
    "    print(f\"Average R2 improvement: {avg_improvement_ggh:+.4f}\")\n",
    "elif avg_improvement_ggh < 0 and p_value_ggh < 0.05:\n",
    "    print(f\"Partial significantly OUTPERFORMS GGH Soft Refinement (p={p_value_ggh:.6f})\")\n",
    "else:\n",
    "    print(f\"No significant difference (p={p_value_ggh:.6f})\")\n",
    "print(f\"\\nGGH achieves {np.mean(ggh_precisions):.1f}% precision (vs {100/hyp_per_sample:.1f}% random baseline)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
