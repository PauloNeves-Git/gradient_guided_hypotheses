{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN + GGH Fusion Benchmark (Using GGH_2 Package)",
    "",
    "Simplified benchmark notebook using the refactored GGH_2 package."
   ],
   "id": "header"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA determinism fix (must be before torch import)\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Add parent directory to path for packages\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Import from GGH_2 package\n",
    "from GGH_2 import (\n",
    "    HypothesisAmplifyingModel,\n",
    "    set_to_deterministic,\n",
    "    train_with_soft_weights,\n",
    "    evaluate_on_test,\n",
    "    run_ggh_soft_refinement,\n",
    ")\n",
    "from GGH_2.ggh import DEFAULT_CONFIG\n",
    "\n",
    "# Import from GGH package (correct module paths)\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.imputation_methods import Imputer\n",
    "from GGH.models import initialize_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.selection_algorithms import AlgoModulators\n",
    "from GGH.inspector import Inspector\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "data_path = '../data/dataset_photo_pce10/data.csv'\n",
    "results_path = \"../saved_results/TabPFN_GGH_Fusion\"\n",
    "\n",
    "# Variables\n",
    "inpt_vars = ['P3HT', 'PTB7-Th']\n",
    "target_vars = ['Degradation']\n",
    "miss_vars = ['PCBM']\n",
    "\n",
    "# Hypothesis values (6 PCBM concentration values)\n",
    "hypothesis = [[0.03, 0.11, 0.20, 0.32, 0.43, 0.6]]\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_N_RUNS = 3\n",
    "PARTIAL_PERCENTAGES = [0.03, 0.10, 0.25]\n",
    "\n",
    "# GGH config (can override defaults)\n",
    "GGH_CONFIG = {\n",
    "    'iter1_epochs': 60,\n",
    "    'iter1_analysis_epochs': 5,\n",
    "    'iter1_lr': 0.01,\n",
    "    'iter2_epochs': 30,\n",
    "    'iter2_lr': 0.01,\n",
    "    'scoring_passes': 5,\n",
    "    'min_weight': 0.1,\n",
    "    'temperature_iter1': 1.0,\n",
    "    'temperature_iter3': 0.8,\n",
    "    'loss_influence': 0.25,\n",
    "    'partial_base_weight': 2.0,\n",
    "    'shared_hidden': 16,\n",
    "    'hypothesis_hidden': 32,\n",
    "    'final_hidden': 32,\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "GGH_FINAL_EPOCHS = 200\n",
    "GGH_BENCHMARK_LR = 0.01\n",
    "\n",
    "print(f\"Dataset: Photocell Degradation\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")\n",
    "print(f\"Partial percentages: {PARTIAL_PERCENTAGES}\")"
   ],
   "id": "config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN BENCHMARK\n",
    "# =============================================================================\n",
    "import time\n",
    "\n",
    "results = {pct: {'full_info': [], 'partial': [], 'ggh': [], 'tabpfn': []} for pct in PARTIAL_PERCENTAGES}\n",
    "\n",
    "for partial_pct in PARTIAL_PERCENTAGES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with {partial_pct*100:.0f}% partial data\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for run in range(BENCHMARK_N_RUNS):\n",
    "        r_state = run\n",
    "        set_to_deterministic(r_state)\n",
    "        \n",
    "        # Create DataOperator\n",
    "        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                          partial_pct, r_state, device=DEVICE)\n",
    "        \n",
    "        if DO.lack_partial_coverage:\n",
    "            print(f\"  Run {run+1}: Skipping (lack coverage)\")\n",
    "            continue\n",
    "        \n",
    "        n_shared = len(DO.inpt_vars)\n",
    "        n_hyp = len(DO.miss_vars)\n",
    "        out_size = len(DO.target_vars)\n",
    "        \n",
    "        partial_gids = set(DO.df_train_hypothesis[\n",
    "            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "        ].index.tolist())\n",
    "        \n",
    "        print(f\"\\n  Run {run+1}/{BENCHMARK_N_RUNS}\")\n",
    "        \n",
    "        # === Full Info ===\n",
    "        n_samples_full = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "        full_info_weights = {}\n",
    "        for sample_idx in range(n_samples_full):\n",
    "            for hyp_idx in range(hyp_per_sample):\n",
    "                gid = sample_idx * hyp_per_sample + hyp_idx\n",
    "                if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n",
    "                    full_info_weights[gid] = 1.0\n",
    "        \n",
    "        set_to_deterministic(r_state + 600)\n",
    "        model_full = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                               GGH_CONFIG['hypothesis_hidden'], \n",
    "                                               GGH_CONFIG['final_hidden'], out_size).to(DEVICE)\n",
    "        model_full, _, _ = train_with_soft_weights(DO, model_full, full_info_weights, set(),\n",
    "                                                    1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        _, _, full_r2 = evaluate_on_test(DO, model_full)\n",
    "        results[partial_pct]['full_info'].append(full_r2)\n",
    "        print(f\"    Full Info R2: {full_r2:.4f}\")\n",
    "        \n",
    "        # === Partial Only ===\n",
    "        set_to_deterministic(r_state + 400)\n",
    "        model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                                  GGH_CONFIG['hypothesis_hidden'],\n",
    "                                                  GGH_CONFIG['final_hidden'], out_size).to(DEVICE)\n",
    "        model_partial, _, _ = train_with_soft_weights(DO, model_partial, {}, partial_gids,\n",
    "                                                       1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        _, _, partial_r2 = evaluate_on_test(DO, model_partial)\n",
    "        results[partial_pct]['partial'].append(partial_r2)\n",
    "        print(f\"    Partial R2: {partial_r2:.4f}\")\n",
    "        \n",
    "        # === GGH ===\n",
    "        ggh_weights, ggh_precision, _, ggh_partial_weight = run_ggh_soft_refinement(DO, r_state, GGH_CONFIG)\n",
    "        set_to_deterministic(r_state + 200)\n",
    "        model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                              GGH_CONFIG['hypothesis_hidden'],\n",
    "                                              GGH_CONFIG['final_hidden'], out_size).to(DEVICE)\n",
    "        model_ggh, _, _ = train_with_soft_weights(DO, model_ggh, ggh_weights, partial_gids,\n",
    "                                                   ggh_partial_weight, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        _, _, ggh_r2 = evaluate_on_test(DO, model_ggh)\n",
    "        results[partial_pct]['ggh'].append(ggh_r2)\n",
    "        print(f\"    GGH R2: {ggh_r2:.4f} (precision: {ggh_precision:.1f}%)\")\n",
    "        \n",
    "        # === TabPFN Standalone ===\n",
    "        tabpfn_probs, _ = get_tabpfn_probabilities(DO, r_state, verbose=False)\n",
    "        tabpfn_weights = {}\n",
    "        partial_sample_indices = set(gid // hyp_per_sample for gid in partial_gids)\n",
    "        n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "        for sample_idx in range(n_samples):\n",
    "            if sample_idx in partial_sample_indices:\n",
    "                continue\n",
    "            if tabpfn_probs and sample_idx in tabpfn_probs:\n",
    "                pred_class = np.argmax(tabpfn_probs[sample_idx])\n",
    "                gid = sample_idx * hyp_per_sample + pred_class\n",
    "                tabpfn_weights[gid] = 1.0\n",
    "        \n",
    "        set_to_deterministic(r_state + 500)\n",
    "        model_tabpfn = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                                  GGH_CONFIG['hypothesis_hidden'],\n",
    "                                                  GGH_CONFIG['final_hidden'], out_size).to(DEVICE)\n",
    "        model_tabpfn, _, _ = train_with_soft_weights(DO, model_tabpfn, tabpfn_weights, partial_gids,\n",
    "                                                      GGH_CONFIG['partial_base_weight'], \n",
    "                                                      GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        _, _, tabpfn_r2 = evaluate_on_test(DO, model_tabpfn)\n",
    "        results[partial_pct]['tabpfn'].append(tabpfn_r2)\n",
    "        print(f\"    TabPFN R2: {tabpfn_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*60)"
   ],
   "id": "benchmark"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Partial %':<12} | {'Full Info':<12} | {'Partial':<12} | {'GGH':<12} | {'TabPFN':<12}\")\n",
    "print(\"-\"*68)\n",
    "\n",
    "for pct in PARTIAL_PERCENTAGES:\n",
    "    full_avg = np.mean(results[pct]['full_info']) if results[pct]['full_info'] else 0\n",
    "    partial_avg = np.mean(results[pct]['partial']) if results[pct]['partial'] else 0\n",
    "    ggh_avg = np.mean(results[pct]['ggh']) if results[pct]['ggh'] else 0\n",
    "    tabpfn_avg = np.mean(results[pct]['tabpfn']) if results[pct]['tabpfn'] else 0\n",
    "    \n",
    "    print(f\"{pct*100:>10.0f}% | {full_avg:>10.4f} | {partial_avg:>10.4f} | {ggh_avg:>10.4f} | {tabpfn_avg:>10.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ],
   "id": "summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPEED BENCHMARK: CPU vs GPU\n",
    "# =============================================================================\n",
    "import time\n",
    "\n",
    "SPEED_N_RUNS = 2\n",
    "SPEED_PARTIAL_PERC = 0.10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPEED BENCHMARK: CPU vs GPU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def run_speed_test(device_name, device):\n",
    "    \"\"\"Run all methods on specified device and return timing dict.\"\"\"\n",
    "    times = {'Full Info': [], 'Partial': [], 'GGH': [], 'TabPFN': []}\n",
    "    \n",
    "    for run in range(SPEED_N_RUNS):\n",
    "        r_state = run\n",
    "        set_to_deterministic(r_state)\n",
    "        \n",
    "        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                          SPEED_PARTIAL_PERC, r_state, device=device)\n",
    "        \n",
    "        if DO.lack_partial_coverage:\n",
    "            continue\n",
    "        \n",
    "        n_shared = len(DO.inpt_vars)\n",
    "        n_hyp = len(DO.miss_vars)\n",
    "        out_size = len(DO.target_vars)\n",
    "        \n",
    "        partial_gids = set(DO.df_train_hypothesis[\n",
    "            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "        ].index.tolist())\n",
    "        \n",
    "        print(f\"\\n  Run {run+1}/{SPEED_N_RUNS} on {device_name}\")\n",
    "        \n",
    "        # Full Info\n",
    "        start = time.time()\n",
    "        n_samples_full = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "        full_info_weights = {sample_idx * hyp_per_sample + hyp_idx: 1.0\n",
    "                            for sample_idx in range(n_samples_full)\n",
    "                            for hyp_idx in range(hyp_per_sample)\n",
    "                            if DO.df_train_hypothesis.iloc[sample_idx * hyp_per_sample + hyp_idx]['correct_hypothesis']}\n",
    "        \n",
    "        set_to_deterministic(r_state + 600)\n",
    "        model = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                          GGH_CONFIG['hypothesis_hidden'],\n",
    "                                          GGH_CONFIG['final_hidden'], out_size).to(device)\n",
    "        train_with_soft_weights(DO, model, full_info_weights, set(), 1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        times['Full Info'].append(time.time() - start)\n",
    "        print(f\"    Full Info: {times['Full Info'][-1]:.2f}s\")\n",
    "        \n",
    "        # Partial\n",
    "        start = time.time()\n",
    "        set_to_deterministic(r_state + 400)\n",
    "        model = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                          GGH_CONFIG['hypothesis_hidden'],\n",
    "                                          GGH_CONFIG['final_hidden'], out_size).to(device)\n",
    "        train_with_soft_weights(DO, model, {}, partial_gids, 1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        times['Partial'].append(time.time() - start)\n",
    "        print(f\"    Partial: {times['Partial'][-1]:.2f}s\")\n",
    "        \n",
    "        # GGH\n",
    "        start = time.time()\n",
    "        ggh_weights, _, _, ggh_partial_weight = run_ggh_soft_refinement(DO, r_state, GGH_CONFIG)\n",
    "        set_to_deterministic(r_state + 200)\n",
    "        model = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                          GGH_CONFIG['hypothesis_hidden'],\n",
    "                                          GGH_CONFIG['final_hidden'], out_size).to(device)\n",
    "        train_with_soft_weights(DO, model, ggh_weights, partial_gids, ggh_partial_weight, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        times['GGH'].append(time.time() - start)\n",
    "        print(f\"    GGH: {times['GGH'][-1]:.2f}s\")\n",
    "        \n",
    "        # TabPFN\n",
    "        start = time.time()\n",
    "        tabpfn_probs, _ = get_tabpfn_probabilities(DO, r_state, verbose=False)\n",
    "        tabpfn_weights = {}\n",
    "        partial_sample_indices = set(gid // hyp_per_sample for gid in partial_gids)\n",
    "        n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "        for sample_idx in range(n_samples):\n",
    "            if sample_idx not in partial_sample_indices and tabpfn_probs and sample_idx in tabpfn_probs:\n",
    "                gid = sample_idx * hyp_per_sample + np.argmax(tabpfn_probs[sample_idx])\n",
    "                tabpfn_weights[gid] = 1.0\n",
    "        \n",
    "        set_to_deterministic(r_state + 500)\n",
    "        model = HypothesisAmplifyingModel(n_shared, n_hyp, GGH_CONFIG['shared_hidden'],\n",
    "                                          GGH_CONFIG['hypothesis_hidden'],\n",
    "                                          GGH_CONFIG['final_hidden'], out_size).to(device)\n",
    "        train_with_soft_weights(DO, model, tabpfn_weights, partial_gids, GGH_CONFIG['partial_base_weight'], GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n",
    "        times['TabPFN'].append(time.time() - start)\n",
    "        print(f\"    TabPFN: {times['TabPFN'][-1]:.2f}s\")\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"\\nTesting on CPU...\")\n",
    "cpu_times = run_speed_test(\"CPU\", torch.device('cpu'))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nTesting on GPU...\")\n",
    "    gpu_times = run_speed_test(\"GPU\", torch.device('cuda'))\n",
    "else:\n",
    "    gpu_times = None\n",
    "    print(\"\\nGPU not available\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPEED SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for method in ['Full Info', 'Partial', 'GGH', 'TabPFN']:\n",
    "    cpu_avg = np.mean(cpu_times[method]) if cpu_times[method] else 0\n",
    "    if gpu_times and gpu_times[method]:\n",
    "        gpu_avg = np.mean(gpu_times[method])\n",
    "        speedup = cpu_avg / gpu_avg if gpu_avg > 0 else 0\n",
    "        print(f\"{method:<12}: CPU={cpu_avg:.1f}s, GPU={gpu_avg:.1f}s, Speedup={speedup:.1f}x\")\n",
    "    else:\n",
    "        print(f\"{method:<12}: CPU={cpu_avg:.1f}s\")"
   ],
   "id": "speed_benchmark"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}