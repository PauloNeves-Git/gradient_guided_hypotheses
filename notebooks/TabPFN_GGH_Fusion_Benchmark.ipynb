{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TabPFN + GGH Fusion Benchmark\n",
    "\n",
    "## Goal\n",
    "Create a hybrid method that combines GGH and TabPFN to get the best of both worlds:\n",
    "- **GGH excels** in extreme missingness (very little partial data)\n",
    "- **TabPFN excels** when there's more partial data available\n",
    "\n",
    "## Option A: TabPFN Confidence as GGH Prior\n",
    "Use TabPFN's prediction probabilities to weight GGH's scoring:\n",
    "\n",
    "```python\n",
    "# TabPFN predicts probability for each hypothesis\n",
    "tabpfn_probs = TabPFN.predict_proba(sample, hypotheses)\n",
    "\n",
    "# GGH computes gradient-based scores  \n",
    "ggh_scores = compute_enriched_scores(gradients, anchors)\n",
    "\n",
    "# Combine: TabPFN acts as prior, GGH as evidence\n",
    "final_score = ggh_score + alpha * log(tabpfn_prob)\n",
    "```\n",
    "\n",
    "## Methods Compared\n",
    "1. **GGH Soft Refinement** (baseline)\n",
    "2. **TabPFN Classifier** (constrained to hypothesis values)\n",
    "3. **TabPFN + GGH Fusion** (Option A)\n",
    "4. **Partial Only** (lower bound)\n",
    "5. **Full Info** (oracle upper bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport sys\nsys.path.insert(0, '../')\nsys.path.insert(0, '../GGH')\n\nfrom GGH.data_ops import DataOperator\nfrom GGH.selection_algorithms import AlgoModulators\nfrom GGH.models import initialize_model\nfrom GGH.train_val_loop import TrainValidationManager\nfrom GGH.inspector import Inspector\nfrom scipy import stats\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.autograd import grad\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# TabPFN imports\nfrom tabpfn import TabPFNClassifier\n\ndef set_to_deterministic(rand_state):\n    import random\n    random.seed(rand_state)\n    np.random.seed(rand_state)\n    torch.manual_seed(rand_state)\n    torch.set_num_threads(1)\n    torch.use_deterministic_algorithms(True)\n\nprint(\"Imports successful!\")\n\n# GPU Detection\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Photocell Degradation\n",
      "Hypothesis values: [0.03, 0.11, 0.2, 0.32, 0.43, 0.6]\n",
      "Partial percentages to test: [0.03, 0.1, 0.25]\n",
      "Fusion alpha: 1.0\n",
      "Confidence threshold: 0.4\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA CONFIGURATION - Photocell Degradation Dataset\n",
    "# =============================================================================\n",
    "data_path = '../data/dataset_photo_pce10/data.csv'\n",
    "results_path = \"../saved_results/TabPFN_GGH_Fusion\"\n",
    "\n",
    "# Variables\n",
    "inpt_vars = ['P3HT', 'PTB7-Th']\n",
    "target_vars = ['Degradation']\n",
    "miss_vars = ['PCBM']\n",
    "\n",
    "# Hypothesis values (6 PCBM concentration values)\n",
    "hypothesis = [[0.03, 0.11, 0.20, 0.32, 0.43, 0.6]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])  # 6 hypotheses\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "dropout = 0.05\n",
    "lr = 0.004\n",
    "nu = 0.1\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_N_RUNS = 3\n",
    "BENCHMARK_EPOCHS = 600\n",
    "\n",
    "# GGH parameters\n",
    "GGH_ITER1_EPOCHS = 60\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5\n",
    "GGH_ITER1_LR = 0.01\n",
    "GGH_ITER2_EPOCHS = 30\n",
    "GGH_ITER2_LR = 0.01\n",
    "GGH_SCORING_PASSES = 5\n",
    "GGH_FINAL_EPOCHS = 200\n",
    "GGH_MIN_WEIGHT = 0.1\n",
    "GGH_TEMPERATURE_ITER1 = 1.0\n",
    "GGH_TEMPERATURE_ITER3 = 0.8\n",
    "GGH_LOSS_INFLUENCE = 0.25\n",
    "GGH_PARTIAL_BASE_WEIGHT = 2.0\n",
    "GGH_BENCHMARK_LR = 0.01\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "# Fusion parameters\n",
    "FUSION_ALPHA = 1.0  # Weight for TabPFN log-probability\n",
    "CONFIDENCE_THRESHOLD = 0.4  # Only apply TabPFN when confidence > threshold\n",
    "\n",
    "# Test multiple partial percentages (reduced set for speed)\n",
    "PARTIAL_PERCENTAGES = [0.03, 0.10, 0.25]\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset: Photocell Degradation\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")\n",
    "print(f\"Partial percentages to test: {PARTIAL_PERCENTAGES}\")\n",
    "print(f\"Fusion alpha: {FUSION_ALPHA}\")\n",
    "print(f\"Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ggh_functions_header",
   "metadata": {},
   "source": [
    "## GGH Soft Refinement Functions\n",
    "\n",
    "Standard GGH implementation (copied from benchmark notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ggh_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGH functions defined.\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# GGH SOFT WEIGHT ITERATIVE REFINEMENT FUNCTIONS\n# =============================================================================\n\nclass HypothesisAmplifyingModel(nn.Module):\n    \"\"\"Neural network that amplifies the impact of hypothesis feature on gradients.\"\"\"\n    def __init__(self, n_shared_features, n_hypothesis_features=1, \n                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n        super().__init__()\n        \n        self.shared_path = nn.Sequential(\n            nn.Linear(n_shared_features, shared_hidden),\n            nn.ReLU(),\n        )\n        \n        self.hypothesis_path = nn.Sequential(\n            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n            nn.ReLU(),\n            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n            nn.ReLU(),\n        )\n        \n        combined_size = shared_hidden + hypothesis_hidden\n        self.final_path = nn.Sequential(\n            nn.Linear(combined_size, final_hidden),\n            nn.ReLU(),\n            nn.Linear(final_hidden, output_size)\n        )\n        \n        self.n_shared = n_shared_features\n        \n    def forward(self, x):\n        shared_features = x[:, :self.n_shared]\n        hypothesis_feature = x[:, self.n_shared:]\n        \n        shared_emb = self.shared_path(shared_features)\n        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n        \n        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n        return self.final_path(combined)\n\n\nclass UnbiasedTrainer:\n    \"\"\"Train on ALL hypotheses equally. Track per-hypothesis losses and gradients.\"\"\"\n    def __init__(self, DO, model, lr=0.001, device=DEVICE):\n        self.DO = DO\n        self.model = model\n        self.device = device\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss(reduction='none')\n        self.hyp_per_sample = DO.num_hyp_comb\n        \n        self.loss_history = {}\n        self.gradient_history = {}\n        \n    def train_epoch(self, dataloader, epoch, track_data=False):\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n            inputs = inputs.to(self.device)\n            targets = targets.to(self.device)\n            \n            predictions = self.model(inputs)\n            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n            batch_loss = individual_losses.mean()\n            \n            if track_data:\n                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n            \n            self.optimizer.zero_grad()\n            batch_loss.backward()\n            self.optimizer.step()\n            \n            total_loss += batch_loss.item()\n            num_batches += 1\n        \n        return total_loss / num_batches\n    \n    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n        self.model.eval()\n        \n        for i in range(len(inputs)):\n            gid = global_ids[i].item()\n            \n            if gid not in self.loss_history:\n                self.loss_history[gid] = []\n            self.loss_history[gid].append(losses[i].item())\n            \n            inp = inputs[i:i+1].clone().requires_grad_(True)\n            pred = self.model(inp)\n            loss = nn.MSELoss()(pred, targets[i:i+1])\n            \n            params = list(self.model.parameters())\n            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n            grad_vec = grad_param.flatten().detach().cpu().numpy()\n            \n            if gid not in self.gradient_history:\n                self.gradient_history[gid] = []\n            self.gradient_history[gid].append(grad_vec)\n        \n        self.model.train()\n    \n    def get_hypothesis_analysis(self):\n        analysis = {}\n        for gid in self.loss_history:\n            analysis[gid] = {\n                'avg_loss': np.mean(self.loss_history[gid]),\n                'loss_std': np.std(self.loss_history[gid]),\n                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n            }\n        return analysis\n\n\nclass WeightedTrainer:\n    \"\"\"Train on ALL samples with continuous weights.\"\"\"\n    def __init__(self, DO, model, sample_weights, partial_gids, partial_weight, lr=0.001, device=DEVICE):\n        self.DO = DO\n        self.model = model\n        self.device = device\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss(reduction='none')\n        self.hyp_per_sample = DO.num_hyp_comb\n        \n        self.sample_weights = sample_weights\n        self.partial_gids = set(partial_gids)\n        self.partial_weight = partial_weight\n        \n    def train_epoch(self, dataloader, epoch, track_data=False):\n        self.model.train()\n        total_loss = 0\n        total_weight = 0\n        \n        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n            inputs = inputs.to(self.device)\n            targets = targets.to(self.device)\n            \n            predictions = self.model(inputs)\n            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n            \n            weights = torch.zeros(len(inputs), device=self.device)\n            \n            for i, gid in enumerate(global_ids):\n                gid = gid.item()\n                if gid in self.partial_gids:\n                    weights[i] = self.partial_weight\n                elif gid in self.sample_weights:\n                    weights[i] = self.sample_weights[gid]\n            \n            if weights.sum() == 0:\n                continue\n            \n            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n            \n            self.optimizer.zero_grad()\n            weighted_loss.backward()\n            self.optimizer.step()\n            \n            total_loss += weighted_loss.item() * weights.sum().item()\n            total_weight += weights.sum().item()\n        \n        return total_loss / total_weight if total_weight > 0 else 0\n\n\nclass RemainingDataScorer:\n    \"\"\"Score remaining data using biased model.\"\"\"\n    def __init__(self, DO, model, remaining_sample_indices, device=DEVICE):\n        self.DO = DO\n        self.model = model\n        self.device = device\n        self.hyp_per_sample = DO.num_hyp_comb\n        self.remaining_sample_indices = set(remaining_sample_indices)\n        \n        self.loss_scores = {}\n        self.gradient_history = {}\n        \n    def compute_scores(self, dataloader, n_passes=5):\n        self.model.eval()\n        \n        for pass_idx in range(n_passes):\n            for inputs, targets, global_ids in dataloader:\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                for i in range(len(inputs)):\n                    gid = global_ids[i].item()\n                    sample_idx = gid // self.hyp_per_sample\n                    \n                    if sample_idx not in self.remaining_sample_indices:\n                        continue\n                    \n                    inp = inputs[i:i+1].clone().requires_grad_(True)\n                    pred = self.model(inp)\n                    loss = nn.MSELoss()(pred, targets[i:i+1])\n                    \n                    if gid not in self.loss_scores:\n                        self.loss_scores[gid] = []\n                    self.loss_scores[gid].append(loss.item())\n                    \n                    params = list(self.model.parameters())\n                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n                    \n                    if gid not in self.gradient_history:\n                        self.gradient_history[gid] = []\n                    self.gradient_history[gid].append(grad_vec)\n    \n    def get_analysis(self):\n        analysis = {}\n        for gid in self.loss_scores:\n            analysis[gid] = {\n                'avg_loss': np.mean(self.loss_scores[gid]),\n                'loss_std': np.std(self.loss_scores[gid]),\n                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n            }\n        return analysis\n\n\ndef sigmoid_stable(x):\n    \"\"\"Numerically stable sigmoid.\"\"\"\n    x = np.array(x, dtype=np.float64)\n    return np.where(x >= 0,\n                    1 / (1 + np.exp(-x)),\n                    np.exp(x) / (1 + np.exp(x)))\n\n\ndef compute_soft_weights(scores, min_weight=0.1, temperature=1.0):\n    \"\"\"Convert scores to soft weights using sigmoid.\"\"\"\n    scores = np.array(scores, dtype=np.float64)\n    if len(scores) == 0:\n        return np.array([])\n    \n    mean_s = np.mean(scores)\n    std_s = np.std(scores) + 1e-8\n    normalized = (scores - mean_s) / std_s\n    \n    raw_weights = sigmoid_stable(normalized / temperature)\n    weights = min_weight + (1 - min_weight) * raw_weights\n    \n    return weights\n\n\ndef create_dataloader_with_gids(DO, batch_size=32):\n    \"\"\"Create dataloader that includes global_ids.\"\"\"\n    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n    n_samples = len(DO.df_train_hypothesis)\n    global_ids = torch.arange(n_samples)\n    \n    dataset = TensorDataset(\n        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n        global_ids\n    )\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\ndef evaluate_on_test(DO, model):\n    \"\"\"Evaluate model on test set.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n        test_preds = model(test_inputs)\n        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n        \n        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n    return test_loss, test_mae, r2_score\n\n\ndef train_with_soft_weights(DO, model, sample_weights, partial_gids, partial_weight, lr, n_epochs=200, batch_size=32):\n    \"\"\"Train model with soft weights and validation-based epoch selection.\"\"\"\n    dataloader = create_dataloader_with_gids(DO, batch_size)\n    \n    trainer = WeightedTrainer(DO, model, sample_weights=sample_weights, \n                             partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n    \n    best_val_loss = float('inf')\n    best_epoch = 0\n    best_state = None\n    \n    for epoch in range(n_epochs):\n        trainer.train_epoch(dataloader, epoch, track_data=False)\n        \n        model.eval()\n        with torch.no_grad():\n            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n            val_preds = model(val_inputs)\n            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n        model.train()\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n    \n    model.load_state_dict(best_state)\n    return model, best_epoch, best_val_loss\n\n\nprint(\"GGH functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ggh_anchor_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor and scoring functions defined.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GGH ANCHOR AND SCORING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"Compute gradient-only anchors AND enriched anchors for each class.\"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "print(\"Anchor and scoring functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tabpfn_header",
   "metadata": {},
   "source": [
    "## TabPFN Classifier for Hypothesis Selection\n",
    "\n",
    "Use TabPFN as a classifier to predict which hypothesis class is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tabpfn_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFN functions with diagnostics defined.\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# TABPFN HYPOTHESIS CLASSIFIER WITH DIAGNOSTICS\n# =============================================================================\n\ndef get_tabpfn_probabilities(DO, rand_state, verbose=False):\n    \"\"\"\n    Use TabPFN to predict hypothesis class probabilities for all samples.\n    \n    Returns:\n        tabpfn_probs: dict mapping sample_idx -> array of probabilities for each hypothesis class\n        diagnostics: dict with diagnostic info (classes seen, confidence stats, etc.)\n    \"\"\"\n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    \n    # Get partial data for training TabPFN\n    partial_correct_gids = DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist()\n    \n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    # Prepare training data: input features -> hypothesis class\n    X_train = []\n    y_train = []\n    \n    for gid in partial_correct_gids:\n        row = DO.df_train_hypothesis.iloc[gid]\n        features = row[DO.inpt_vars].values.astype(np.float64)\n        class_id = int(row['hyp_class_id'])\n        X_train.append(features)\n        y_train.append(class_id)\n    \n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    \n    # DIAGNOSTIC: Class coverage\n    unique_classes = np.unique(y_train)\n    class_counts = {c: np.sum(y_train == c) for c in unique_classes}\n    \n    diagnostics = {\n        'n_partial_samples': len(X_train),\n        'classes_seen': unique_classes.tolist(),\n        'n_classes_seen': len(unique_classes),\n        'class_counts': class_counts,\n        'missing_classes': [c for c in range(hyp_per_sample) if c not in unique_classes],\n    }\n    \n    if verbose:\n        print(f\"    TabPFN Training: {len(X_train)} samples, {len(unique_classes)}/{hyp_per_sample} classes\")\n        print(f\"    Classes seen: {unique_classes.tolist()}, Missing: {diagnostics['missing_classes']}\")\n    \n    if len(X_train) < 2:\n        print(\"    Warning: Not enough partial data for TabPFN\")\n        return None, diagnostics\n    \n    # Prepare test data: all non-partial samples\n    X_test = []\n    test_sample_indices = []\n    \n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        gid = sample_idx * hyp_per_sample\n        row = DO.df_train_hypothesis.iloc[gid]\n        features = row[DO.inpt_vars].values.astype(np.float64)\n        X_test.append(features)\n        test_sample_indices.append(sample_idx)\n    \n    X_test = np.array(X_test)\n    \n    if len(X_test) == 0:\n        return {}, diagnostics\n    \n    # Train TabPFN and get probabilities\n    try:\n        tabpfn = TabPFNClassifier(device=DEVICE)\n        tabpfn.fit(X_train, y_train)\n        probs = tabpfn.predict_proba(X_test)\n        \n        # Map to sample indices\n        tabpfn_probs = {}\n        confidence_scores = []\n        \n        for i, sample_idx in enumerate(test_sample_indices):\n            if probs.shape[1] < hyp_per_sample:\n                # Pad with uniform probabilities for missing classes\n                full_probs = np.ones(hyp_per_sample) / hyp_per_sample\n                for j, cls in enumerate(tabpfn.classes_):\n                    full_probs[cls] = probs[i, j]\n                tabpfn_probs[sample_idx] = full_probs\n            else:\n                tabpfn_probs[sample_idx] = probs[i]\n            \n            # Track confidence (max probability)\n            confidence_scores.append(np.max(tabpfn_probs[sample_idx]))\n        \n        # DIAGNOSTIC: Confidence statistics\n        diagnostics['avg_confidence'] = np.mean(confidence_scores)\n        diagnostics['std_confidence'] = np.std(confidence_scores)\n        diagnostics['min_confidence'] = np.min(confidence_scores)\n        diagnostics['max_confidence'] = np.max(confidence_scores)\n        \n        if verbose:\n            print(f\"    TabPFN Confidence: avg={diagnostics['avg_confidence']:.3f}, \"\n                  f\"std={diagnostics['std_confidence']:.3f}, \"\n                  f\"range=[{diagnostics['min_confidence']:.3f}, {diagnostics['max_confidence']:.3f}]\")\n        \n        return tabpfn_probs, diagnostics\n        \n    except Exception as e:\n        print(f\"    TabPFN error: {e}\")\n        diagnostics['error'] = str(e)\n        return None, diagnostics\n\n\ndef analyze_tabpfn_vs_ggh_decisions(DO, tabpfn_probs, ggh_scores, sample_scores, verbose=True):\n    \"\"\"\n    Analyze where TabPFN and GGH agree/disagree.\n    \n    Returns diagnostic dict with agreement/disagreement statistics.\n    \"\"\"\n    hyp_per_sample = DO.num_hyp_comb\n    \n    # For each sample, find TabPFN's top choice vs GGH's top choice\n    agreement_stats = {\n        'total_samples': 0,\n        'both_correct': 0,\n        'both_wrong': 0,\n        'ggh_correct_tabpfn_wrong': 0,\n        'ggh_wrong_tabpfn_correct': 0,\n        'tabpfn_predictions': [],\n        'ggh_predictions': [],\n        'true_classes': [],\n    }\n    \n    for sample_idx, (ggh_score, ggh_gid, ggh_is_correct) in sample_scores.items():\n        if tabpfn_probs is None or sample_idx not in tabpfn_probs:\n            continue\n            \n        agreement_stats['total_samples'] += 1\n        \n        # GGH's choice\n        ggh_class = DO.df_train_hypothesis.iloc[ggh_gid]['hyp_class_id']\n        \n        # TabPFN's choice (argmax of probabilities)\n        tabpfn_class = np.argmax(tabpfn_probs[sample_idx])\n        \n        # True class\n        start = sample_idx * hyp_per_sample\n        true_gid = None\n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                true_gid = gid\n                break\n        \n        if true_gid is None:\n            continue\n            \n        true_class = DO.df_train_hypothesis.iloc[true_gid]['hyp_class_id']\n        \n        tabpfn_correct = (tabpfn_class == true_class)\n        \n        agreement_stats['tabpfn_predictions'].append(tabpfn_class)\n        agreement_stats['ggh_predictions'].append(ggh_class)\n        agreement_stats['true_classes'].append(true_class)\n        \n        if ggh_is_correct and tabpfn_correct:\n            agreement_stats['both_correct'] += 1\n        elif not ggh_is_correct and not tabpfn_correct:\n            agreement_stats['both_wrong'] += 1\n        elif ggh_is_correct and not tabpfn_correct:\n            agreement_stats['ggh_correct_tabpfn_wrong'] += 1\n        else:\n            agreement_stats['ggh_wrong_tabpfn_correct'] += 1\n    \n    if verbose and agreement_stats['total_samples'] > 0:\n        total = agreement_stats['total_samples']\n        print(f\"\\n    === TabPFN vs GGH Agreement Analysis ===\")\n        print(f\"    Both correct:           {agreement_stats['both_correct']:4d} ({agreement_stats['both_correct']/total*100:5.1f}%)\")\n        print(f\"    Both wrong:             {agreement_stats['both_wrong']:4d} ({agreement_stats['both_wrong']/total*100:5.1f}%)\")\n        print(f\"    GGH correct, TabPFN wrong: {agreement_stats['ggh_correct_tabpfn_wrong']:4d} ({agreement_stats['ggh_correct_tabpfn_wrong']/total*100:5.1f}%)\")\n        print(f\"    GGH wrong, TabPFN correct: {agreement_stats['ggh_wrong_tabpfn_correct']:4d} ({agreement_stats['ggh_wrong_tabpfn_correct']/total*100:5.1f}%)\")\n        \n        # TabPFN standalone accuracy\n        tabpfn_accuracy = (agreement_stats['both_correct'] + agreement_stats['ggh_wrong_tabpfn_correct']) / total * 100\n        ggh_accuracy = (agreement_stats['both_correct'] + agreement_stats['ggh_correct_tabpfn_wrong']) / total * 100\n        print(f\"    TabPFN standalone accuracy: {tabpfn_accuracy:.1f}%\")\n        print(f\"    GGH standalone accuracy:    {ggh_accuracy:.1f}%\")\n    \n    return agreement_stats\n\n\nprint(\"TabPFN functions with diagnostics defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fi9wvtwfgtk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TabPFN Imputation Diagnostic: Classification vs Regression\n",
      "======================================================================\n",
      "Partial percentage: 8.0%\n",
      "Random states: 5\n",
      "======================================================================\n",
      "\n",
      "r_state=0: Train=59, Test=689\n",
      "  Classification 2-feat: Acc=30.2%, Conf=0.360\n",
      "  Classification 3-feat: Acc=33.7%, Conf=0.367\n",
      "  Regression 2-feat:     MAE=0.1286, ClassAcc=24.5%\n",
      "  Regression 3-feat:     MAE=0.1249, ClassAcc=27.3%\n",
      "\n",
      "r_state=1: Train=59, Test=689\n",
      "  Classification 2-feat: Acc=29.8%, Conf=0.298\n",
      "  Classification 3-feat: Acc=36.0%, Conf=0.335\n",
      "  Regression 2-feat:     MAE=0.1256, ClassAcc=22.8%\n",
      "  Regression 3-feat:     MAE=0.1209, ClassAcc=26.6%\n",
      "\n",
      "r_state=2: Train=59, Test=689\n",
      "  Classification 2-feat: Acc=26.3%, Conf=0.359\n",
      "  Classification 3-feat: Acc=38.8%, Conf=0.449\n",
      "  Regression 2-feat:     MAE=0.1241, ClassAcc=24.1%\n",
      "  Regression 3-feat:     MAE=0.1115, ClassAcc=27.6%\n",
      "\n",
      "r_state=3: Train=59, Test=689\n",
      "  Classification 2-feat: Acc=29.5%, Conf=0.333\n",
      "  Classification 3-feat: Acc=37.6%, Conf=0.383\n",
      "  Regression 2-feat:     MAE=0.1273, ClassAcc=23.9%\n",
      "  Regression 3-feat:     MAE=0.1201, ClassAcc=28.7%\n",
      "\n",
      "r_state=4: Train=59, Test=689\n",
      "  Classification 2-feat: Acc=23.5%, Conf=0.296\n",
      "  Classification 3-feat: Acc=31.9%, Conf=0.308\n",
      "  Regression 2-feat:     MAE=0.1386, ClassAcc=24.2%\n",
      "  Regression 3-feat:     MAE=0.1187, ClassAcc=28.2%\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: TabPFN Imputation Approaches\n",
      "======================================================================\n",
      "\n",
      "Approach                  |   Accuracy |        MAE | Confidence\n",
      "----------------------------------------------------------------------\n",
      "Classification 2-feat     |      27.8% |        N/A |      0.329\n",
      "Classification 3-feat     |      35.6% |        N/A |      0.368\n",
      "Regression 2-feat         |      23.9% |     0.1288 |        N/A\n",
      "Regression 3-feat         |      27.7% |     0.1192 |        N/A\n",
      "\n",
      ">>> Adding Degradation feature improves classification by: +7.8%\n",
      ">>> Adding Degradation feature improves regression class accuracy by: +3.7%\n",
      "\n",
      ">>> BEST APPROACH: clf_3feat with 35.6% accuracy\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# TABPFN DIAGNOSTIC: Classification vs Regression, 2-feat vs 3-feat\n# =============================================================================\n# Key insight: For imputation, we should use ALL available information including\n# the target variable (Degradation) which is known for each training sample.\n\nfrom tabpfn import TabPFNClassifier, TabPFNRegressor\n\nDIAGNOSTIC_PARTIAL = 0.08  # 8% partial data\nDIAGNOSTIC_RUNS = 5\n\n# Store results\nresults = {\n    'clf_2feat': {'accuracy': [], 'confidence': []},\n    'clf_3feat': {'accuracy': [], 'confidence': []},\n    'reg_2feat': {'mae': [], 'class_accuracy': []},\n    'reg_3feat': {'mae': [], 'class_accuracy': []},\n}\n\nprint(\"=\" * 70)\nprint(\"TabPFN Imputation Diagnostic: Classification vs Regression\")\nprint(\"=\" * 70)\nprint(f\"Partial percentage: {DIAGNOSTIC_PARTIAL*100}%\")\nprint(f\"Random states: {DIAGNOSTIC_RUNS}\")\nprint(\"=\" * 70)\n\n# Hypothesis values for mapping regression to class\nhyp_values = np.array(hypothesis[0])  # [0.03, 0.11, 0.20, 0.32, 0.43, 0.6]\n\nfor r_state in range(DIAGNOSTIC_RUNS):\n    set_to_deterministic(r_state)\n    \n    # Create DataOperator\n    DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                      DIAGNOSTIC_PARTIAL, r_state, device=DEVICE)\n    \n    if DO.lack_partial_coverage:\n        print(f\"  r_state={r_state}: Skipping (lack coverage)\")\n        continue\n    \n    hyp_per_sample = DO.num_hyp_comb\n    \n    # Get partial data (training) and remaining data (test)\n    partial_correct_gids = DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist()\n    \n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    \n    # Build training data from partial samples\n    X_train_2feat = []\n    X_train_3feat = []\n    y_train_class = []\n    y_train_continuous = []\n    \n    for gid in partial_correct_gids:\n        row = DO.df_train_hypothesis.iloc[gid]\n        # 2-feature: just input vars\n        X_train_2feat.append(row[DO.inpt_vars].values.astype(np.float64))\n        # 3-feature: input vars + target (Degradation)\n        feat_3 = np.concatenate([row[DO.inpt_vars].values.astype(np.float64), \n                                  row[DO.target_vars].values.astype(np.float64)])\n        X_train_3feat.append(feat_3)\n        # Class target\n        y_train_class.append(int(row['hyp_class_id']))\n        # Continuous target (actual PCBM value)\n        y_train_continuous.append(float(row['PCBM_hypothesis']))\n    \n    X_train_2feat = np.array(X_train_2feat)\n    X_train_3feat = np.array(X_train_3feat)\n    y_train_class = np.array(y_train_class)\n    y_train_continuous = np.array(y_train_continuous)\n    \n    # Build test data from non-partial samples (use correct hypothesis)\n    X_test_2feat = []\n    X_test_3feat = []\n    y_test_class = []\n    y_test_continuous = []\n    \n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        # Find the correct hypothesis for this sample\n        start = sample_idx * hyp_per_sample\n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                row = DO.df_train_hypothesis.iloc[gid]\n                X_test_2feat.append(row[DO.inpt_vars].values.astype(np.float64))\n                feat_3 = np.concatenate([row[DO.inpt_vars].values.astype(np.float64), \n                                          row[DO.target_vars].values.astype(np.float64)])\n                X_test_3feat.append(feat_3)\n                y_test_class.append(int(row['hyp_class_id']))\n                y_test_continuous.append(float(row['PCBM_hypothesis']))\n                break\n    \n    X_test_2feat = np.array(X_test_2feat)\n    X_test_3feat = np.array(X_test_3feat)\n    y_test_class = np.array(y_test_class)\n    y_test_continuous = np.array(y_test_continuous)\n    \n    print(f\"\\nr_state={r_state}: Train={len(X_train_2feat)}, Test={len(X_test_2feat)}\")\n    \n    # === 1. Classification with 2 features ===\n    try:\n        clf_2 = TabPFNClassifier(device=DEVICE)\n        clf_2.fit(X_train_2feat, y_train_class)\n        pred_2 = clf_2.predict(X_test_2feat)\n        probs_2 = clf_2.predict_proba(X_test_2feat)\n        acc_2 = np.mean(pred_2 == y_test_class) * 100\n        conf_2 = np.mean(np.max(probs_2, axis=1))\n        results['clf_2feat']['accuracy'].append(acc_2)\n        results['clf_2feat']['confidence'].append(conf_2)\n        print(f\"  Classification 2-feat: Acc={acc_2:.1f}%, Conf={conf_2:.3f}\")\n    except Exception as e:\n        print(f\"  Classification 2-feat: ERROR - {e}\")\n    \n    # === 2. Classification with 3 features (+ Degradation) ===\n    try:\n        clf_3 = TabPFNClassifier(device=DEVICE)\n        clf_3.fit(X_train_3feat, y_train_class)\n        pred_3 = clf_3.predict(X_test_3feat)\n        probs_3 = clf_3.predict_proba(X_test_3feat)\n        acc_3 = np.mean(pred_3 == y_test_class) * 100\n        conf_3 = np.mean(np.max(probs_3, axis=1))\n        results['clf_3feat']['accuracy'].append(acc_3)\n        results['clf_3feat']['confidence'].append(conf_3)\n        print(f\"  Classification 3-feat: Acc={acc_3:.1f}%, Conf={conf_3:.3f}\")\n    except Exception as e:\n        print(f\"  Classification 3-feat: ERROR - {e}\")\n    \n    # === 3. Regression with 2 features ===\n    try:\n        reg_2 = TabPFNRegressor(device=DEVICE)\n        reg_2.fit(X_train_2feat, y_train_continuous)\n        pred_reg_2 = reg_2.predict(X_test_2feat)\n        mae_2 = np.mean(np.abs(pred_reg_2 - y_test_continuous))\n        # Map to nearest hypothesis class\n        pred_class_2 = np.array([np.argmin(np.abs(hyp_values - p)) for p in pred_reg_2])\n        class_acc_2 = np.mean(pred_class_2 == y_test_class) * 100\n        results['reg_2feat']['mae'].append(mae_2)\n        results['reg_2feat']['class_accuracy'].append(class_acc_2)\n        print(f\"  Regression 2-feat:     MAE={mae_2:.4f}, ClassAcc={class_acc_2:.1f}%\")\n    except Exception as e:\n        print(f\"  Regression 2-feat: ERROR - {e}\")\n    \n    # === 4. Regression with 3 features (+ Degradation) ===\n    try:\n        reg_3 = TabPFNRegressor(device=DEVICE)\n        reg_3.fit(X_train_3feat, y_train_continuous)\n        pred_reg_3 = reg_3.predict(X_test_3feat)\n        mae_3 = np.mean(np.abs(pred_reg_3 - y_test_continuous))\n        # Map to nearest hypothesis class\n        pred_class_3 = np.array([np.argmin(np.abs(hyp_values - p)) for p in pred_reg_3])\n        class_acc_3 = np.mean(pred_class_3 == y_test_class) * 100\n        results['reg_3feat']['mae'].append(mae_3)\n        results['reg_3feat']['class_accuracy'].append(class_acc_3)\n        print(f\"  Regression 3-feat:     MAE={mae_3:.4f}, ClassAcc={class_acc_3:.1f}%\")\n    except Exception as e:\n        print(f\"  Regression 3-feat: ERROR - {e}\")\n\n# === Summary ===\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SUMMARY: TabPFN Imputation Approaches\")\nprint(\"=\" * 70)\n\nprint(f\"\\n{'Approach':<25} | {'Accuracy':>10} | {'MAE':>10} | {'Confidence':>10}\")\nprint(\"-\" * 70)\n\nif results['clf_2feat']['accuracy']:\n    acc = np.mean(results['clf_2feat']['accuracy'])\n    conf = np.mean(results['clf_2feat']['confidence'])\n    print(f\"{'Classification 2-feat':<25} | {acc:>9.1f}% | {'N/A':>10} | {conf:>10.3f}\")\n\nif results['clf_3feat']['accuracy']:\n    acc = np.mean(results['clf_3feat']['accuracy'])\n    conf = np.mean(results['clf_3feat']['confidence'])\n    print(f\"{'Classification 3-feat':<25} | {acc:>9.1f}% | {'N/A':>10} | {conf:>10.3f}\")\n\nif results['reg_2feat']['mae']:\n    mae = np.mean(results['reg_2feat']['mae'])\n    acc = np.mean(results['reg_2feat']['class_accuracy'])\n    print(f\"{'Regression 2-feat':<25} | {acc:>9.1f}% | {mae:>10.4f} | {'N/A':>10}\")\n\nif results['reg_3feat']['mae']:\n    mae = np.mean(results['reg_3feat']['mae'])\n    acc = np.mean(results['reg_3feat']['class_accuracy'])\n    print(f\"{'Regression 3-feat':<25} | {acc:>9.1f}% | {mae:>10.4f} | {'N/A':>10}\")\n\n# Improvement analysis\nif results['clf_2feat']['accuracy'] and results['clf_3feat']['accuracy']:\n    improvement = np.mean(results['clf_3feat']['accuracy']) - np.mean(results['clf_2feat']['accuracy'])\n    print(f\"\\n>>> Adding Degradation feature improves classification by: {improvement:+.1f}%\")\n\nif results['reg_2feat']['class_accuracy'] and results['reg_3feat']['class_accuracy']:\n    improvement = np.mean(results['reg_3feat']['class_accuracy']) - np.mean(results['reg_2feat']['class_accuracy'])\n    print(f\">>> Adding Degradation feature improves regression class accuracy by: {improvement:+.1f}%\")\n\n# Best approach\nall_accs = {\n    'clf_2feat': np.mean(results['clf_2feat']['accuracy']) if results['clf_2feat']['accuracy'] else 0,\n    'clf_3feat': np.mean(results['clf_3feat']['accuracy']) if results['clf_3feat']['accuracy'] else 0,\n    'reg_2feat': np.mean(results['reg_2feat']['class_accuracy']) if results['reg_2feat']['class_accuracy'] else 0,\n    'reg_3feat': np.mean(results['reg_3feat']['class_accuracy']) if results['reg_3feat']['class_accuracy'] else 0,\n}\nbest = max(all_accs, key=all_accs.get)\nprint(f\"\\n>>> BEST APPROACH: {best} with {all_accs[best]:.1f}% accuracy\")"
  },
  {
   "cell_type": "markdown",
   "id": "fusion_header",
   "metadata": {},
   "source": [
    "## Option A: TabPFN + GGH Fusion\n",
    "\n",
    "Combine TabPFN probabilities with GGH gradient scores:\n",
    "\n",
    "```\n",
    "final_score = ggh_score + alpha * log(tabpfn_prob)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fusion_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion function with confidence gating defined.\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# TABPFN + GGH FUSION (OPTION A) WITH CONFIDENCE GATING\n# =============================================================================\n\ndef run_ggh_with_tabpfn_prior(DO, rand_state, alpha=1.0, confidence_threshold=0.4, verbose_diagnostics=True):\n    \"\"\"\n    GGH Soft Refinement with TabPFN probabilities as prior.\n    \n    CONFIDENCE-GATED: Only applies TabPFN when max(tabpfn_prob) > threshold.\n    Otherwise falls back to pure GGH.\n    \n    final_score = ggh_score + alpha * log(tabpfn_prob)  [if confident]\n    final_score = ggh_score                              [if not confident]\n    \n    Returns: gid_weights, effective_precision, partial_gids, partial_weight_dynamic, diagnostics\n    \"\"\"\n    set_to_deterministic(rand_state)\n    \n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    n_shared = len(DO.inpt_vars)\n    n_hyp = len(DO.miss_vars)\n    out_size = len(DO.target_vars)\n    \n    partial_correct_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist())\n    blacklisted_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n    ].index.tolist())\n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n    \n    # === Get TabPFN probabilities with diagnostics ===\n    print(\"    Getting TabPFN probabilities...\")\n    tabpfn_probs, tabpfn_diagnostics = get_tabpfn_probabilities(DO, rand_state, verbose=verbose_diagnostics)\n    \n    # === ITERATION 1: Unbiased training + Initial soft weights ===\n    print(\"    Iter1: Unbiased training...\")\n    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n                                               MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n    \n    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n    \n    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n    analysis = trainer_unbiased.get_hypothesis_analysis()\n    input_cols = anchor_data['input_cols']\n    \n    # === SCORING WITH CONFIDENCE-GATED TABPFN PRIOR ===\n    ggh_only_scores = {}  # For diagnostic comparison\n    sample_scores = {}\n    \n    # Track gating statistics\n    n_samples_with_fusion = 0\n    n_samples_pure_ggh = 0\n    \n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        start = sample_idx * hyp_per_sample\n        best_ggh_score, best_ggh_gid, best_ggh_is_correct = -np.inf, None, False\n        best_combined_score, best_combined_gid, best_combined_is_correct = -np.inf, None, False\n        \n        # Get TabPFN probabilities for this sample\n        if tabpfn_probs and sample_idx in tabpfn_probs:\n            sample_tabpfn_probs = tabpfn_probs[sample_idx]\n            max_tabpfn_confidence = np.max(sample_tabpfn_probs)\n        else:\n            sample_tabpfn_probs = np.ones(hyp_per_sample) / hyp_per_sample\n            max_tabpfn_confidence = 1.0 / hyp_per_sample  # Uniform = low confidence\n        \n        # Determine if we should use TabPFN for this sample\n        use_tabpfn = (max_tabpfn_confidence > confidence_threshold)\n        \n        if use_tabpfn:\n            n_samples_with_fusion += 1\n        else:\n            n_samples_pure_ggh += 1\n        \n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n                continue\n            \n            gradient = analysis[gid]['avg_gradient']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n            is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n            \n            # GGH gradient score (pure)\n            ggh_score = compute_enriched_score(gradient, features, class_id, anchor_data)\n            \n            # Track best GGH-only choice\n            if ggh_score > best_ggh_score:\n                best_ggh_score = ggh_score\n                best_ggh_gid = gid\n                best_ggh_is_correct = is_correct\n            \n            # Combined score: CONFIDENCE-GATED\n            if use_tabpfn:\n                # TabPFN is confident - apply fusion\n                tabpfn_prob = sample_tabpfn_probs[class_id]\n                tabpfn_log_prob = np.log(tabpfn_prob + 1e-10)\n                combined_score = ggh_score + alpha * tabpfn_log_prob\n            else:\n                # TabPFN is not confident - use pure GGH\n                combined_score = ggh_score\n            \n            if combined_score > best_combined_score:\n                best_combined_score = combined_score\n                best_combined_gid = gid\n                best_combined_is_correct = is_correct\n        \n        if best_ggh_gid is not None:\n            ggh_only_scores[sample_idx] = (best_ggh_score, best_ggh_gid, best_ggh_is_correct)\n        if best_combined_gid is not None:\n            sample_scores[sample_idx] = (best_combined_score, best_combined_gid, best_combined_is_correct)\n    \n    # === DIAGNOSTIC: Gating statistics ===\n    total_samples_scored = n_samples_with_fusion + n_samples_pure_ggh\n    if verbose_diagnostics and total_samples_scored > 0:\n        print(f\"\\n    === Confidence Gating (threshold={confidence_threshold}) ===\")\n        print(f\"    Samples using TabPFN+GGH fusion: {n_samples_with_fusion:4d} ({n_samples_with_fusion/total_samples_scored*100:5.1f}%)\")\n        print(f\"    Samples using pure GGH:          {n_samples_pure_ggh:4d} ({n_samples_pure_ggh/total_samples_scored*100:5.1f}%)\")\n    \n    # === DIAGNOSTIC: Analyze GGH vs TabPFN agreement ===\n    if verbose_diagnostics:\n        agreement_stats = analyze_tabpfn_vs_ggh_decisions(DO, tabpfn_probs, None, ggh_only_scores, verbose=True)\n        \n        # Check: when fusion flips GGH's decision, is it helpful or harmful?\n        fusion_flips_to_correct = 0\n        fusion_flips_to_wrong = 0\n        fusion_agrees_with_ggh = 0\n        \n        for sample_idx in ggh_only_scores:\n            if sample_idx not in sample_scores:\n                continue\n            \n            _, ggh_gid, ggh_correct = ggh_only_scores[sample_idx]\n            _, fusion_gid, fusion_correct = sample_scores[sample_idx]\n            \n            if ggh_gid == fusion_gid:\n                fusion_agrees_with_ggh += 1\n            elif not ggh_correct and fusion_correct:\n                fusion_flips_to_correct += 1\n            elif ggh_correct and not fusion_correct:\n                fusion_flips_to_wrong += 1\n        \n        total_comparable = len(ggh_only_scores)\n        print(f\"\\n    === Fusion Impact Analysis ===\")\n        print(f\"    Fusion agrees with GGH:      {fusion_agrees_with_ggh:4d} ({fusion_agrees_with_ggh/total_comparable*100:5.1f}%)\")\n        print(f\"    Fusion flips GGH wrong\u2192correct: {fusion_flips_to_correct:4d} ({fusion_flips_to_correct/total_comparable*100:5.1f}%) [GOOD]\")\n        print(f\"    Fusion flips GGH correct\u2192wrong: {fusion_flips_to_wrong:4d} ({fusion_flips_to_wrong/total_comparable*100:5.1f}%) [BAD]\")\n        \n        net_benefit = fusion_flips_to_correct - fusion_flips_to_wrong\n        print(f\"    Net benefit of fusion: {net_benefit:+d} samples\")\n    \n    # Convert to soft weights\n    scores_list = [s[0] for s in sample_scores.values()]\n    weights_iter1 = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n    \n    gid_weights = {}\n    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n        gid_weights[gid] = float(weights_iter1[i])\n    \n    iter1_correct = sum(1 for s in sample_scores.values() if s[2])\n    iter1_precision = iter1_correct / len(sample_scores) * 100 if sample_scores else 0\n    \n    print(f\"    Iter1+TabPFN: {len(sample_scores)} samples, precision: {iter1_precision:.1f}%\")\n    \n    # === ITERATION 2: Weighted training ===\n    print(\"    Iter2: Weighted training...\")\n    set_to_deterministic(rand_state + 100)\n    model_weighted = HypothesisAmplifyingModel(n_shared, n_hyp,\n                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n                                               MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n    \n    trainer_weighted = WeightedTrainer(DO, model_weighted, sample_weights=gid_weights,\n                                       partial_gids=partial_correct_gids,\n                                       partial_weight=GGH_PARTIAL_BASE_WEIGHT, lr=GGH_ITER2_LR)\n    \n    for epoch in range(GGH_ITER2_EPOCHS):\n        trainer_weighted.train_epoch(dataloader, epoch)\n    \n    # === ITERATION 3: Biased rescoring -> Multiply weights ===\n    print(\"    Iter3: Biased rescoring...\")\n    selected_sample_indices = set(sample_scores.keys())\n    scorer = RemainingDataScorer(DO, model_weighted, selected_sample_indices | partial_sample_indices)\n    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n    biased_analysis = scorer.get_analysis()\n    \n    # Build biased anchor data\n    anchor_data_biased = {\n        'anchor_correct_grad': {},\n        'anchor_incorrect_grad': {},\n        'anchor_correct_enriched': {},\n        'anchor_incorrect_enriched': {},\n        'feature_norm_params': {},\n        'loss_norm_params': {},\n    }\n    \n    all_grads = [biased_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n                 if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None]\n    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n    anchor_data_biased['grad_scale'] = grad_scale\n    \n    inpt_vars_list = DO.inpt_vars\n    \n    for class_id in range(hyp_per_sample):\n        correct_grads, incorrect_grads = [], []\n        correct_features, incorrect_features = [], []\n        correct_losses, incorrect_losses = [], []\n        \n        for gid in partial_correct_gids:\n            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n                if biased_analysis[gid]['avg_gradient'] is not None:\n                    correct_grads.append(biased_analysis[gid]['avg_gradient'])\n                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n                    correct_losses.append(biased_analysis[gid]['avg_loss'])\n        \n        for gid in blacklisted_gids:\n            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n                if biased_analysis[gid]['avg_gradient'] is not None:\n                    incorrect_grads.append(biased_analysis[gid]['avg_gradient'])\n                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n                    incorrect_losses.append(biased_analysis[gid]['avg_loss'])\n        \n        if correct_grads and incorrect_grads:\n            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n            \n            all_features = correct_features + incorrect_features\n            feat_mean = np.mean(all_features, axis=0)\n            feat_std = np.std(all_features, axis=0) + 1e-8\n            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n            \n            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n            \n            all_losses = correct_losses + incorrect_losses\n            loss_mean = np.mean(all_losses)\n            loss_std = np.std(all_losses) + 1e-8\n            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n            \n            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n            \n            correct_enriched = [np.concatenate([g, f, [l]]) \n                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n            incorrect_enriched = [np.concatenate([g, f, [l]]) \n                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n            \n            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n    \n    # Rescore with biased model (NO TabPFN in Iter3 - just gradient-based)\n    iter3_scores = {}\n    for sample_idx, (_, gid, _) in sample_scores.items():\n        if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None:\n            gradient = biased_analysis[gid]['avg_gradient']\n            loss = biased_analysis[gid]['avg_loss']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n            \n            norm_params = anchor_data_biased.get('feature_norm_params', {}).get(class_id)\n            loss_params = anchor_data_biased.get('loss_norm_params', {}).get(class_id)\n            \n            if norm_params:\n                features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n            else:\n                features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n            \n            if loss_params:\n                loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n            else:\n                loss_norm = -loss * grad_scale\n            \n            enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n            \n            anchor_c = anchor_data_biased.get('anchor_correct_enriched', {}).get(class_id)\n            anchor_i = anchor_data_biased.get('anchor_incorrect_enriched', {}).get(class_id)\n            \n            if anchor_c is not None:\n                sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n                sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n                iter3_scores[gid] = sim_c - sim_i\n            else:\n                iter3_scores[gid] = 0.0\n    \n    scores_list_iter3 = list(iter3_scores.values())\n    gids_iter3 = list(iter3_scores.keys())\n    weights_iter3_raw = compute_soft_weights(scores_list_iter3, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER3)\n    \n    for i, gid in enumerate(gids_iter3):\n        gid_weights[gid] = gid_weights[gid] * weights_iter3_raw[i]\n    \n    if gid_weights:\n        max_w = max(gid_weights.values())\n        if max_w > 0:\n            for gid in gid_weights:\n                gid_weights[gid] = GGH_MIN_WEIGHT + (gid_weights[gid] / max_w) * (1 - GGH_MIN_WEIGHT)\n    \n    # === ITERATION 4: Loss-based adjustment ===\n    losses = {gid: biased_analysis[gid]['avg_loss']\n              for gid in gid_weights if gid in biased_analysis}\n    \n    if losses:\n        loss_values = list(losses.values())\n        loss_mean = np.mean(loss_values)\n        loss_std = np.std(loss_values) + 1e-8\n        \n        for gid in gid_weights:\n            if gid in losses:\n                norm_loss = (losses[gid] - loss_mean) / loss_std\n                loss_factor = 1 - GGH_LOSS_INFLUENCE * sigmoid_stable(norm_loss)\n                gid_weights[gid] = max(GGH_MIN_WEIGHT, gid_weights[gid] * loss_factor)\n    \n    # Calculate effective precision\n    correct_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n    total_weight_correct = sum(correct_weights_final)\n    total_weight_all = sum(gid_weights.values())\n    effective_precision = total_weight_correct / total_weight_all * 100 if total_weight_all > 0 else 0\n    \n    print(f\"    Final effective precision: {effective_precision:.1f}%\")\n    \n    avg_final_weight = np.mean(list(gid_weights.values())) if gid_weights else 0.5\n    partial_weight_dynamic = GGH_PARTIAL_BASE_WEIGHT * (1 + (1 - avg_final_weight))\n    \n    # Collect all diagnostics\n    all_diagnostics = {\n        'tabpfn': tabpfn_diagnostics,\n        'iter1_precision': iter1_precision,\n        'effective_precision': effective_precision,\n        'n_samples_with_fusion': n_samples_with_fusion,\n        'n_samples_pure_ggh': n_samples_pure_ggh,\n        'fusion_ratio': n_samples_with_fusion / total_samples_scored if total_samples_scored > 0 else 0,\n    }\n    \n    return gid_weights, effective_precision, partial_correct_gids, partial_weight_dynamic, all_diagnostics\n\n\nprint(\"Fusion function with confidence gating defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "standard_ggh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard GGH function defined (full 4 iterations).\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# STANDARD GGH (WITHOUT TABPFN) FOR COMPARISON - FULL 4 ITERATIONS\n# =============================================================================\n\ndef run_ggh_soft_refinement(DO, rand_state):\n    \"\"\"\n    Standard GGH soft refinement (without TabPFN prior).\n    Full 4-iteration implementation matching Photocell_Benchmark.\n    \"\"\"\n    set_to_deterministic(rand_state)\n    \n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    n_shared = len(DO.inpt_vars)\n    n_hyp = len(DO.miss_vars)\n    out_size = len(DO.target_vars)\n    \n    partial_correct_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist())\n    blacklisted_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n    ].index.tolist())\n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n    \n    # === ITERATION 1: Unbiased training + Initial soft weights ===\n    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n                                               MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n    \n    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n    \n    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n    analysis = trainer_unbiased.get_hypothesis_analysis()\n    input_cols = anchor_data['input_cols']\n    \n    # Standard scoring (no TabPFN)\n    sample_scores = {}\n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        start = sample_idx * hyp_per_sample\n        best_score, best_gid, best_is_correct = -np.inf, None, False\n        \n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n                continue\n            \n            gradient = analysis[gid]['avg_gradient']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n            \n            if score > best_score:\n                best_score = score\n                best_gid = gid\n                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n        \n        if best_gid is not None:\n            sample_scores[sample_idx] = (best_score, best_gid, best_is_correct)\n    \n    # Convert to soft weights\n    scores_list = [s[0] for s in sample_scores.values()]\n    weights_iter1 = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n    \n    gid_weights = {}\n    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n        gid_weights[gid] = float(weights_iter1[i])\n    \n    iter1_correct = sum(1 for s in sample_scores.values() if s[2])\n    iter1_precision = iter1_correct / len(sample_scores) * 100 if sample_scores else 0\n    \n    # === ITERATION 2: Weighted training ===\n    set_to_deterministic(rand_state + 100)\n    model_weighted = HypothesisAmplifyingModel(n_shared, n_hyp,\n                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n                                               MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n    \n    trainer_weighted = WeightedTrainer(DO, model_weighted, sample_weights=gid_weights,\n                                       partial_gids=partial_correct_gids,\n                                       partial_weight=GGH_PARTIAL_BASE_WEIGHT, lr=GGH_ITER2_LR)\n    \n    for epoch in range(GGH_ITER2_EPOCHS):\n        trainer_weighted.train_epoch(dataloader, epoch)\n    \n    # === ITERATION 3: Biased rescoring -> Multiply weights ===\n    selected_sample_indices = set(sample_scores.keys())\n    scorer = RemainingDataScorer(DO, model_weighted, selected_sample_indices | partial_sample_indices)\n    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n    biased_analysis = scorer.get_analysis()\n    \n    # Build biased anchor data\n    anchor_data_biased = {\n        'anchor_correct_grad': {},\n        'anchor_incorrect_grad': {},\n        'anchor_correct_enriched': {},\n        'anchor_incorrect_enriched': {},\n        'feature_norm_params': {},\n        'loss_norm_params': {},\n    }\n    \n    all_grads = [biased_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n                 if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None]\n    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n    anchor_data_biased['grad_scale'] = grad_scale\n    \n    inpt_vars_list = DO.inpt_vars\n    \n    for class_id in range(hyp_per_sample):\n        correct_grads, incorrect_grads = [], []\n        correct_features, incorrect_features = [], []\n        correct_losses, incorrect_losses = [], []\n        \n        for gid in partial_correct_gids:\n            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n                if biased_analysis[gid]['avg_gradient'] is not None:\n                    correct_grads.append(biased_analysis[gid]['avg_gradient'])\n                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n                    correct_losses.append(biased_analysis[gid]['avg_loss'])\n        \n        for gid in blacklisted_gids:\n            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n                if biased_analysis[gid]['avg_gradient'] is not None:\n                    incorrect_grads.append(biased_analysis[gid]['avg_gradient'])\n                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n                    incorrect_losses.append(biased_analysis[gid]['avg_loss'])\n        \n        if correct_grads and incorrect_grads:\n            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n            \n            all_features = correct_features + incorrect_features\n            feat_mean = np.mean(all_features, axis=0)\n            feat_std = np.std(all_features, axis=0) + 1e-8\n            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n            \n            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n            \n            all_losses = correct_losses + incorrect_losses\n            loss_mean = np.mean(all_losses)\n            loss_std = np.std(all_losses) + 1e-8\n            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n            \n            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n            \n            correct_enriched = [np.concatenate([g, f, [l]]) \n                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n            incorrect_enriched = [np.concatenate([g, f, [l]]) \n                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n            \n            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n    \n    # Rescore with biased model\n    iter3_scores = {}\n    for sample_idx, (_, gid, _) in sample_scores.items():\n        if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None:\n            gradient = biased_analysis[gid]['avg_gradient']\n            loss = biased_analysis[gid]['avg_loss']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n            \n            norm_params = anchor_data_biased.get('feature_norm_params', {}).get(class_id)\n            loss_params = anchor_data_biased.get('loss_norm_params', {}).get(class_id)\n            \n            if norm_params:\n                features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n            else:\n                features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n            \n            if loss_params:\n                loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n            else:\n                loss_norm = -loss * grad_scale\n            \n            enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n            \n            anchor_c = anchor_data_biased.get('anchor_correct_enriched', {}).get(class_id)\n            anchor_i = anchor_data_biased.get('anchor_incorrect_enriched', {}).get(class_id)\n            \n            if anchor_c is not None:\n                sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n                sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n                iter3_scores[gid] = sim_c - sim_i\n            else:\n                iter3_scores[gid] = 0.0\n    \n    # Multiply weights\n    scores_list_iter3 = list(iter3_scores.values())\n    gids_iter3 = list(iter3_scores.keys())\n    weights_iter3_raw = compute_soft_weights(scores_list_iter3, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER3)\n    \n    for i, gid in enumerate(gids_iter3):\n        gid_weights[gid] = gid_weights[gid] * weights_iter3_raw[i]\n    \n    # Renormalize\n    if gid_weights:\n        max_w = max(gid_weights.values())\n        if max_w > 0:\n            for gid in gid_weights:\n                gid_weights[gid] = GGH_MIN_WEIGHT + (gid_weights[gid] / max_w) * (1 - GGH_MIN_WEIGHT)\n    \n    # === ITERATION 4: Loss-based adjustment ===\n    losses = {gid: biased_analysis[gid]['avg_loss']\n              for gid in gid_weights if gid in biased_analysis}\n    \n    if losses:\n        loss_values = list(losses.values())\n        loss_mean = np.mean(loss_values)\n        loss_std = np.std(loss_values) + 1e-8\n        \n        for gid in gid_weights:\n            if gid in losses:\n                norm_loss = (losses[gid] - loss_mean) / loss_std\n                loss_factor = 1 - GGH_LOSS_INFLUENCE * sigmoid_stable(norm_loss)\n                gid_weights[gid] = max(GGH_MIN_WEIGHT, gid_weights[gid] * loss_factor)\n    \n    # Calculate effective precision\n    correct_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n    total_weight_correct = sum(correct_weights_final) if correct_weights_final else 0\n    total_weight_all = sum(gid_weights.values()) if gid_weights else 1\n    effective_precision = total_weight_correct / total_weight_all * 100 if total_weight_all > 0 else 0\n    \n    avg_final_weight = np.mean(list(gid_weights.values())) if gid_weights else 0.5\n    partial_weight_dynamic = GGH_PARTIAL_BASE_WEIGHT * (1 + (1 - avg_final_weight))\n    \n    return gid_weights, effective_precision, partial_correct_gids, partial_weight_dynamic\n\n\nprint(\"Standard GGH function defined (full 4 iterations).\")"
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## Benchmark: GGH vs TabPFN+GGH Fusion\n",
    "\n",
    "Compare across multiple partial percentages to see where each method excels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH vs TabPFN+GGH Fusion (Confidence-Gated)\n",
      "================================================================================\n",
      "Dataset: Photocell Degradation\n",
      "Partial percentages: [0.03, 0.1, 0.25]\n",
      "Runs per percentage: 3\n",
      "Fusion alpha: 1.0\n",
      "Confidence threshold: 0.4\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "PARTIAL PERCENTAGE: 3.0%\n",
      "============================================================\n",
      "\n",
      "Run 1/3 (r_state=0)\n",
      "  Running Standard GGH...\n",
      "    GGH R2: 0.4798, Precision: 31.7%\n",
      "  Running TabPFN+GGH Fusion (Confidence-Gated)...\n",
      "    Getting TabPFN probabilities...\n",
      "    TabPFN Training: 22 samples, 6/6 classes\n",
      "    Classes seen: [0, 1, 2, 3, 4, 5], Missing: []\n",
      "    TabPFN Confidence: avg=0.338, std=0.069, range=[0.200, 0.497]\n",
      "    Iter1: Unbiased training...\n",
      "\n",
      "    === Confidence Gating (threshold=0.4) ===\n",
      "    Samples using TabPFN+GGH fusion:  148 ( 20.4%)\n",
      "    Samples using pure GGH:           578 ( 79.6%)\n",
      "\n",
      "    === TabPFN vs GGH Agreement Analysis ===\n",
      "    Both correct:            110 ( 15.2%)\n",
      "    Both wrong:              453 ( 62.4%)\n",
      "    GGH correct, TabPFN wrong:   87 ( 12.0%)\n",
      "    GGH wrong, TabPFN correct:   76 ( 10.5%)\n",
      "    TabPFN standalone accuracy: 25.6%\n",
      "    GGH standalone accuracy:    27.1%\n",
      "\n",
      "    === Fusion Impact Analysis ===\n",
      "    Fusion agrees with GGH:       694 ( 95.6%)\n",
      "    Fusion flips GGH wrong\u2192correct:   20 (  2.8%) [GOOD]\n",
      "    Fusion flips GGH correct\u2192wrong:    6 (  0.8%) [BAD]\n",
      "    Net benefit of fusion: +14 samples\n",
      "    Iter1+TabPFN: 726 samples, precision: 29.1%\n",
      "    Iter2: Weighted training...\n",
      "    Iter3: Biased rescoring...\n",
      "    Final effective precision: 30.2%\n",
      "    Fusion R2: 0.4009, Precision: 30.2%\n",
      "  Running TabPFN Standalone...\n",
      "    TabPFN R2: 0.1211\n",
      "  Running Partial Only...\n",
      "    Partial R2: 0.4732\n",
      "  Running Full Info...\n",
      "    Full Info R2: 0.5752\n",
      "  >>> Fusion vs GGH: -0.0789\n",
      "\n",
      "Run 2/3 (r_state=1)\n",
      "  Running Standard GGH...\n"
     ]
    }
   ],
   "source": "# =============================================================================\n# BENCHMARK EXECUTION WITH CONFIDENCE-GATED FUSION\n# =============================================================================\n\nprint(\"=\" * 80)\nprint(\"BENCHMARK: GGH vs TabPFN+GGH Fusion (Confidence-Gated)\")\nprint(\"=\" * 80)\nprint(f\"Dataset: Photocell Degradation\")\nprint(f\"Partial percentages: {PARTIAL_PERCENTAGES}\")\nprint(f\"Runs per percentage: {BENCHMARK_N_RUNS}\")\nprint(f\"Fusion alpha: {FUSION_ALPHA}\")\nprint(f\"Confidence threshold: {CONFIDENCE_THRESHOLD}\")\nprint(\"=\" * 80)\n\nall_results = {partial_perc: {'GGH': [], 'TabPFN': [], 'TabPFN+GGH': [], 'Partial': [], 'Full Info': []} \n               for partial_perc in PARTIAL_PERCENTAGES}\n\n# Store aggregated diagnostics\nall_diagnostics = {partial_perc: [] for partial_perc in PARTIAL_PERCENTAGES}\n\nfor partial_perc in PARTIAL_PERCENTAGES:\n    print(f\"\\n{'='*60}\")\n    print(f\"PARTIAL PERCENTAGE: {partial_perc*100}%\")\n    print(f\"{'='*60}\")\n    \n    valid_runs = 0\n    r_state = 0\n    \n    while valid_runs < BENCHMARK_N_RUNS and r_state < 2000:\n        set_to_deterministic(r_state)\n        \n        # Create DataOperator\n        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                          partial_perc, r_state, device=DEVICE)\n        \n        if DO.lack_partial_coverage:\n            r_state += 1\n            continue\n        \n        print(f\"\\nRun {valid_runs + 1}/{BENCHMARK_N_RUNS} (r_state={r_state})\")\n        \n        n_shared = len(DO.inpt_vars)\n        n_hyp = len(DO.miss_vars)\n        out_size = len(DO.target_vars)\n        \n        partial_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n        ].index.tolist())\n        \n        # === Standard GGH ===\n        print(\"  Running Standard GGH...\")\n        ggh_weights, ggh_precision, _, ggh_partial_weight = run_ggh_soft_refinement(DO, r_state)\n        \n        set_to_deterministic(r_state + 200)\n        model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_ggh, _, _ = train_with_soft_weights(\n            DO, model_ggh, sample_weights=ggh_weights, partial_gids=partial_gids,\n            partial_weight=ggh_partial_weight, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n        )\n        _, _, ggh_r2 = evaluate_on_test(DO, model_ggh)\n        print(f\"    GGH R2: {ggh_r2:.4f}, Precision: {ggh_precision:.1f}%\")\n        \n        # === TabPFN + GGH Fusion (Confidence-Gated) ===\n        print(\"  Running TabPFN+GGH Fusion (Confidence-Gated)...\")\n        verbose = (valid_runs == 0)  # Only verbose for first run\n        fusion_result = run_ggh_with_tabpfn_prior(\n            DO, r_state, \n            alpha=FUSION_ALPHA, \n            confidence_threshold=CONFIDENCE_THRESHOLD,\n            verbose_diagnostics=verbose\n        )\n        fusion_weights, fusion_precision, _, fusion_partial_weight, diagnostics = fusion_result\n        \n        if verbose:\n            all_diagnostics[partial_perc].append(diagnostics)\n        \n        set_to_deterministic(r_state + 300)\n        model_fusion = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                                 MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_fusion, _, _ = train_with_soft_weights(\n            DO, model_fusion, sample_weights=fusion_weights, partial_gids=partial_gids,\n            partial_weight=fusion_partial_weight, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n        )\n        _, _, fusion_r2 = evaluate_on_test(DO, model_fusion)\n        print(f\"    Fusion R2: {fusion_r2:.4f}, Precision: {fusion_precision:.1f}%\")\n        \n        # === TabPFN Standalone ===\n        print(\"  Running TabPFN Standalone...\")\n        # Get TabPFN probabilities\n        tabpfn_probs, _ = get_tabpfn_probabilities(DO, r_state, verbose=False)\n        \n        # Create TabPFN-only weights (hard assignment)\n        tabpfn_weights = {}\n        hyp_per_sample = DO.num_hyp_comb\n        n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n        partial_sample_indices = set(gid // hyp_per_sample for gid in partial_gids)\n        \n        for sample_idx in range(n_samples):\n            if sample_idx in partial_sample_indices:\n                continue\n            if sample_idx in tabpfn_probs:\n                pred_class = np.argmax(tabpfn_probs[sample_idx])\n                gid = sample_idx * hyp_per_sample + pred_class\n                tabpfn_weights[gid] = 1.0\n        \n        set_to_deterministic(r_state + 500)\n        model_tabpfn = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_tabpfn, _, _ = train_with_soft_weights(DO, model_tabpfn, tabpfn_weights, partial_gids,\n                                                      GGH_PARTIAL_BASE_WEIGHT, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n        _, _, tabpfn_r2 = evaluate_on_test(DO, model_tabpfn)\n        print(f\"    TabPFN R2: {tabpfn_r2:.4f}\")\n        \n        # === Partial Only ===\n        print(\"  Running Partial Only...\")\n        set_to_deterministic(r_state + 400)\n        model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_partial, _, _ = train_with_soft_weights(\n            DO, model_partial, sample_weights={}, partial_gids=partial_gids,\n            partial_weight=1.0, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n        )\n        _, _, partial_r2 = evaluate_on_test(DO, model_partial)\n        print(f\"    Partial R2: {partial_r2:.4f}\")\n        \n        # === Full Info (Fair - same architecture as GGH/Fusion) ===\n        print(\"  Running Full Info...\")\n        hyp_per_sample = len(DO.hypothesis)\n        n_samples_full = len(DO.df_train_hypothesis) // hyp_per_sample\n        full_info_weights = {}\n        for sample_idx in range(n_samples_full):\n            for hyp_idx in range(hyp_per_sample):\n                gid = sample_idx * hyp_per_sample + hyp_idx\n                if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                    full_info_weights[gid] = 1.0\n        \n        set_to_deterministic(r_state + 600)\n        model_full = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                               MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_full, _, _ = train_with_soft_weights(DO, model_full, full_info_weights, set(),\n                                                    1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n        _, _, full_r2 = evaluate_on_test(DO, model_full)\n        print(f\"    Full Info R2: {full_r2:.4f}\")\n        \n        # Store results\n        all_results[partial_perc]['GGH'].append({'r2': ggh_r2, 'precision': ggh_precision})\n        all_results[partial_perc]['TabPFN'].append({'r2': tabpfn_r2})\n        all_results[partial_perc]['TabPFN+GGH'].append({'r2': fusion_r2, 'precision': fusion_precision})\n        all_results[partial_perc]['Partial'].append({'r2': partial_r2})\n        all_results[partial_perc]['Full Info'].append({'r2': full_r2})\n        \n        print(f\"  >>> Fusion vs GGH: {fusion_r2 - ggh_r2:+.4f}\")\n        \n        valid_runs += 1\n        r_state += 1\n\nprint(f\"\\n{'='*80}\")\nprint(\"BENCHMARK COMPLETE\")\nprint(f\"{'='*80}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: GGH vs TabPFN+GGH Fusion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for partial_perc in PARTIAL_PERCENTAGES:\n",
    "    ggh_r2s = [r['r2'] for r in all_results[partial_perc]['GGH']]\n",
    "    tabpfn_r2s = [r['r2'] for r in all_results[partial_perc]['TabPFN']]\n",
    "    fusion_r2s = [r['r2'] for r in all_results[partial_perc]['TabPFN+GGH']]\n",
    "    partial_r2s = [r['r2'] for r in all_results[partial_perc]['Partial']]\n",
    "    full_r2s = [r['r2'] for r in all_results[partial_perc]['Full Info']]\n",
    "    \n",
    "    ggh_prec = [r['precision'] for r in all_results[partial_perc]['GGH']]\n",
    "    fusion_prec = [r['precision'] for r in all_results[partial_perc]['TabPFN+GGH']]\n",
    "    \n",
    "    if ggh_r2s and fusion_r2s:\n",
    "        t_stat, p_val = stats.ttest_rel(fusion_r2s, ggh_r2s)\n",
    "        diff = np.mean(fusion_r2s) - np.mean(ggh_r2s)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Partial %': f\"{partial_perc*100:.0f}%\",\n",
    "            'Full Info': f\"{np.mean(full_r2s):.4f}\",\n",
    "            'GGH R2': f\"{np.mean(ggh_r2s):.4f} \u00b1 {np.std(ggh_r2s):.4f}\",\n",
    "            'Fusion R2': f\"{np.mean(fusion_r2s):.4f} \u00b1 {np.std(fusion_r2s):.4f}\",\n",
    "            'Partial R2': f\"{np.mean(partial_r2s):.4f}\",\n",
    "            '\u0394 (Fusion-GGH)': f\"{diff:+.4f}\",\n",
    "            'p-value': f\"{p_val:.4f}\",\n",
    "            'Winner': 'Fusion' if diff > 0 and p_val < 0.05 else ('GGH' if diff < 0 and p_val < 0.05 else 'Tie'),\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nPartial {partial_perc*100:.0f}%:\")\n",
    "        print(f\"  Full Info:    {np.mean(full_r2s):.4f}\")\n",
    "        print(f\"  TabPFN:       {np.mean(tabpfn_r2s):.4f} \u00b1 {np.std(tabpfn_r2s):.4f}\")\n",
    "        print(f\"  GGH:          {np.mean(ggh_r2s):.4f} \u00b1 {np.std(ggh_r2s):.4f} (precision: {np.mean(ggh_prec):.1f}%)\")\n",
    "        print(f\"  TabPFN+GGH:   {np.mean(fusion_r2s):.4f} \u00b1 {np.std(fusion_r2s):.4f} (precision: {np.mean(fusion_prec):.1f}%)\")\n",
    "        print(f\"  Partial:      {np.mean(partial_r2s):.4f}\")\n",
    "        print(f\"  >>> Fusion vs GGH: {diff:+.4f} (p={p_val:.4f})\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Summary Table:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: R2 by Partial Percentage\n",
    "ax1 = axes[0]\n",
    "\n",
    "partial_pcts = [p * 100 for p in PARTIAL_PERCENTAGES]\n",
    "ggh_means = [np.mean([r['r2'] for r in all_results[p]['GGH']]) for p in PARTIAL_PERCENTAGES]\n",
    "ggh_stds = [np.std([r['r2'] for r in all_results[p]['GGH']]) for p in PARTIAL_PERCENTAGES]\n",
    "fusion_means = [np.mean([r['r2'] for r in all_results[p]['TabPFN+GGH']]) for p in PARTIAL_PERCENTAGES]\n",
    "fusion_stds = [np.std([r['r2'] for r in all_results[p]['TabPFN+GGH']]) for p in PARTIAL_PERCENTAGES]\n",
    "partial_means = [np.mean([r['r2'] for r in all_results[p]['Partial']]) for p in PARTIAL_PERCENTAGES]\n",
    "full_means = [np.mean([r['r2'] for r in all_results[p]['Full Info']]) for p in PARTIAL_PERCENTAGES]\n",
    "\n",
    "ax1.errorbar(partial_pcts, ggh_means, yerr=ggh_stds, marker='o', label='GGH', capsize=5, linewidth=2)\n",
    "ax1.errorbar(partial_pcts, fusion_means, yerr=fusion_stds, marker='s', label='TabPFN+GGH Fusion', capsize=5, linewidth=2)\n",
    "ax1.plot(partial_pcts, partial_means, marker='^', label='Partial Only', linestyle='--', alpha=0.7)\n",
    "ax1.plot(partial_pcts, full_means, marker='d', label='Full Info (Oracle)', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Partial Data (%)', fontsize=12)\n",
    "ax1.set_ylabel('Test R2 Score', fontsize=12)\n",
    "ax1.set_title('Test R2 by Partial Percentage', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Improvement (Fusion - GGH)\n",
    "ax2 = axes[1]\n",
    "\n",
    "improvements = [np.mean([r['r2'] for r in all_results[p]['TabPFN+GGH']]) - \n",
    "                np.mean([r['r2'] for r in all_results[p]['GGH']]) \n",
    "                for p in PARTIAL_PERCENTAGES]\n",
    "\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "bars = ax2.bar(partial_pcts, improvements, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax2.set_xlabel('Partial Data (%)', fontsize=12)\n",
    "ax2.set_ylabel('R2 Improvement (Fusion - GGH)', fontsize=12)\n",
    "ax2.set_title('TabPFN+GGH Fusion Improvement over GGH', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, improvements):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002 * np.sign(val), \n",
    "             f'{val:+.4f}', ha='center', va='bottom' if val > 0 else 'top', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_vs_fusion_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# CONCLUSION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Find crossover point\n",
    "fusion_wins = sum(1 for imp in improvements if imp > 0)\n",
    "ggh_wins = sum(1 for imp in improvements if imp < 0)\n",
    "\n",
    "print(f\"\\nAcross {len(PARTIAL_PERCENTAGES)} partial percentages tested:\")\n",
    "print(f\"  TabPFN+GGH Fusion wins: {fusion_wins}\")\n",
    "print(f\"  Standard GGH wins: {ggh_wins}\")\n",
    "print(f\"  Ties: {len(PARTIAL_PERCENTAGES) - fusion_wins - ggh_wins}\")\n",
    "\n",
    "# Identify regimes\n",
    "if len([imp for imp in improvements[:2] if imp < 0]) > 0:\n",
    "    print(f\"\\n>>> At low partial data: GGH may still be better (gradient signal dominates)\")\n",
    "if len([imp for imp in improvements[-2:] if imp > 0]) > 0:\n",
    "    print(f\">>> At high partial data: Fusion benefits from TabPFN's pattern recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_cell",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# AIRFOIL BENCHMARK: GGH vs TabPFN (5-feat) vs Fusion\n# =============================================================================\n# TabPFN uses ALL available information: 4 input features + 1 target (sound pressure)\n# to predict the missing variable (chord-length class)\n\nprint(\"=\" * 80)\nprint(\"AIRFOIL BENCHMARK: GGH vs TabPFN (5-feat) vs Fusion\")\nprint(\"=\" * 80)\n\n# Airfoil configuration\nAIRFOIL_DATA_PATH = '../data/airfoil_self_noise/data.csv'\nAIRFOIL_RESULTS_PATH = \"../saved_results/Airfoil_TabPFN_Fusion\"\nAIRFOIL_INPT_VARS = ['frequency', 'attack-angle', 'free-stream-velocity', 'suction-side-displacement-thickness']\nAIRFOIL_TARGET_VARS = ['scaled-sound-pressure']\nAIRFOIL_MISS_VARS = ['chord-length']\nAIRFOIL_HYPOTHESIS = [[0.0254, 0.0508, 0.1016, 0.1524, 0.2286, 0.3048]]\nAIRFOIL_HYP_VALUES = np.array(AIRFOIL_HYPOTHESIS[0])\n\n# Benchmark parameters\nAIRFOIL_PARTIAL_PERCENTAGES = [0.03, 0.08]\nAIRFOIL_N_RUNS = 8\nAIRFOIL_EPOCHS = 500\nAIRFOIL_FUSION_ALPHA = 1.0\nAIRFOIL_CONFIDENCE_THRESHOLD = 0.4\n\n# GGH parameters for Airfoil\nAIRFOIL_GGH_ITER1_EPOCHS = 60\nAIRFOIL_GGH_ITER1_ANALYSIS_EPOCHS = 5\nAIRFOIL_GGH_ITER2_EPOCHS = 30\nAIRFOIL_GGH_FINAL_EPOCHS = 200\n\nos.makedirs(AIRFOIL_RESULTS_PATH, exist_ok=True)\n\nprint(f\"Input features: {AIRFOIL_INPT_VARS}\")\nprint(f\"Target: {AIRFOIL_TARGET_VARS}\")\nprint(f\"Missing: {AIRFOIL_MISS_VARS}\")\nprint(f\"Hypothesis values: {AIRFOIL_HYPOTHESIS[0]}\")\nprint(f\"Partial percentages: {AIRFOIL_PARTIAL_PERCENTAGES}\")\nprint(f\"Runs per percentage: {AIRFOIL_N_RUNS}\")\nprint(\"=\" * 80)\n\n# Store results\nairfoil_results = {p: {'GGH': [], 'TabPFN': [], 'Fusion': [], 'Partial': [], 'Full': []} \n                   for p in AIRFOIL_PARTIAL_PERCENTAGES}\n\ndef get_tabpfn_5feat_probs(DO, rand_state, verbose=False):\n    \"\"\"TabPFN with 5 features: 4 inputs + 1 target (sound pressure).\"\"\"\n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    \n    # Get partial data\n    partial_correct_gids = DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist()\n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    # Build training data with 5 features\n    X_train = []\n    y_train = []\n    \n    for gid in partial_correct_gids:\n        row = DO.df_train_hypothesis.iloc[gid]\n        # 5 features: 4 inputs + 1 target\n        features = np.concatenate([\n            row[DO.inpt_vars].values.astype(np.float64),\n            row[DO.target_vars].values.astype(np.float64)\n        ])\n        X_train.append(features)\n        y_train.append(int(row['hyp_class_id']))\n    \n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    \n    unique_classes = np.unique(y_train)\n    \n    diagnostics = {\n        'n_partial': len(X_train),\n        'n_classes': len(unique_classes),\n        'classes': unique_classes.tolist(),\n    }\n    \n    if verbose:\n        print(f\"    TabPFN 5-feat: {len(X_train)} samples, {len(unique_classes)}/6 classes\")\n    \n    if len(X_train) < 2:\n        return None, diagnostics\n    \n    # Build test data\n    X_test = []\n    test_sample_indices = []\n    \n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        # Use the first hypothesis row to get input features (same for all hypotheses)\n        gid = sample_idx * hyp_per_sample\n        row = DO.df_train_hypothesis.iloc[gid]\n        \n        # Find the correct hypothesis to get the actual target value\n        correct_gid = None\n        for hyp_idx in range(hyp_per_sample):\n            test_gid = sample_idx * hyp_per_sample + hyp_idx\n            if DO.df_train_hypothesis.iloc[test_gid]['correct_hypothesis']:\n                correct_gid = test_gid\n                break\n        \n        if correct_gid is None:\n            continue\n            \n        correct_row = DO.df_train_hypothesis.iloc[correct_gid]\n        features = np.concatenate([\n            correct_row[DO.inpt_vars].values.astype(np.float64),\n            correct_row[DO.target_vars].values.astype(np.float64)\n        ])\n        X_test.append(features)\n        test_sample_indices.append(sample_idx)\n    \n    X_test = np.array(X_test)\n    \n    if len(X_test) == 0:\n        return {}, diagnostics\n    \n    # Train TabPFN\n    try:\n        tabpfn = TabPFNClassifier(device=DEVICE)\n        tabpfn.fit(X_train, y_train)\n        probs = tabpfn.predict_proba(X_test)\n        \n        tabpfn_probs = {}\n        confidence_scores = []\n        \n        for i, sample_idx in enumerate(test_sample_indices):\n            if probs.shape[1] < hyp_per_sample:\n                full_probs = np.ones(hyp_per_sample) / hyp_per_sample\n                for j, cls in enumerate(tabpfn.classes_):\n                    full_probs[cls] = probs[i, j]\n                tabpfn_probs[sample_idx] = full_probs\n            else:\n                tabpfn_probs[sample_idx] = probs[i]\n            confidence_scores.append(np.max(tabpfn_probs[sample_idx]))\n        \n        diagnostics['avg_confidence'] = np.mean(confidence_scores)\n        diagnostics['predictions'] = tabpfn.predict(X_test)\n        diagnostics['test_indices'] = test_sample_indices\n        \n        if verbose:\n            print(f\"    TabPFN Confidence: avg={diagnostics['avg_confidence']:.3f}\")\n        \n        return tabpfn_probs, diagnostics\n        \n    except Exception as e:\n        print(f\"    TabPFN error: {e}\")\n        return None, diagnostics\n\n\ndef run_airfoil_ggh(DO, rand_state):\n    \"\"\"Run GGH soft refinement for Airfoil.\"\"\"\n    set_to_deterministic(rand_state)\n    \n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    n_shared = len(DO.inpt_vars)\n    n_hyp = len(DO.miss_vars)\n    out_size = len(DO.target_vars)\n    \n    partial_correct_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist())\n    blacklisted_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n    ].index.tolist())\n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n    \n    # Iteration 1\n    model = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                      MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n    trainer = UnbiasedTrainer(DO, model, lr=GGH_ITER1_LR)\n    \n    for epoch in range(AIRFOIL_GGH_ITER1_EPOCHS - AIRFOIL_GGH_ITER1_ANALYSIS_EPOCHS):\n        trainer.train_epoch(dataloader, epoch, track_data=False)\n    for epoch in range(AIRFOIL_GGH_ITER1_EPOCHS - AIRFOIL_GGH_ITER1_ANALYSIS_EPOCHS, AIRFOIL_GGH_ITER1_EPOCHS):\n        trainer.train_epoch(dataloader, epoch, track_data=True)\n    \n    anchor_data = compute_anchor_data(trainer, DO)\n    analysis = trainer.get_hypothesis_analysis()\n    input_cols = anchor_data['input_cols']\n    \n    # Scoring\n    sample_scores = {}\n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        start = sample_idx * hyp_per_sample\n        best_score, best_gid, best_is_correct = -np.inf, None, False\n        \n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n                continue\n            \n            gradient = analysis[gid]['avg_gradient']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n            \n            if score > best_score:\n                best_score = score\n                best_gid = gid\n                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n        \n        if best_gid is not None:\n            sample_scores[sample_idx] = (best_score, best_gid, best_is_correct)\n    \n    # Soft weights\n    scores_list = [s[0] for s in sample_scores.values()]\n    weights = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n    \n    gid_weights = {}\n    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n        gid_weights[gid] = float(weights[i])\n    \n    precision = sum(1 for s in sample_scores.values() if s[2]) / len(sample_scores) * 100 if sample_scores else 0\n    \n    return gid_weights, precision, partial_correct_gids, sample_scores\n\n\ndef run_airfoil_fusion(DO, rand_state, tabpfn_probs, alpha=1.0, threshold=0.4):\n    \"\"\"Run GGH with TabPFN prior for Airfoil.\"\"\"\n    set_to_deterministic(rand_state)\n    \n    hyp_per_sample = DO.num_hyp_comb\n    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n    n_shared = len(DO.inpt_vars)\n    n_hyp = len(DO.miss_vars)\n    out_size = len(DO.target_vars)\n    \n    partial_correct_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n    ].index.tolist())\n    blacklisted_gids = set(DO.df_train_hypothesis[\n        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n    ].index.tolist())\n    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n    \n    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n    \n    # Iteration 1\n    model = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                      MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n    trainer = UnbiasedTrainer(DO, model, lr=GGH_ITER1_LR)\n    \n    for epoch in range(AIRFOIL_GGH_ITER1_EPOCHS - AIRFOIL_GGH_ITER1_ANALYSIS_EPOCHS):\n        trainer.train_epoch(dataloader, epoch, track_data=False)\n    for epoch in range(AIRFOIL_GGH_ITER1_EPOCHS - AIRFOIL_GGH_ITER1_ANALYSIS_EPOCHS, AIRFOIL_GGH_ITER1_EPOCHS):\n        trainer.train_epoch(dataloader, epoch, track_data=True)\n    \n    anchor_data = compute_anchor_data(trainer, DO)\n    analysis = trainer.get_hypothesis_analysis()\n    input_cols = anchor_data['input_cols']\n    \n    # Scoring with TabPFN prior (confidence-gated)\n    sample_scores = {}\n    n_fusion = 0\n    n_pure_ggh = 0\n    \n    for sample_idx in range(n_samples):\n        if sample_idx in partial_sample_indices:\n            continue\n        \n        start = sample_idx * hyp_per_sample\n        best_score, best_gid, best_is_correct = -np.inf, None, False\n        \n        # Get TabPFN probs\n        if tabpfn_probs and sample_idx in tabpfn_probs:\n            sample_probs = tabpfn_probs[sample_idx]\n            max_conf = np.max(sample_probs)\n        else:\n            sample_probs = np.ones(hyp_per_sample) / hyp_per_sample\n            max_conf = 1.0 / hyp_per_sample\n        \n        use_tabpfn = (max_conf > threshold)\n        if use_tabpfn:\n            n_fusion += 1\n        else:\n            n_pure_ggh += 1\n        \n        for hyp_idx in range(hyp_per_sample):\n            gid = start + hyp_idx\n            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n                continue\n            \n            gradient = analysis[gid]['avg_gradient']\n            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n            ggh_score = compute_enriched_score(gradient, features, class_id, anchor_data)\n            \n            if use_tabpfn:\n                tabpfn_log = np.log(sample_probs[class_id] + 1e-10)\n                score = ggh_score + alpha * tabpfn_log\n            else:\n                score = ggh_score\n            \n            if score > best_score:\n                best_score = score\n                best_gid = gid\n                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n        \n        if best_gid is not None:\n            sample_scores[sample_idx] = (best_score, best_gid, best_is_correct)\n    \n    # Soft weights\n    scores_list = [s[0] for s in sample_scores.values()]\n    weights = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n    \n    gid_weights = {}\n    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n        gid_weights[gid] = float(weights[i])\n    \n    precision = sum(1 for s in sample_scores.values() if s[2]) / len(sample_scores) * 100 if sample_scores else 0\n    \n    return gid_weights, precision, partial_correct_gids, n_fusion, n_pure_ggh\n\n\n# Run benchmark\nfor partial_perc in AIRFOIL_PARTIAL_PERCENTAGES:\n    print(f\"\\n{'='*60}\")\n    print(f\"PARTIAL: {partial_perc*100}%\")\n    print(f\"{'='*60}\")\n    \n    valid_runs = 0\n    r_state = 0\n    \n    while valid_runs < AIRFOIL_N_RUNS and r_state < 500:\n        set_to_deterministic(r_state)\n        \n        DO = DataOperator(AIRFOIL_DATA_PATH, AIRFOIL_INPT_VARS, AIRFOIL_TARGET_VARS, \n                          AIRFOIL_MISS_VARS, AIRFOIL_HYPOTHESIS, partial_perc, r_state, device=DEVICE)\n        \n        if DO.lack_partial_coverage:\n            r_state += 1\n            continue\n        \n        print(f\"\\nRun {valid_runs+1}/{AIRFOIL_N_RUNS} (r_state={r_state})\")\n        \n        n_shared = len(DO.inpt_vars)\n        n_hyp = len(DO.miss_vars)\n        out_size = len(DO.target_vars)\n        hyp_per_sample = DO.num_hyp_comb\n        \n        partial_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n        ].index.tolist())\n        \n        # === TabPFN 5-feat (standalone) ===\n        verbose = (valid_runs == 0)\n        tabpfn_probs, tabpfn_diag = get_tabpfn_5feat_probs(DO, r_state, verbose=verbose)\n        \n        # Compute TabPFN standalone accuracy\n        tabpfn_acc = 0\n        if tabpfn_probs and 'test_indices' in tabpfn_diag:\n            correct = 0\n            total = 0\n            for i, sample_idx in enumerate(tabpfn_diag['test_indices']):\n                pred_class = np.argmax(tabpfn_probs[sample_idx])\n                # Find true class\n                for hyp_idx in range(hyp_per_sample):\n                    gid = sample_idx * hyp_per_sample + hyp_idx\n                    if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                        true_class = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n                        if pred_class == true_class:\n                            correct += 1\n                        total += 1\n                        break\n            tabpfn_acc = correct / total * 100 if total > 0 else 0\n        \n        print(f\"  TabPFN 5-feat accuracy: {tabpfn_acc:.1f}%\")\n        \n        # === GGH standalone ===\n        ggh_weights, ggh_prec, _, ggh_scores = run_airfoil_ggh(DO, r_state)\n        \n        set_to_deterministic(r_state + 200)\n        model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_ggh, _, _ = train_with_soft_weights(DO, model_ggh, ggh_weights, partial_gids,\n                                                   GGH_PARTIAL_BASE_WEIGHT, GGH_BENCHMARK_LR, \n                                                   AIRFOIL_GGH_FINAL_EPOCHS)\n        _, _, ggh_r2 = evaluate_on_test(DO, model_ggh)\n        print(f\"  GGH: R2={ggh_r2:.4f}, Precision={ggh_prec:.1f}%\")\n        \n        # === Fusion (GGH + TabPFN) ===\n        fusion_weights, fusion_prec, _, n_fus, n_pure = run_airfoil_fusion(\n            DO, r_state, tabpfn_probs, AIRFOIL_FUSION_ALPHA, AIRFOIL_CONFIDENCE_THRESHOLD)\n        \n        set_to_deterministic(r_state + 300)\n        model_fusion = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                                 MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_fusion, _, _ = train_with_soft_weights(DO, model_fusion, fusion_weights, partial_gids,\n                                                      GGH_PARTIAL_BASE_WEIGHT, GGH_BENCHMARK_LR, \n                                                      AIRFOIL_GGH_FINAL_EPOCHS)\n        _, _, fusion_r2 = evaluate_on_test(DO, model_fusion)\n        print(f\"  Fusion: R2={fusion_r2:.4f}, Precision={fusion_prec:.1f}% (fusion:{n_fus}, pure:{n_pure})\")\n        \n        # === Partial Only ===\n        set_to_deterministic(r_state + 400)\n        model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_partial, _, _ = train_with_soft_weights(DO, model_partial, {}, partial_gids,\n                                                       1.0, GGH_BENCHMARK_LR, AIRFOIL_GGH_FINAL_EPOCHS)\n        _, _, partial_r2 = evaluate_on_test(DO, model_partial)\n        \n        # === Full Info (Fair - same architecture as GGH/Fusion) ===\n        # Get all correct hypothesis GIDs with weight=1\n        n_samples_full = len(DO.df_train_hypothesis) // hyp_per_sample\n        full_info_weights = {}\n        for sample_idx in range(n_samples_full):\n            for hyp_idx in range(hyp_per_sample):\n                gid = sample_idx * hyp_per_sample + hyp_idx\n                if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                    full_info_weights[gid] = 1.0\n        \n        set_to_deterministic(r_state + 600)\n        model_full = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                                MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_full, _, _ = train_with_soft_weights(DO, model_full, full_info_weights, partial_gids,\n                                                    GGH_PARTIAL_BASE_WEIGHT, GGH_BENCHMARK_LR, AIRFOIL_GGH_FINAL_EPOCHS)\n        _, _, full_r2 = evaluate_on_test(DO, model_full)\n        \n        # Store\n        airfoil_results[partial_perc]['GGH'].append({'r2': ggh_r2, 'precision': ggh_prec})\n        airfoil_results[partial_perc]['TabPFN'].append({'accuracy': tabpfn_acc})\n        airfoil_results[partial_perc]['Fusion'].append({'r2': fusion_r2, 'precision': fusion_prec})\n        airfoil_results[partial_perc]['Partial'].append({'r2': partial_r2})\n        airfoil_results[partial_perc]['Full'].append({'r2': full_r2})\n        \n        print(f\"  Full Info: R2={full_r2:.4f}\")\n        print(f\"  >>> Fusion vs GGH: {fusion_r2 - ggh_r2:+.4f}, Fusion vs Full: {fusion_r2 - full_r2:+.4f}\")\n        \n        valid_runs += 1\n        r_state += 1\n\n# Summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"AIRFOIL BENCHMARK SUMMARY\")\nprint(\"=\" * 80)\n\nfor partial_perc in AIRFOIL_PARTIAL_PERCENTAGES:\n    ggh_r2s = [r['r2'] for r in airfoil_results[partial_perc]['GGH']]\n    fusion_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Fusion']]\n    tabpfn_accs = [r['accuracy'] for r in airfoil_results[partial_perc]['TabPFN']]\n    partial_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Partial']]\n    full_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Full']]\n    \n    ggh_prec = [r['precision'] for r in airfoil_results[partial_perc]['GGH']]\n    fusion_prec = [r['precision'] for r in airfoil_results[partial_perc]['Fusion']]\n    \n    print(f\"\\nPartial {partial_perc*100}%:\")\n    print(f\"  Full Info:         {np.mean(full_r2s):.4f} \u00b1 {np.std(full_r2s):.4f}\")\n    print(f\"  GGH:               {np.mean(ggh_r2s):.4f} \u00b1 {np.std(ggh_r2s):.4f} (prec: {np.mean(ggh_prec):.1f}%)\")\n    print(f\"  Fusion:            {np.mean(fusion_r2s):.4f} \u00b1 {np.std(fusion_r2s):.4f} (prec: {np.mean(fusion_prec):.1f}%)\")\n    print(f\"  TabPFN 5-feat acc: {np.mean(tabpfn_accs):.1f}%\")\n    print(f\"  Partial Only:      {np.mean(partial_r2s):.4f}\")\n    \n    diff_fusion_ggh = np.mean(fusion_r2s) - np.mean(ggh_r2s)\n    diff_fusion_full = np.mean(fusion_r2s) - np.mean(full_r2s)\n    _, p_fusion_ggh = stats.ttest_rel(fusion_r2s, ggh_r2s) if len(fusion_r2s) > 1 else (0, 1)\n    _, p_fusion_full = stats.ttest_rel(fusion_r2s, full_r2s) if len(fusion_r2s) > 1 else (0, 1)\n    print(f\"  >>> Fusion vs GGH:  {diff_fusion_ggh:+.4f} (p={p_fusion_ggh:.4f})\")\n    print(f\"  >>> Fusion vs Full: {diff_fusion_full:+.4f} (p={p_fusion_full:.4f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "etgrliacz3t",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TABPFN STANDALONE R2 - Complete Comparison\n# =============================================================================\n# Train a model using TabPFN's hard predictions (no GGH) to get TabPFN standalone R2\n\nprint(\"=\" * 80)\nprint(\"TABPFN STANDALONE R2 COMPARISON\")\nprint(\"=\" * 80)\nprint(\"Using stored TabPFN predictions to train models and compute R2\")\nprint(\"=\" * 80)\n\n# Re-run with TabPFN standalone R2\ntabpfn_r2_results = {p: [] for p in AIRFOIL_PARTIAL_PERCENTAGES}\n\nfor partial_perc in AIRFOIL_PARTIAL_PERCENTAGES:\n    print(f\"\\n{'='*60}\")\n    print(f\"PARTIAL: {partial_perc*100}% - TabPFN Standalone R2\")\n    print(f\"{'='*60}\")\n    \n    valid_runs = 0\n    r_state = 0\n    \n    while valid_runs < AIRFOIL_N_RUNS and r_state < 500:\n        set_to_deterministic(r_state)\n        \n        DO = DataOperator(AIRFOIL_DATA_PATH, AIRFOIL_INPT_VARS, AIRFOIL_TARGET_VARS, \n                          AIRFOIL_MISS_VARS, AIRFOIL_HYPOTHESIS, partial_perc, r_state, device=DEVICE)\n        \n        if DO.lack_partial_coverage:\n            r_state += 1\n            continue\n        \n        n_shared = len(DO.inpt_vars)\n        n_hyp = len(DO.miss_vars)\n        out_size = len(DO.target_vars)\n        hyp_per_sample = DO.num_hyp_comb\n        \n        partial_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n        ].index.tolist())\n        \n        # Get TabPFN predictions\n        tabpfn_probs, tabpfn_diag = get_tabpfn_5feat_probs(DO, r_state, verbose=False)\n        \n        if tabpfn_probs is None or 'test_indices' not in tabpfn_diag:\n            print(f\"  r_state={r_state}: TabPFN failed, skipping\")\n            r_state += 1\n            continue\n        \n        # Create TabPFN-only weights (hard assignment: weight=1 for predicted class)\n        tabpfn_weights = {}\n        n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n        partial_sample_indices = set(gid // hyp_per_sample for gid in partial_gids)\n        \n        for sample_idx in tabpfn_diag['test_indices']:\n            if sample_idx in partial_sample_indices:\n                continue\n            \n            # Get TabPFN's predicted class\n            pred_class = np.argmax(tabpfn_probs[sample_idx])\n            \n            # Find the gid for this predicted class\n            gid = sample_idx * hyp_per_sample + pred_class\n            tabpfn_weights[gid] = 1.0  # Hard assignment\n        \n        # Train model with TabPFN predictions\n        set_to_deterministic(r_state + 500)\n        model_tabpfn = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_tabpfn, _, _ = train_with_soft_weights(DO, model_tabpfn, tabpfn_weights, partial_gids,\n                                                      GGH_PARTIAL_BASE_WEIGHT, GGH_BENCHMARK_LR, \n                                                      AIRFOIL_GGH_FINAL_EPOCHS)\n        _, _, tabpfn_r2 = evaluate_on_test(DO, model_tabpfn)\n        \n        tabpfn_r2_results[partial_perc].append(tabpfn_r2)\n        print(f\"  Run {valid_runs+1}/{AIRFOIL_N_RUNS} (r_state={r_state}): TabPFN R2={tabpfn_r2:.4f}\")\n        \n        valid_runs += 1\n        r_state += 1\n\n# Final comparison summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPLETE COMPARISON: GGH vs TabPFN vs Fusion\")\nprint(\"=\" * 80)\n\nfor partial_perc in AIRFOIL_PARTIAL_PERCENTAGES:\n    ggh_r2s = [r['r2'] for r in airfoil_results[partial_perc]['GGH']]\n    fusion_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Fusion']]\n    tabpfn_r2s = tabpfn_r2_results[partial_perc]\n    tabpfn_accs = [r['accuracy'] for r in airfoil_results[partial_perc]['TabPFN']]\n    full_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Full']]\n    partial_r2s = [r['r2'] for r in airfoil_results[partial_perc]['Partial']]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"PARTIAL {partial_perc*100}%\")\n    print(f\"{'='*60}\")\n    print(f\"{'Method':<20} | {'R2':>12} | {'vs Full Info':>12}\")\n    print(\"-\" * 50)\n    print(f\"{'Full Info (oracle)':<20} | {np.mean(full_r2s):>12.4f} | {'-':>12}\")\n    print(f\"{'Partial Only':<20} | {np.mean(partial_r2s):>12.4f} | {np.mean(partial_r2s)-np.mean(full_r2s):>+12.4f}\")\n    print(f\"{'GGH':<20} | {np.mean(ggh_r2s):>12.4f} | {np.mean(ggh_r2s)-np.mean(full_r2s):>+12.4f}\")\n    print(f\"{'TabPFN (5-feat)':<20} | {np.mean(tabpfn_r2s):>12.4f} | {np.mean(tabpfn_r2s)-np.mean(full_r2s):>+12.4f}\")\n    print(f\"{'Fusion (GGH+TabPFN)':<20} | {np.mean(fusion_r2s):>12.4f} | {np.mean(fusion_r2s)-np.mean(full_r2s):>+12.4f}\")\n    \n    print(f\"\\nTabPFN classification accuracy: {np.mean(tabpfn_accs):.1f}%\")\n    \n    # Statistical tests\n    if len(tabpfn_r2s) > 1 and len(ggh_r2s) > 1:\n        _, p_tabpfn_ggh = stats.ttest_rel(tabpfn_r2s, ggh_r2s)\n        _, p_fusion_tabpfn = stats.ttest_rel(fusion_r2s, tabpfn_r2s)\n        _, p_fusion_ggh = stats.ttest_rel(fusion_r2s, ggh_r2s)\n        \n        print(f\"\\nStatistical significance (paired t-test):\")\n        print(f\"  TabPFN vs GGH:   {np.mean(tabpfn_r2s)-np.mean(ggh_r2s):+.4f} (p={p_tabpfn_ggh:.4f})\")\n        print(f\"  Fusion vs GGH:   {np.mean(fusion_r2s)-np.mean(ggh_r2s):+.4f} (p={p_fusion_ggh:.4f})\")\n        print(f\"  Fusion vs TabPFN: {np.mean(fusion_r2s)-np.mean(tabpfn_r2s):+.4f} (p={p_fusion_tabpfn:.4f})\")\n\n# Best method identification\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CONCLUSION\")\nprint(\"=\" * 80)\nfor partial_perc in AIRFOIL_PARTIAL_PERCENTAGES:\n    ggh_mean = np.mean([r['r2'] for r in airfoil_results[partial_perc]['GGH']])\n    fusion_mean = np.mean([r['r2'] for r in airfoil_results[partial_perc]['Fusion']])\n    tabpfn_mean = np.mean(tabpfn_r2_results[partial_perc])\n    full_mean = np.mean([r['r2'] for r in airfoil_results[partial_perc]['Full']])\n    \n    methods = {'GGH': ggh_mean, 'TabPFN': tabpfn_mean, 'Fusion': fusion_mean}\n    best = max(methods, key=methods.get)\n    \n    print(f\"\\nPartial {partial_perc*100}%:\")\n    print(f\"  Best method: {best} (R2={methods[best]:.4f})\")\n    if methods[best] > full_mean:\n        print(f\"  >>> EXCEEDS Full Info by {methods[best]-full_mean:+.4f}!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f7dcb-86c5-4417-9a76-183e1d991d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial_vs_full_diagnostic",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DIAGNOSTIC: Partial Only vs Full Info (Verify Partial improves with more data)\n# =============================================================================\n\nDIAG_PARTIAL_PERCENTAGES = [0.03, 0.10, 0.25]\nDIAG_N_RUNS = 10\n\nprint(\"=\" * 80)\nprint(\"DIAGNOSTIC: Partial Only vs Full Info\")\nprint(\"=\" * 80)\nprint(f\"Partial percentages: {DIAG_PARTIAL_PERCENTAGES}\")\nprint(f\"Runs per percentage: {DIAG_N_RUNS}\")\nprint(\"=\" * 80)\n\ndiag_results = {p: {'Partial': [], 'Full Info': [], 'n_partial_samples': []} for p in DIAG_PARTIAL_PERCENTAGES}\n\nfor partial_perc in DIAG_PARTIAL_PERCENTAGES:\n    print(f\"\\n{'='*60}\")\n    print(f\"PARTIAL: {partial_perc*100}%\")\n    print(f\"{'='*60}\")\n    \n    valid_runs = 0\n    r_state = 0\n    \n    while valid_runs < DIAG_N_RUNS and r_state < 500:\n        set_to_deterministic(r_state)\n        \n        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                          partial_perc, r_state, device=DEVICE)\n        \n        if DO.lack_partial_coverage:\n            r_state += 1\n            continue\n        \n        n_shared = len(DO.inpt_vars)\n        n_hyp = len(DO.miss_vars)\n        out_size = len(DO.target_vars)\n        hyp_per_sample = DO.num_hyp_comb\n        \n        partial_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n        ].index.tolist())\n        \n        n_partial = len(partial_gids)\n        \n        # === Partial Only ===\n        set_to_deterministic(r_state + 400)\n        model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                                  MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_partial, _, _ = train_with_soft_weights(\n            DO, model_partial, sample_weights={}, partial_gids=partial_gids,\n            partial_weight=1.0, lr=GGH_BENCHMARK_LR, n_epochs=GGH_FINAL_EPOCHS\n        )\n        _, _, partial_r2 = evaluate_on_test(DO, model_partial)\n        \n        # === Full Info ===\n        n_samples_full = len(DO.df_train_hypothesis) // hyp_per_sample\n        full_info_weights = {}\n        for sample_idx in range(n_samples_full):\n            for hyp_idx in range(hyp_per_sample):\n                gid = sample_idx * hyp_per_sample + hyp_idx\n                if DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']:\n                    full_info_weights[gid] = 1.0\n        \n        set_to_deterministic(r_state + 600)\n        model_full = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n                                               MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size).to(DEVICE)\n        model_full, _, _ = train_with_soft_weights(DO, model_full, full_info_weights, set(),\n                                                    1.0, GGH_BENCHMARK_LR, GGH_FINAL_EPOCHS)\n        _, _, full_r2 = evaluate_on_test(DO, model_full)\n        \n        print(f\"  Run {valid_runs+1}/{DIAG_N_RUNS} (r_state={r_state}): n_partial={n_partial}, Partial R2={partial_r2:.4f}, Full R2={full_r2:.4f}\")\n        \n        diag_results[partial_perc]['Partial'].append(partial_r2)\n        diag_results[partial_perc]['Full Info'].append(full_r2)\n        diag_results[partial_perc]['n_partial_samples'].append(n_partial)\n        \n        valid_runs += 1\n        r_state += 1\n\n# Summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SUMMARY: Partial vs Full Info\")\nprint(\"=\" * 80)\nprint(f\"{'Partial %':<12} | {'N Samples':<12} | {'Partial R2':<20} | {'Full Info R2':<15}\")\nprint(\"-\" * 70)\nfor partial_perc in DIAG_PARTIAL_PERCENTAGES:\n    partial_r2s = diag_results[partial_perc]['Partial']\n    full_r2s = diag_results[partial_perc]['Full Info']\n    n_samples = diag_results[partial_perc]['n_partial_samples']\n    print(f\"{partial_perc*100:>10.0f}% | {np.mean(n_samples):>10.1f} | {np.mean(partial_r2s):>8.4f} \u00b1 {np.std(partial_r2s):.4f} | {np.mean(full_r2s):>8.4f} \u00b1 {np.std(full_r2s):.4f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}