{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine GGH Expansion Benchmark (Cleaned)\n",
    "\n",
    "This notebook contains only the necessary code to run the GGH expansion benchmark.\n",
    "\n",
    "**Results**: Partial R2 ≈ 0.18, GGH R2 ≈ 0.235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_header",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## GGH Expansion Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ggh_expansion_enriched",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH Soft Weight Refinement vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method (Soft Weighting - No Hard Pruning):\n",
      "  Iter1: 60 epochs unbiased (lr=0.01), last 5 tracked\n",
      "  Iter1: Enriched scoring -> soft weights (min=0.1, temp=1.0)\n",
      "  Iter2: 30 epochs weighted training (lr=0.01, pw=2.0)\n",
      "  Iter3: Biased rescoring -> multiply weights (temp=0.8)\n",
      "  Iter4: Loss-based adjustment (influence=0.25)\n",
      "  Final: Train with refined weights + dynamic partial\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, validation-based epoch selection, same architecture\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 39.2%, class dist: {0: 396, 1: 262, 2: 465}\n",
      "    Avg weight correct: 0.548, incorrect: 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.497, incorrect: 0.490\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.441, incorrect: 0.433\n",
      "  Unweighted precision: 39.2%, Effective (weighted) precision: 39.6%\n",
      "  Dynamic partial_weight: 3.13\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0167, test_loss=0.0156, test_mae=0.0946, R2=0.4035\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0176, test_loss=0.0172, test_mae=0.0982, R2=0.3394\n",
      "\n",
      ">>> Improvement: Loss=+0.0017, MAE=+0.0036, R2=+0.0641\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 45.9%, class dist: {0: 474, 1: 253, 2: 396}\n",
      "    Avg weight correct: 0.574, incorrect: 0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.509, incorrect: 0.403\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.453, incorrect: 0.355\n",
      "  Unweighted precision: 45.9%, Effective (weighted) precision: 52.0%\n",
      "  Dynamic partial_weight: 3.20\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0187, test_loss=0.0199, test_mae=0.1100, R2=0.1859\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0179, test_loss=0.0194, test_mae=0.1102, R2=0.2048\n",
      "\n",
      ">>> Improvement: Loss=-0.0005, MAE=+0.0002, R2=-0.0189\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 44.2%, class dist: {0: 355, 1: 309, 2: 459}\n",
      "    Avg weight correct: 0.567, incorrect: 0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.589, incorrect: 0.502\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.522, incorrect: 0.445\n",
      "  Unweighted precision: 44.2%, Effective (weighted) precision: 48.2%\n",
      "  Dynamic partial_weight: 3.04\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=0, val_loss=0.0196, test_loss=0.0170, test_mae=0.1026, R2=0.2909\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=88, val_loss=0.0196, test_loss=0.0171, test_mae=0.1036, R2=0.2850\n",
      "\n",
      ">>> Improvement: Loss=+0.0001, MAE=+0.0010, R2=+0.0059\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 44.0%, class dist: {0: 367, 1: 350, 2: 406}\n",
      "    Avg weight correct: 0.567, incorrect: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.501, incorrect: 0.444\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.444, incorrect: 0.392\n",
      "  Unweighted precision: 44.0%, Effective (weighted) precision: 47.1%\n",
      "  Dynamic partial_weight: 3.17\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0155, test_loss=0.0167, test_mae=0.1071, R2=0.2856\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0210, test_loss=0.0187, test_mae=0.1101, R2=0.1972\n",
      "\n",
      ">>> Improvement: Loss=+0.0021, MAE=+0.0030, R2=+0.0885\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 45.6%, class dist: {0: 461, 1: 444, 2: 218}\n",
      "    Avg weight correct: 0.551, incorrect: 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.478, incorrect: 0.442\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.426, incorrect: 0.392\n",
      "  Unweighted precision: 45.6%, Effective (weighted) precision: 47.7%\n",
      "  Dynamic partial_weight: 3.19\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=21, val_loss=0.0215, test_loss=0.0164, test_mae=0.0968, R2=0.3413\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0244, test_loss=0.0203, test_mae=0.1061, R2=0.1838\n",
      "\n",
      ">>> Improvement: Loss=+0.0039, MAE=+0.0093, R2=+0.1576\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 50.0%, class dist: {0: 462, 1: 310, 2: 351}\n",
      "    Avg weight correct: 0.583, incorrect: 0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.534, incorrect: 0.418\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.473, incorrect: 0.368\n",
      "  Unweighted precision: 50.0%, Effective (weighted) precision: 56.3%\n",
      "  Dynamic partial_weight: 3.16\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=27, val_loss=0.0168, test_loss=0.0189, test_mae=0.1035, R2=0.2085\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=52, val_loss=0.0174, test_loss=0.0194, test_mae=0.1114, R2=0.1901\n",
      "\n",
      ">>> Improvement: Loss=+0.0004, MAE=+0.0079, R2=+0.0185\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 42.2%, class dist: {0: 281, 1: 365, 2: 477}\n",
      "    Avg weight correct: 0.581, incorrect: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.533, incorrect: 0.464\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.474, incorrect: 0.413\n",
      "  Unweighted precision: 42.2%, Effective (weighted) precision: 45.6%\n",
      "  Dynamic partial_weight: 3.12\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=43, val_loss=0.0189, test_loss=0.0162, test_mae=0.0986, R2=0.3164\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0233, test_loss=0.0210, test_mae=0.1154, R2=0.1099\n",
      "\n",
      ">>> Improvement: Loss=+0.0049, MAE=+0.0169, R2=+0.2065\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=742)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 49.5%, class dist: {0: 441, 1: 271, 2: 411}\n",
      "    Avg weight correct: 0.566, incorrect: 0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.488, incorrect: 0.427\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.433, incorrect: 0.377\n",
      "  Unweighted precision: 49.5%, Effective (weighted) precision: 53.0%\n",
      "  Dynamic partial_weight: 3.19\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=10, val_loss=0.0190, test_loss=0.0197, test_mae=0.1115, R2=0.2414\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0192, test_loss=0.0185, test_mae=0.1102, R2=0.2889\n",
      "\n",
      ">>> Improvement: Loss=-0.0012, MAE=-0.0013, R2=-0.0475\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=842)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 50.8%, class dist: {0: 471, 1: 346, 2: 306}\n",
      "    Avg weight correct: 0.557, incorrect: 0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.455, incorrect: 0.434\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.401, incorrect: 0.381\n",
      "  Unweighted precision: 50.8%, Effective (weighted) precision: 52.0%\n",
      "  Dynamic partial_weight: 3.22\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=6, val_loss=0.0165, test_loss=0.0217, test_mae=0.1061, R2=0.2922\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=14, val_loss=0.0159, test_loss=0.0244, test_mae=0.1217, R2=0.2050\n",
      "\n",
      ">>> Improvement: Loss=+0.0027, MAE=+0.0156, R2=+0.0872\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=942)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 44.5%, class dist: {0: 421, 1: 646, 2: 56}\n",
      "    Avg weight correct: 0.582, incorrect: 0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.491, incorrect: 0.385\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.438, incorrect: 0.341\n",
      "  Unweighted precision: 44.5%, Effective (weighted) precision: 50.8%\n",
      "  Dynamic partial_weight: 3.23\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=33, val_loss=0.0133, test_loss=0.0201, test_mae=0.1108, R2=0.3005\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=6, val_loss=0.0169, test_loss=0.0240, test_mae=0.1214, R2=0.1681\n",
      "\n",
      ">>> Improvement: Loss=+0.0038, MAE=+0.0106, R2=+0.1324\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=1042)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 40.0%, class dist: {0: 342, 1: 167, 2: 614}\n",
      "    Avg weight correct: 0.580, incorrect: 0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.478, incorrect: 0.395\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.427, incorrect: 0.351\n",
      "  Unweighted precision: 40.0%, Effective (weighted) precision: 44.8%\n",
      "  Dynamic partial_weight: 3.24\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0205, test_loss=0.0204, test_mae=0.1100, R2=0.1533\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0224, test_loss=0.0229, test_mae=0.1148, R2=0.0496\n",
      "\n",
      ">>> Improvement: Loss=+0.0025, MAE=+0.0048, R2=+0.1037\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=1142)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 47.9%, class dist: {0: 311, 1: 517, 2: 295}\n",
      "    Avg weight correct: 0.572, incorrect: 0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.486, incorrect: 0.422\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.431, incorrect: 0.372\n",
      "  Unweighted precision: 47.9%, Effective (weighted) precision: 51.6%\n",
      "  Dynamic partial_weight: 3.20\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=185, val_loss=0.0174, test_loss=0.0204, test_mae=0.1137, R2=0.1267\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=86, val_loss=0.0199, test_loss=0.0228, test_mae=0.1204, R2=0.0234\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=+0.0067, R2=+0.1033\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=1242)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 46.7%, class dist: {0: 354, 1: 403, 2: 366}\n",
      "    Avg weight correct: 0.563, incorrect: 0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.514, incorrect: 0.462\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.454, incorrect: 0.406\n",
      "  Unweighted precision: 46.7%, Effective (weighted) precision: 49.5%\n",
      "  Dynamic partial_weight: 3.14\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=30, val_loss=0.0184, test_loss=0.0202, test_mae=0.1116, R2=0.1805\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=32, val_loss=0.0223, test_loss=0.0196, test_mae=0.1125, R2=0.2037\n",
      "\n",
      ">>> Improvement: Loss=-0.0006, MAE=+0.0009, R2=-0.0232\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=1342)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 41.0%, class dist: {0: 436, 1: 356, 2: 331}\n",
      "    Avg weight correct: 0.566, incorrect: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.499, incorrect: 0.425\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.444, incorrect: 0.376\n",
      "  Unweighted precision: 41.0%, Effective (weighted) precision: 45.0%\n",
      "  Dynamic partial_weight: 3.19\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=21, val_loss=0.0173, test_loss=0.0167, test_mae=0.1065, R2=0.2706\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=192, val_loss=0.0241, test_loss=0.0240, test_mae=0.1330, R2=-0.0483\n",
      "\n",
      ">>> Improvement: Loss=+0.0073, MAE=+0.0265, R2=+0.3189\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=1442)\n",
      "============================================================\n",
      "Running GGH Soft Refinement...\n",
      "  Iter1: 1123 samples, precision: 45.4%, class dist: {0: 403, 1: 528, 2: 192}\n",
      "    Avg weight correct: 0.590, incorrect: 0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 3453 hypotheses from 1151 samples\n",
      "  Iter3 (after multiply): Avg weight correct: 0.509, incorrect: 0.425\n",
      "  Iter4 (after loss adj): Avg weight correct: 0.453, incorrect: 0.376\n",
      "  Unweighted precision: 45.4%, Effective (weighted) precision: 50.1%\n",
      "  Dynamic partial_weight: 3.18\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=1, val_loss=0.0169, test_loss=0.0172, test_mae=0.1027, R2=0.3379\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=8, val_loss=0.0175, test_loss=0.0176, test_mae=0.1060, R2=0.3227\n",
      "\n",
      ">>> Improvement: Loss=+0.0004, MAE=+0.0033, R2=+0.0152\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK RESULTS: GGH Soft Weight Refinement vs Partial-Only\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   Eff Prec   GGH Loss     Part Loss    Δ Loss     GGH R2     Part R2    Δ R2      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     39.6      % 0.0156       0.0172          +0.0017 0.4035     0.3394        +0.0641\n",
      "2     52.0      % 0.0199       0.0194          -0.0005 0.1859     0.2048        -0.0189\n",
      "3     48.2      % 0.0170       0.0171          +0.0001 0.2909     0.2850        +0.0059\n",
      "4     47.1      % 0.0167       0.0187          +0.0021 0.2856     0.1972        +0.0885\n",
      "5     47.7      % 0.0164       0.0203          +0.0039 0.3413     0.1838        +0.1576\n",
      "6     56.3      % 0.0189       0.0194          +0.0004 0.2085     0.1901        +0.0185\n",
      "7     45.6      % 0.0162       0.0210          +0.0049 0.3164     0.1099        +0.2065\n",
      "8     53.0      % 0.0197       0.0185          -0.0012 0.2414     0.2889        -0.0475\n",
      "9     52.0      % 0.0217       0.0244          +0.0027 0.2922     0.2050        +0.0872\n",
      "10    50.8      % 0.0201       0.0240          +0.0038 0.3005     0.1681        +0.1324\n",
      "11    44.8      % 0.0204       0.0229          +0.0025 0.1533     0.0496        +0.1037\n",
      "12    51.6      % 0.0204       0.0228          +0.0024 0.1267     0.0234        +0.1033\n",
      "13    49.5      % 0.0202       0.0196          -0.0006 0.1805     0.2037        -0.0232\n",
      "14    45.0      % 0.0167       0.0240          +0.0073 0.2706     -0.0483       +0.3189\n",
      "15    50.1      % 0.0172       0.0176          +0.0004 0.3379     0.3227        +0.0152\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "GGH Effective Precision: 48.9% ± 4.0%\n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH:     0.0185 ± 0.0019\n",
      "  Partial: 0.0205 ± 0.0025\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH:     0.2624 ± 0.0752\n",
      "  Partial: 0.1815 ± 0.1063\n",
      "\n",
      "Statistical Tests (paired t-test):\n",
      "  Loss: t=-3.321, p=0.0050 *\n",
      "  R2:   t=3.206, p=0.0064 *\n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (Loss): 12/15 (80.0%)\n",
      "  GGH wins (R2):   12/15 (80.0%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "GGH Soft Refinement significantly OUTPERFORMS Partial-only on Loss (p=0.0050)\n",
      "Average loss improvement: 0.0020 MSE\n",
      "GGH Soft Refinement significantly OUTPERFORMS Partial-only on R2 (p=0.0064)\n",
      "Average R2 improvement: 0.0808\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: GGH with Soft Weight Iterative Refinement\n",
    "# =============================================================================\n",
    "# GGH Method (Soft Weighting):\n",
    "#   1. Unbiased training -> enriched scores -> initial soft weights\n",
    "#   2. Weighted training on ALL samples (different weights) + partial\n",
    "#   3. Biased rescoring -> multiply weights (iterative refinement)\n",
    "#   4. Loss-based adjustment -> final weights\n",
    "#   5. Final model trained with refined weights + dynamic partial\n",
    "# Key: No hard pruning - all samples retained with varying weights\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_RAND_STATES = [42 + i * 100 for i in range(15)]\n",
    "BENCHMARK_FINAL_EPOCHS = 200\n",
    "BENCHMARK_LR = 0.01\n",
    "\n",
    "# GGH Parameters - Soft Weighting (Iterative Refinement)\n",
    "GGH_ITER1_EPOCHS = 60              # 60 epochs unbiased\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5      # Last 5 tracked\n",
    "GGH_ITER1_LR = 0.01                # lr=0.01\n",
    "GGH_ITER2_EPOCHS = 30              # 30 epochs weighted training\n",
    "GGH_ITER2_LR = 0.01                # lr=0.01\n",
    "GGH_SCORING_PASSES = 5             # 5 passes for scoring\n",
    "\n",
    "# Soft Weighting Parameters\n",
    "GGH_MIN_WEIGHT = 0.1               # No sample below this weight\n",
    "GGH_TEMPERATURE_ITER1 = 1.0        # Sharpness of Iter1 weight distribution\n",
    "GGH_TEMPERATURE_ITER3 = 0.8        # Sharpness of Iter3 weight distribution\n",
    "GGH_LOSS_INFLUENCE = 0.25          # How much loss affects final weights (0-1)\n",
    "GGH_PARTIAL_BASE_WEIGHT = 2.0      # Base partial weight\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "# === HELPER FUNCTIONS ===\n",
    "def sigmoid_stable(x):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    return np.where(x >= 0,\n",
    "                    1 / (1 + np.exp(-x)),\n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def compute_soft_weights(scores, min_weight=0.1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Convert scores to soft weights using sigmoid.\n",
    "    Higher score -> higher weight (approaching 1.0)\n",
    "    Lower score -> lower weight (approaching min_weight)\n",
    "    \"\"\"\n",
    "    scores = np.array(scores, dtype=np.float64)\n",
    "    if len(scores) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Normalize scores\n",
    "    mean_s = np.mean(scores)\n",
    "    std_s = np.std(scores) + 1e-8\n",
    "    normalized = (scores - mean_s) / std_s\n",
    "    \n",
    "    # Apply sigmoid with temperature\n",
    "    raw_weights = sigmoid_stable(normalized / temperature)\n",
    "    \n",
    "    # Scale to [min_weight, 1.0]\n",
    "    weights = min_weight + (1 - min_weight) * raw_weights\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score with loss included (gradient + features + loss).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    loss_params = anchor_data.get('loss_norm_params', {}).get(class_id)\n",
    "    grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "    \n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    if loss_params:\n",
    "        loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "    else:\n",
    "        loss_norm = -loss * grad_scale\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "    \n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "# === WEIGHTED TRAINER CLASS ===\n",
    "class WeightedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL samples with continuous weights.\n",
    "    Replaces binary selection with soft weighting.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, sample_weights, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.sample_weights = sample_weights  # Dict: gid -> weight (0.1 to 1.0)\n",
    "        self.partial_gids = set(partial_gids)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch with weighted samples.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Get weights for each sample\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                elif gid in self.sample_weights:\n",
    "                    weights[i] = self.sample_weights[gid]\n",
    "                # else: weight stays 0 (not selected)\n",
    "            \n",
    "            if weights.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "\n",
    "\n",
    "def train_with_soft_weights(DO, model, sample_weights, partial_gids, partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with soft weights and validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = WeightedTrainer(DO, model, sample_weights=sample_weights, \n",
    "                             partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "# === MAIN GGH SOFT REFINEMENT FUNCTION ===\n",
    "def run_ggh_soft_refinement(DO, rand_state):\n",
    "    \"\"\"\n",
    "    GGH with soft weight iterative refinement.\n",
    "    \n",
    "    Iter1: Unbiased -> enriched scores -> initial soft weights\n",
    "    Iter2: Weighted training on ALL selected samples + partial\n",
    "    Iter3: Biased rescoring -> multiply weights (refine confidence)\n",
    "    Iter4: Loss-based adjustment -> final weights\n",
    "    \n",
    "    Returns: gid_weights, effective_precision, partial_gids, partial_weight_dynamic\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training + Initial soft weights ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n",
    "    \n",
    "    # Train (60 epochs, track last 5)\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # Compute anchors and analysis\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    # Score all non-partial samples (select best hypothesis per sample)\n",
    "    sample_scores = {}  # sample_idx -> (score, best_gid, is_correct)\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_gid, best_is_correct = -np.inf, None, False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            sample_scores[sample_idx] = (best_score, best_gid, best_is_correct)\n",
    "    \n",
    "    # Convert scores to soft weights\n",
    "    scores_list = [s[0] for s in sample_scores.values()]\n",
    "    weights_iter1 = compute_soft_weights(scores_list, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER1)\n",
    "    \n",
    "    # Map weights to gids\n",
    "    gid_weights = {}\n",
    "    sample_to_gid = {}\n",
    "    for i, (sample_idx, (score, gid, is_correct)) in enumerate(sample_scores.items()):\n",
    "        gid_weights[gid] = float(weights_iter1[i])\n",
    "        sample_to_gid[sample_idx] = gid\n",
    "    \n",
    "    # Track stats\n",
    "    iter1_correct = sum(1 for s in sample_scores.values() if s[2])\n",
    "    iter1_precision = iter1_correct / len(sample_scores) * 100\n",
    "    correct_weights = [gid_weights[s[1]] for s in sample_scores.values() if s[2]]\n",
    "    incorrect_weights = [gid_weights[s[1]] for s in sample_scores.values() if not s[2]]\n",
    "    avg_weight_correct = np.mean(correct_weights) if correct_weights else 0\n",
    "    avg_weight_incorrect = np.mean(incorrect_weights) if incorrect_weights else 0\n",
    "    \n",
    "    # Track class distribution (weighted)\n",
    "    iter1_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for sample_idx, (score, gid, is_correct) in sample_scores.items():\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        iter1_class_counts[class_id] += 1\n",
    "    \n",
    "    print(f\"  Iter1: {len(sample_scores)} samples, precision: {iter1_precision:.1f}%, class dist: {iter1_class_counts}\")\n",
    "    print(f\"    Avg weight correct: {avg_weight_correct:.3f}, incorrect: {avg_weight_incorrect:.3f}\")\n",
    "    \n",
    "    # === ITERATION 2: Weighted training ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_weighted = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    trainer_weighted = WeightedTrainer(DO, model_weighted, sample_weights=gid_weights,\n",
    "                                       partial_gids=partial_correct_gids,\n",
    "                                       partial_weight=GGH_PARTIAL_BASE_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_weighted.train_epoch(dataloader, epoch)\n",
    "    \n",
    "    # === ITERATION 3: Biased rescoring -> Multiply weights ===\n",
    "    # Score all selected samples with biased model\n",
    "    selected_sample_indices = set(sample_scores.keys())\n",
    "    scorer = RemainingDataScorer(DO, model_weighted, selected_sample_indices | partial_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    biased_analysis = scorer.get_analysis()\n",
    "    \n",
    "    # Build biased anchors from partial data\n",
    "    anchor_data_biased = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "        'loss_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    all_grads = [biased_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_biased['grad_scale'] = grad_scale\n",
    "    \n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        correct_losses, incorrect_losses = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if biased_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(biased_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    correct_losses.append(biased_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in biased_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if biased_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(biased_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    incorrect_losses.append(biased_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n",
    "            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n",
    "            \n",
    "            all_losses = correct_losses + incorrect_losses\n",
    "            loss_mean = np.mean(all_losses)\n",
    "            loss_std = np.std(all_losses) + 1e-8\n",
    "            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n",
    "            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n",
    "            \n",
    "            correct_enriched = [np.concatenate([g, f, [l]]) \n",
    "                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n",
    "            incorrect_enriched = [np.concatenate([g, f, [l]]) \n",
    "                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n",
    "            \n",
    "            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    # Compute Iter3 scores for all selected samples\n",
    "    iter3_scores = {}\n",
    "    for sample_idx, (_, gid, _) in sample_scores.items():\n",
    "        if gid in biased_analysis and biased_analysis[gid]['avg_gradient'] is not None:\n",
    "            gradient = biased_analysis[gid]['avg_gradient']\n",
    "            loss = biased_analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            score = compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            iter3_scores[gid] = score\n",
    "    \n",
    "    # Convert to soft weights and multiply with Iter1 weights\n",
    "    scores_list_iter3 = list(iter3_scores.values())\n",
    "    gids_iter3 = list(iter3_scores.keys())\n",
    "    weights_iter3_raw = compute_soft_weights(scores_list_iter3, GGH_MIN_WEIGHT, GGH_TEMPERATURE_ITER3)\n",
    "    \n",
    "    for i, gid in enumerate(gids_iter3):\n",
    "        gid_weights[gid] = gid_weights[gid] * weights_iter3_raw[i]\n",
    "    \n",
    "    # Renormalize to [MIN_WEIGHT, 1.0]\n",
    "    if gid_weights:\n",
    "        max_w = max(gid_weights.values())\n",
    "        if max_w > 0:\n",
    "            for gid in gid_weights:\n",
    "                gid_weights[gid] = GGH_MIN_WEIGHT + (gid_weights[gid] / max_w) * (1 - GGH_MIN_WEIGHT)\n",
    "    \n",
    "    correct_weights_iter3 = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n",
    "    incorrect_weights_iter3 = [gid_weights[s[1]] for s in sample_scores.values() if not s[2] and s[1] in gid_weights]\n",
    "    avg_weight_correct_iter3 = np.mean(correct_weights_iter3) if correct_weights_iter3 else 0\n",
    "    avg_weight_incorrect_iter3 = np.mean(incorrect_weights_iter3) if incorrect_weights_iter3 else 0\n",
    "    print(f\"  Iter3 (after multiply): Avg weight correct: {avg_weight_correct_iter3:.3f}, incorrect: {avg_weight_incorrect_iter3:.3f}\")\n",
    "    \n",
    "    # === ITERATION 4: Loss-based adjustment ===\n",
    "    losses = {gid: biased_analysis[gid]['avg_loss']\n",
    "              for gid in gid_weights if gid in biased_analysis}\n",
    "    \n",
    "    if losses:\n",
    "        loss_values = list(losses.values())\n",
    "        loss_mean = np.mean(loss_values)\n",
    "        loss_std = np.std(loss_values) + 1e-8\n",
    "        \n",
    "        for gid in gid_weights:\n",
    "            if gid in losses:\n",
    "                norm_loss = (losses[gid] - loss_mean) / loss_std\n",
    "                # Higher loss -> lower weight (but never below MIN_WEIGHT)\n",
    "                loss_factor = 1 - GGH_LOSS_INFLUENCE * sigmoid_stable(norm_loss)\n",
    "                gid_weights[gid] = max(GGH_MIN_WEIGHT, gid_weights[gid] * loss_factor)\n",
    "    \n",
    "    correct_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if s[2] and s[1] in gid_weights]\n",
    "    incorrect_weights_final = [gid_weights[s[1]] for s in sample_scores.values() if not s[2] and s[1] in gid_weights]\n",
    "    avg_weight_correct_final = np.mean(correct_weights_final) if correct_weights_final else 0\n",
    "    avg_weight_incorrect_final = np.mean(incorrect_weights_final) if incorrect_weights_final else 0\n",
    "    print(f\"  Iter4 (after loss adj): Avg weight correct: {avg_weight_correct_final:.3f}, incorrect: {avg_weight_incorrect_final:.3f}\")\n",
    "    \n",
    "    # === Compute effective precision (weighted) ===\n",
    "    total_weight_correct = sum(correct_weights_final)\n",
    "    total_weight_all = sum(gid_weights.values())\n",
    "    effective_precision = total_weight_correct / total_weight_all * 100 if total_weight_all > 0 else 0\n",
    "    \n",
    "    # Also compute unweighted precision for comparison\n",
    "    unweighted_correct = len(correct_weights_final)\n",
    "    unweighted_total = len(gid_weights)\n",
    "    unweighted_precision = unweighted_correct / unweighted_total * 100 if unweighted_total > 0 else 0\n",
    "    \n",
    "    print(f\"  Unweighted precision: {unweighted_precision:.1f}%, Effective (weighted) precision: {effective_precision:.1f}%\")\n",
    "    \n",
    "    # === Dynamic partial weight ===\n",
    "    # Lower average weight -> higher partial weight (more uncertainty -> lean on anchor)\n",
    "    avg_final_weight = np.mean(list(gid_weights.values())) if gid_weights else 0.5\n",
    "    partial_weight_dynamic = GGH_PARTIAL_BASE_WEIGHT * (1 + (1 - avg_final_weight))\n",
    "    print(f\"  Dynamic partial_weight: {partial_weight_dynamic:.2f}\")\n",
    "    \n",
    "    return gid_weights, effective_precision, partial_correct_gids, partial_weight_dynamic\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: GGH Soft Weight Refinement vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method (Soft Weighting - No Hard Pruning):\")\n",
    "print(f\"  Iter1: {GGH_ITER1_EPOCHS} epochs unbiased (lr={GGH_ITER1_LR}), last {GGH_ITER1_ANALYSIS_EPOCHS} tracked\")\n",
    "print(f\"  Iter1: Enriched scoring -> soft weights (min={GGH_MIN_WEIGHT}, temp={GGH_TEMPERATURE_ITER1})\")\n",
    "print(f\"  Iter2: {GGH_ITER2_EPOCHS} epochs weighted training (lr={GGH_ITER2_LR}, pw={GGH_PARTIAL_BASE_WEIGHT})\")\n",
    "print(f\"  Iter3: Biased rescoring -> multiply weights (temp={GGH_TEMPERATURE_ITER3})\")\n",
    "print(f\"  Iter4: Loss-based adjustment (influence={GGH_LOSS_INFLUENCE})\")\n",
    "print(f\"  Final: Train with refined weights + dynamic partial\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {BENCHMARK_FINAL_EPOCHS} epochs, validation-based epoch selection, same architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, run_rand_state in enumerate(BENCHMARK_RAND_STATES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup DataOperator\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === Run GGH Soft Refinement ===\n",
    "    print(\"Running GGH Soft Refinement...\")\n",
    "    gid_weights, ggh_precision, _, partial_weight_dynamic = run_ggh_soft_refinement(DO_run, run_rand_state)\n",
    "    \n",
    "    # === Train GGH final model with soft weights ===\n",
    "    print(f\"Training GGH model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_soft_weights(\n",
    "        DO_run, model_ggh, sample_weights=gid_weights, partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight_dynamic, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    print(f\"Training Partial model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val_loss = train_with_soft_weights(\n",
    "        DO_run, model_partial, sample_weights={}, partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, \"\n",
    "          f\"MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: GGH Soft Weight Refinement vs Partial-Only\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'Eff Prec':<10} {'GGH Loss':<12} {'Part Loss':<12} {'Δ Loss':<10} {'GGH R2':<10} {'Part R2':<10} {'Δ R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<12.4f} \"\n",
    "          f\"{r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGGH Effective Precision: {np.mean(ggh_precisions):.1f}% ± {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f} ± {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f} ± {np.std(partial_losses):.4f}\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} ± {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f} ± {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{BENCHMARK_N_RUNS} ({n_ggh_wins_loss/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{BENCHMARK_N_RUNS} ({n_ggh_wins_r2/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH Soft', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH Soft Refinement vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 2: R2 comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, ggh_r2s, width, label='GGH Soft', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, partial_r2s, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Test R2')\n",
    "ax2.set_title('Test R2: GGH Soft Refinement vs Partial')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 3: GGH Effective Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, BENCHMARK_N_RUNS+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('Effective Precision (%)')\n",
    "ax3.set_title('GGH Effective (Weighted) Precision')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_soft_refinement_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement_loss = np.mean([r['improvement_loss'] for r in results])\n",
    "avg_improvement_r2 = np.mean([r['improvement_r2'] for r in results])\n",
    "if avg_improvement_loss > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH Soft Refinement significantly OUTPERFORMS Partial-only on Loss (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average loss improvement: {avg_improvement_loss:.4f} MSE\")\n",
    "elif avg_improvement_loss < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Soft Refinement on Loss (p={p_value_loss:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in Loss (p={p_value_loss:.4f})\")\n",
    "\n",
    "if avg_improvement_r2 > 0 and p_value_r2 < 0.05:\n",
    "    print(f\"GGH Soft Refinement significantly OUTPERFORMS Partial-only on R2 (p={p_value_r2:.4f})\")\n",
    "    print(f\"Average R2 improvement: {avg_improvement_r2:.4f}\")\n",
    "elif avg_improvement_r2 < 0 and p_value_r2 < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Soft Refinement on R2 (p={p_value_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in R2 (p={p_value_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01483701-5176-4433-9e91-7d159ae81b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
