{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 0\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypothesisDataset defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that includes global IDs for tracking.\"\"\"\n",
    "    def __init__(self, DO):\n",
    "        # Input features = inpt_vars + hypothesis column\n",
    "        input_cols = DO.inpt_vars + [f'{DO.miss_vars[0]}_hypothesis']\n",
    "        self.inputs = torch.tensor(\n",
    "            DO.df_train_hypothesis[input_cols].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.targets = torch.tensor(\n",
    "            DO.df_train_hypothesis[DO.target_vars].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.global_ids = torch.arange(len(self.inputs))\n",
    "        self.input_cols = input_cols\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.global_ids[idx]\n",
    "\n",
    "print(\"HypothesisDataset defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive_header",
   "metadata": {},
   "source": [
    "## Adaptive Context Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined_scoring_header",
   "metadata": {},
   "source": [
    "## Combined Loss + Gradient Scoring (for Iteration 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined_scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scoring utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_combined_score(loss, gradient, features, class_id, anchor_data, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Combine loss and gradient signals for scoring.\n",
    "    \n",
    "    For a truth-biased model:\n",
    "    - Lower loss = more likely correct (aligned with truth)\n",
    "    - Gradient similarity to correct anchor = more likely correct\n",
    "    \n",
    "    Final score = (1 - loss_weight) * gradient_score + loss_weight * (-normalized_loss)\n",
    "    Higher score = more likely correct\n",
    "    \"\"\"\n",
    "    # Gradient score (same as adaptive)\n",
    "    grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "    \n",
    "    # Loss score: lower loss = higher score\n",
    "    # We'll normalize this later when we have all losses\n",
    "    loss_score = -loss  # Negative because lower loss is better\n",
    "    \n",
    "    return {\n",
    "        'grad_score': grad_score,\n",
    "        'loss_score': loss_score,\n",
    "        'raw_loss': loss\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_and_combine_scores(all_scores, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Normalize loss scores per class and combine with gradient scores.\n",
    "    \n",
    "    Returns combined scores where higher = more likely correct.\n",
    "    \"\"\"\n",
    "    # Group by class\n",
    "    class_losses = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        if class_id not in class_losses:\n",
    "            class_losses[class_id] = []\n",
    "        class_losses[class_id].append(scores['raw_loss'])\n",
    "    \n",
    "    # Compute per-class mean and std for loss normalization\n",
    "    class_stats = {}\n",
    "    for class_id, losses in class_losses.items():\n",
    "        class_stats[class_id] = {\n",
    "            'mean': np.mean(losses),\n",
    "            'std': np.std(losses) + 1e-8\n",
    "        }\n",
    "    \n",
    "    # Normalize and combine\n",
    "    combined_scores = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        stats = class_stats[class_id]\n",
    "        \n",
    "        # Z-score normalize loss (then negate: lower loss = higher score)\n",
    "        normalized_loss_score = -(scores['raw_loss'] - stats['mean']) / stats['std']\n",
    "        \n",
    "        # Combine: weighted average of gradient and loss scores\n",
    "        combined = (1 - loss_weight) * scores['grad_score'] + loss_weight * normalized_loss_score\n",
    "        \n",
    "        combined_scores[sample_idx] = {\n",
    "            'gid': gid,\n",
    "            'combined_score': combined,\n",
    "            'grad_score': scores['grad_score'],\n",
    "            'loss_score': normalized_loss_score,\n",
    "            'raw_loss': scores['raw_loss'],\n",
    "            'class_id': class_id,\n",
    "            'is_correct': scores['is_correct']\n",
    "        }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "print(\"Combined scoring utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_header",
   "metadata": {},
   "source": [
    "## Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "analysis_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def analyze_threshold_precision(all_selections, title=\"Precision Analysis\", verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze precision at different thresholds.\n",
    "    \n",
    "    all_selections: list of (score, is_correct, sample_idx) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    if not all_selections:\n",
    "        print(\"No selections to analyze\")\n",
    "        return None, None\n",
    "    \n",
    "    # Compute precision at different percentiles\n",
    "    results = []\n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        n_include = max(1, int(len(all_selections) * pct / 100))\n",
    "        top_selections = all_selections[:n_include]\n",
    "        n_correct = sum(1 for _, is_correct, _ in top_selections if is_correct)\n",
    "        precision = n_correct / n_include\n",
    "        results.append({\n",
    "            'percentile': pct,\n",
    "            'n_samples': n_include,\n",
    "            'n_correct': n_correct,\n",
    "            'precision': precision\n",
    "        })\n",
    "    \n",
    "    # Compute precision in score bins\n",
    "    scores = [s[0] for s in all_selections]\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    n_bins = 10\n",
    "    bin_results = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_low = min_score + (max_score - min_score) * i / n_bins\n",
    "        bin_high = min_score + (max_score - min_score) * (i + 1) / n_bins\n",
    "        bin_selections = [(s, c) for s, c, _ in all_selections if bin_low <= s < bin_high]\n",
    "        if bin_selections:\n",
    "            bin_correct = sum(1 for _, c in bin_selections if c)\n",
    "            bin_results.append({\n",
    "                'bin': f'{bin_low:.2f}-{bin_high:.2f}',\n",
    "                'n_samples': len(bin_selections),\n",
    "                'precision': bin_correct / len(bin_selections)\n",
    "            })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(title)\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\nPrecision by Top Percentile (highest scores first):\")\n",
    "        print(\"-\" * 50)\n",
    "        for r in results:\n",
    "            print(f\"Top {r['percentile']:>3}%: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "        \n",
    "        if bin_results:\n",
    "            print(\"\\nPrecision by Score Bin:\")\n",
    "            print(\"-\" * 50)\n",
    "            for r in bin_results:\n",
    "                print(f\"Score {r['bin']}: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "    \n",
    "    return results, bin_results\n",
    "\n",
    "\n",
    "def select_hypotheses_adaptive(trainer, DO, anchor_data=None):\n",
    "    \"\"\"\n",
    "    Select best hypothesis per sample using adaptive context.\n",
    "    Returns list of (score, is_correct, sample_idx) sorted by score descending.\n",
    "    \"\"\"\n",
    "    if anchor_data is None:\n",
    "        anchor_data = compute_anchor_data(trainer, DO)\n",
    "    \n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    partial_sample_indices = anchor_data['partial_sample_indices']\n",
    "    blacklisted_gids = anchor_data['blacklisted_gids']\n",
    "    \n",
    "    all_selections = []\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_is_correct = False\n",
    "        best_gid = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return all_selections, anchor_data\n",
    "\n",
    "print(\"Analysis utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration1_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 1: Unbiased Training\n",
    "\n",
    "Train on ALL hypotheses equally. No selection = no feedback loop bias.\n",
    "Use Adaptive Context Selection to score hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iteration1_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack partial coverage: False\n",
      "Number of training hypotheses: 3453\n",
      "Hypotheses per sample: 3\n",
      "Number of samples: 1151\n",
      "Partial data samples: 28\n"
     ]
    }
   ],
   "source": [
    "# Initialize data\n",
    "set_to_deterministic(rand_state)\n",
    "\n",
    "DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "                  partial_perc, rand_state, device='cpu')\n",
    "DO.problem_type = 'regression'\n",
    "\n",
    "print(f\"Lack partial coverage: {DO.lack_partial_coverage}\")\n",
    "print(f\"Number of training hypotheses: {len(DO.df_train_hypothesis)}\")\n",
    "print(f\"Hypotheses per sample: {DO.num_hyp_comb}\")\n",
    "print(f\"Number of samples: {len(DO.df_train_hypothesis) // DO.num_hyp_comb}\")\n",
    "\n",
    "# Count partial data\n",
    "partial_correct_gids = DO.df_train_hypothesis[\n",
    "    (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "    (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "].index.tolist()\n",
    "print(f\"Partial data samples: {len(partial_correct_gids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iteration1_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input structure:\n",
      "  Total input size: 4\n",
      "  Shared features: 3\n",
      "  Hypothesis feature: 1\n",
      "\n",
      "Model created: HypothesisAmplifyingModel\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create dataloader\n",
    "    dataset = HypothesisDataset(DO)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Check input structure\n",
    "    input_size = dataset.inputs.shape[1]\n",
    "    n_shared_features = len(inpt_vars)\n",
    "    n_hypothesis_features = 1\n",
    "    \n",
    "    print(f\"\\nInput structure:\")\n",
    "    print(f\"  Total input size: {input_size}\")\n",
    "    print(f\"  Shared features: {n_shared_features}\")\n",
    "    print(f\"  Hypothesis feature: {n_hypothesis_features}\")\n",
    "    \n",
    "    # Create model\n",
    "    model_iter1 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel created: HypothesisAmplifyingModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "iteration1_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 1: Unbiased Training\n",
      "======================================================================\n",
      "Training on ALL hypotheses equally for 60 epochs\n",
      "Tracking gradients in last 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [00:00<00:01, 29.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60: Loss = 0.0226 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 43/60 [00:01<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/60: Loss = 0.0214 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:07<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60: Loss = 0.0214 (tracking)\n",
      "\n",
      "Iteration 1 complete. Final loss: 0.0214\n",
      "Tracked 3453 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Iteration 1\n",
    "if not DO.lack_partial_coverage:\n",
    "    trainer_iter1 = UnbiasedTrainer(DO, model_iter1, lr=lr)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 1: Unbiased Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training on ALL hypotheses equally for {iter1_epochs} epochs\")\n",
    "    print(f\"Tracking gradients in last {iter1_analysis_epochs} epochs\")\n",
    "    \n",
    "    iter1_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter1_epochs)):\n",
    "        # Track data in last N epochs\n",
    "        track = epoch >= (iter1_epochs - iter1_analysis_epochs)\n",
    "        \n",
    "        loss = trainer_iter1.train_epoch(dataloader, epoch, track_data=track)\n",
    "        iter1_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            status = \"(tracking)\" if track else \"\"\n",
    "            print(f\"Epoch {epoch+1}/{iter1_epochs}: Loss = {loss:.4f} {status}\")\n",
    "    \n",
    "    print(f\"\\nIteration 1 complete. Final loss: {iter1_losses[-1]:.4f}\")\n",
    "    print(f\"Tracked {len(trainer_iter1.loss_history)} hypotheses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "iteration1_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Selection Analysis (Adaptive Context)\n",
      "======================================================================\n",
      "Per-class method selection:\n",
      "  Class 0: grad_sim=-0.985 (good) -> GRADIENT-ONLY\n",
      "  Class 1: grad_sim=+0.956 (poor) -> ENRICHED (enriched_sim=-0.396)\n",
      "  Class 2: grad_sim=+0.825 (poor) -> ENRICHED (enriched_sim=-0.304)\n",
      "======================================================================\n",
      "ITERATION 1: Precision by Threshold\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:  112 samples, precision=58.9%\n",
      "Top  20%:  224 samples, precision=67.4%\n",
      "Top  30%:  336 samples, precision=67.6%\n",
      "Top  40%:  449 samples, precision=64.8%\n",
      "Top  50%:  561 samples, precision=59.9%\n",
      "Top  60%:  673 samples, precision=54.2%\n",
      "Top  70%:  786 samples, precision=52.0%\n",
      "Top  80%:  898 samples, precision=51.8%\n",
      "Top  90%: 1010 samples, precision=50.5%\n",
      "Top 100%: 1123 samples, precision=48.4%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -0.85--0.56:    8 samples, precision=37.5%\n",
      "Score -0.56--0.28:   24 samples, precision=20.8%\n",
      "Score -0.28-0.00:   35 samples, precision=25.7%\n",
      "Score 0.00-0.29:   40 samples, precision=32.5%\n",
      "Score 0.29-0.57:   56 samples, precision=32.1%\n",
      "Score 0.57-0.86:   98 samples, precision=50.0%\n",
      "Score 0.86-1.14:  121 samples, precision=48.8%\n",
      "Score 1.14-1.42:  108 samples, precision=30.6%\n",
      "Score 1.42-1.71:   32 samples, precision=28.1%\n",
      "Score 1.71-1.99:  600 samples, precision=57.5%\n"
     ]
    }
   ],
   "source": [
    "# Analyze Iteration 1 results\n",
    "if not DO.lack_partial_coverage:\n",
    "    # Get selections with adaptive context\n",
    "    all_selections_iter1, anchor_data_iter1 = select_hypotheses_adaptive(trainer_iter1, DO)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 1: Selection Analysis (Adaptive Context)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print_adaptive_method_summary(anchor_data_iter1, hyp_per_sample)\n",
    "    \n",
    "    # Convert to format for analysis\n",
    "    selections_for_analysis = [(s[0], s[1], s[2]) for s in all_selections_iter1]\n",
    "    results_iter1, _ = analyze_threshold_precision(\n",
    "        selections_for_analysis, \n",
    "        title=\"ITERATION 1: Precision by Threshold\"\n",
    "    )\n",
    "    \n",
    "    # Store precision for comparison\n",
    "    iter1_precision = {r['percentile']: r['precision'] for r in results_iter1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "iteration1_select_top",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Top 30% Selection\n",
      "======================================================================\n",
      "Selected 336 samples (top 30%)\n",
      "Correct: 227 (67.6% precision)\n",
      "Remaining samples: 787\n"
     ]
    }
   ],
   "source": [
    "# Select top N% for Iteration 2\n",
    "if not DO.lack_partial_coverage:\n",
    "    n_total = len(all_selections_iter1)\n",
    "    n_top = max(1, int(n_total * top_percentile / 100))\n",
    "    \n",
    "    top_selections = all_selections_iter1[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    top_gids = set(s[3] for s in top_selections)\n",
    "    \n",
    "    # Remaining samples (not in top N%)\n",
    "    remaining_sample_indices = set(s[2] for s in all_selections_iter1[n_top:])\n",
    "    \n",
    "    # Count correct in top selection\n",
    "    n_correct_top = sum(1 for s in top_selections if s[1])\n",
    "    precision_top = n_correct_top / n_top\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ITERATION 1: Top {top_percentile}% Selection\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Selected {n_top} samples (top {top_percentile}%)\")\n",
    "    print(f\"Correct: {n_correct_top} ({precision_top*100:.1f}% precision)\")\n",
    "    print(f\"Remaining samples: {len(remaining_sample_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration2_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 2: Biased Training\n",
    "\n",
    "Train a NEW model on:\n",
    "- Top 30% from Iteration 1 (high precision selections)\n",
    "- Partial data with upweighting (~25% of effective training)\n",
    "\n",
    "This creates a \"truth-biased\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "iteration2_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 2: Biased Training Setup\n",
      "======================================================================\n",
      "Training data:\n",
      "  Selected (top 30%): 336 samples\n",
      "  Partial data: 28 samples\n",
      "  Partial weight: 4.00x\n",
      "  Effective partial: 112.0 (25.0% of training)\n",
      "  Effective total: 448.0\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Get partial data GIDs (correct hypotheses from partial data)\n",
    "    partial_gids = set(anchor_data_iter1['partial_correct_gids'])\n",
    "    n_partial = len(partial_gids)\n",
    "    n_selected = len(top_gids)\n",
    "    \n",
    "    # Calculate weight for partial data\n",
    "    # Target: partial should be ~25% of effective training\n",
    "    # partial_weight * n_partial / (partial_weight * n_partial + n_selected) = 0.25\n",
    "    # Solving: partial_weight = 0.25 * n_selected / (0.75 * n_partial)\n",
    "    partial_weight = (partial_target_ratio * n_selected) / ((1 - partial_target_ratio) * n_partial)\n",
    "    partial_weight = max(1.0, partial_weight)  # At least weight 1\n",
    "    \n",
    "    effective_partial = n_partial * partial_weight\n",
    "    effective_total = effective_partial + n_selected\n",
    "    actual_partial_ratio = effective_partial / effective_total\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 2: Biased Training Setup\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training data:\")\n",
    "    print(f\"  Selected (top {top_percentile}%): {n_selected} samples\")\n",
    "    print(f\"  Partial data: {n_partial} samples\")\n",
    "    print(f\"  Partial weight: {partial_weight:.2f}x\")\n",
    "    print(f\"  Effective partial: {effective_partial:.1f} ({actual_partial_ratio*100:.1f}% of training)\")\n",
    "    print(f\"  Effective total: {effective_total:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "iteration2_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 2: Training Biased Model\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:00<00:00, 24.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: Loss = 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 24.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2 complete. Final loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create new model for Iteration 2\n",
    "    set_to_deterministic(rand_state + 1)  # Different seed for variety\n",
    "    \n",
    "    model_iter2 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    # Create biased trainer\n",
    "    trainer_iter2 = BiasedTrainer(\n",
    "        DO, model_iter2, \n",
    "        selected_gids=top_gids,\n",
    "        partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight,\n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 2: Training Biased Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    iter2_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter2_epochs)):\n",
    "        loss = trainer_iter2.train_epoch(dataloader, epoch, track_data=False)\n",
    "        iter2_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{iter2_epochs}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nIteration 2 complete. Final loss: {iter2_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparison: Iteration 1 vs Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tsq1hztx7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\n",
      "======================================================================\n",
      "\n",
      "Scoring Iter1's top 30% (336 samples) with biased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Scored 336 samples\n",
      "\n",
      "Original top 30%: 336 samples, 67.6% precision\n",
      "\n",
      "Precision after REMOVING lowest-confidence samples:\n",
      "------------------------------------------------------------\n",
      "Remove     Remaining    Correct    Precision    Change    \n",
      "------------------------------------------------------------\n",
      "  0%       336          227          67.6%        +0.0pp\n",
      " 10%       303          210          69.3%        +1.7pp\n",
      " 20%       269          194          72.1%        +4.6pp\n",
      " 30%       236          173          73.3%        +5.7pp **\n",
      " 40%       202          148          73.3%        +5.7pp **\n",
      " 50%       168          125          74.4%        +6.8pp **\n",
      "\n",
      "** = >5pp improvement over original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRUNING APPROACH: Use biased model to remove likely-wrong samples from Iter1's top 30%\n",
    "# =============================================================================\n",
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Score Iter1's top 30% samples with the biased model\n",
    "    print(f\"\\nScoring Iter1's top {top_percentile}% ({n_top} samples) with biased model...\")\n",
    "    \n",
    "    # Create scorer for top samples\n",
    "    top_scorer = RemainingDataScorer(DO, model_iter2, top_sample_indices)\n",
    "    top_scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    top_analysis = top_scorer.get_analysis()\n",
    "    \n",
    "    # Also need to score partial data with biased model to get anchors (reuse from before)\n",
    "    # anchor_data_iter3 was already computed\n",
    "    \n",
    "    # Compute combined scores for each top sample\n",
    "    top_scores = []\n",
    "    for sample_idx in top_sample_indices:\n",
    "        start = sample_idx * hyp_per_sample\n",
    "        \n",
    "        # Find the gid that Iter1 selected for this sample\n",
    "        iter1_selected_gid = None\n",
    "        for s in top_selections:\n",
    "            if s[2] == sample_idx:\n",
    "                iter1_selected_gid = s[3]\n",
    "                break\n",
    "        \n",
    "        if iter1_selected_gid is None or iter1_selected_gid not in top_analysis:\n",
    "            continue\n",
    "        \n",
    "        gid = iter1_selected_gid\n",
    "        if top_analysis[gid]['avg_gradient'] is None:\n",
    "            continue\n",
    "            \n",
    "        gradient = top_analysis[gid]['avg_gradient']\n",
    "        loss = top_analysis[gid]['avg_loss']\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        features = DO.df_train_hypothesis.loc[gid, inpt_vars].values.astype(np.float64)\n",
    "        is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        # Compute gradient score using biased model anchors\n",
    "        grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data_iter3)\n",
    "        \n",
    "        # Use negative loss as score (lower loss = higher score)\n",
    "        loss_score = -loss\n",
    "        \n",
    "        # Combined score (can tune weights)\n",
    "        combined = 0.5 * grad_score + 0.5 * loss_score\n",
    "        \n",
    "        top_scores.append({\n",
    "            'sample_idx': sample_idx,\n",
    "            'gid': gid,\n",
    "            'is_correct': is_correct,\n",
    "            'grad_score': grad_score,\n",
    "            'loss': loss,\n",
    "            'combined_score': combined\n",
    "        })\n",
    "    \n",
    "    print(f\"Scored {len(top_scores)} samples\")\n",
    "    \n",
    "    # Sort by combined score (ascending - lowest scores are most likely wrong)\n",
    "    top_scores_sorted = sorted(top_scores, key=lambda x: x['combined_score'])\n",
    "    \n",
    "    # Analyze precision after removing bottom N%\n",
    "    print(f\"\\nOriginal top {top_percentile}%: {n_top} samples, {precision_top*100:.1f}% precision\")\n",
    "    print(f\"\\nPrecision after REMOVING lowest-confidence samples:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Remove':<10} {'Remaining':<12} {'Correct':<10} {'Precision':<12} {'Change':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for remove_pct in [0, 10, 20, 30, 40, 50]:\n",
    "        n_remove = int(len(top_scores_sorted) * remove_pct / 100)\n",
    "        remaining = top_scores_sorted[n_remove:]  # Remove lowest scores\n",
    "        \n",
    "        n_remaining = len(remaining)\n",
    "        n_correct = sum(1 for s in remaining if s['is_correct'])\n",
    "        prec = n_correct / n_remaining * 100 if n_remaining > 0 else 0\n",
    "        change = prec - precision_top * 100\n",
    "        \n",
    "        marker = \" **\" if change > 5 else \"\"\n",
    "        print(f\"{remove_pct:>3}%       {n_remaining:<12} {n_correct:<10} {prec:>6.1f}%      {change:>+6.1f}pp{marker}\")\n",
    "    \n",
    "    print(\"\\n** = >5pp improvement over original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lft4j7pu59",
   "metadata": {},
   "source": [
    "---\n",
    "# Corrected Iterative Cycle: Gradient-Based with Loss as Context\n",
    "\n",
    "The previous loss-only approach failed (precision decreased from 67.6% to 52.9%).\n",
    "\n",
    "**Correct approach:**\n",
    "- Use **adaptive context selection** (gradient-only OR gradient+context per class)\n",
    "- Include **LOSS** in the enriched context (alongside features)\n",
    "- Classes with good gradient separation → use gradient-only\n",
    "- Classes with poor gradient separation → use gradient + features + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "okadf6fgly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions with loss context defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data_with_loss(analysis, DO, loss_data):\n",
    "    \"\"\"\n",
    "    Compute anchors that include loss in the enriched representation.\n",
    "    \n",
    "    For each class:\n",
    "    - Compute gradient-only anchors (same as before)\n",
    "    - Compute enriched anchors: gradient + features + normalized_loss\n",
    "    - Decide which to use based on gradient anchor similarity\n",
    "    \"\"\"\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Storage\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    feature_norm_params = {}\n",
    "    loss_norm_params = {}\n",
    "    \n",
    "    # Get gradient scale for normalization\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect data for correct hypotheses\n",
    "        correct_grads, correct_features, correct_losses = [], [], []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                correct_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                correct_losses.append(loss_data[gid])\n",
    "        \n",
    "        # Collect data for incorrect hypotheses\n",
    "        incorrect_grads, incorrect_features, incorrect_losses = [], [], []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                incorrect_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                incorrect_losses.append(loss_data[gid])\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "        \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Convert to arrays\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        correct_losses = np.array(correct_losses, dtype=np.float64)\n",
    "        incorrect_losses = np.array(incorrect_losses, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Normalize losses\n",
    "        all_losses = np.concatenate([correct_losses, incorrect_losses])\n",
    "        loss_mean = np.mean(all_losses)\n",
    "        loss_std = np.std(all_losses) + 1e-8\n",
    "        loss_norm_params[class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "        \n",
    "        # Negate loss: lower loss = higher value (more likely correct)\n",
    "        correct_losses_norm = -((correct_losses - loss_mean) / loss_std) * grad_scale\n",
    "        incorrect_losses_norm = -((incorrect_losses - loss_mean) / loss_std) * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm, correct_losses_norm.reshape(-1, 1)])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm, incorrect_losses_norm.reshape(-1, 1)])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'loss_norm_params': loss_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method with loss included in enriched context.\n",
    "    \n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features + loss) for classes with poor gradient separation\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Normalize features\n",
    "        feat_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if feat_params:\n",
    "            features_norm = (features - feat_params['mean']) / feat_params['std'] * feat_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        \n",
    "        # Normalize loss (negated: lower loss = higher value)\n",
    "        loss_params = anchor_data['loss_norm_params'].get(class_id)\n",
    "        if loss_params:\n",
    "            loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "        else:\n",
    "            loss_norm = -loss\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "print(\"Functions with loss context defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vehgbcrq24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON STUDY: GGH vs Partial-Only\n",
      "================================================================================\n",
      "Data splits: 72% train pool, 16% val, 12% test\n",
      "  - Partial trains on: ~2.5% (partial data only, 100% correct)\n",
      "  - GGH trains on: ~2.5% partial + ~15% selected (~73% correct)\n",
      "Number of runs: 15\n",
      "Training epochs: 200 (with validation-based epoch selection)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 55.4% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=12, val_loss=0.0189, test_loss=0.0181, test_mae=0.1003, R2=0.3074\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0176, test_loss=0.0172, test_mae=0.0982, R2=0.3394\n",
      "\n",
      ">>> Improvement: Loss=-0.0008, MAE=-0.0021, R2=-0.0320\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 33.9% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0178, test_loss=0.0189, test_mae=0.1061, R2=0.2253\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0179, test_loss=0.0194, test_mae=0.1102, R2=0.2048\n",
      "\n",
      ">>> Improvement: Loss=+0.0005, MAE=+0.0041, R2=+0.0205\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 41.1% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0198, test_loss=0.0172, test_mae=0.1050, R2=0.2828\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=88, val_loss=0.0196, test_loss=0.0171, test_mae=0.1036, R2=0.2850\n",
      "\n",
      ">>> Improvement: Loss=-0.0001, MAE=-0.0014, R2=-0.0022\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 41.7% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0159, test_loss=0.0161, test_mae=0.1028, R2=0.3087\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0210, test_loss=0.0187, test_mae=0.1101, R2=0.1972\n",
      "\n",
      ">>> Improvement: Loss=+0.0026, MAE=+0.0073, R2=+0.1115\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 34.5% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=27, val_loss=0.0225, test_loss=0.0212, test_mae=0.1073, R2=0.1493\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0244, test_loss=0.0203, test_mae=0.1061, R2=0.1838\n",
      "\n",
      ">>> Improvement: Loss=-0.0009, MAE=-0.0012, R2=-0.0345\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "GGH selection: 168 hypotheses at 43.5% precision\n",
      "GGH trains on: 196 hypotheses (selection + partial)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=42, val_loss=0.0160, test_loss=0.0185, test_mae=0.1093, R2=0.2280\n",
      "\n",
      "--- Partial Only ---\n",
      "Partial trains on: 28 hypotheses (partial only)\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=52, val_loss=0.0174, test_loss=0.0194, test_mae=0.1114, R2=0.1901\n",
      "\n",
      ">>> Improvement: Loss=+0.0009, MAE=+0.0021, R2=+0.0379\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Total: 1151 samples, 3453 hypotheses\n",
      "Partial: 28 hypotheses (0.8%)\n",
      "\n",
      "--- GGH Method ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON STUDY: GGH vs Partial-Only (15 Random States)\n",
    "# =============================================================================\n",
    "# Compare:\n",
    "#   - GGH: Single-pass pruning (~73% precision) + partial (~2.5%)\n",
    "#   - Partial: Partial data only (~2.5%)\n",
    "# Both: 200 epochs, validation-based epoch selection, test evaluation\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train model with validation-based epoch selection.\n",
    "    Returns best model state, best epoch, and validation losses.\n",
    "    \"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, val_losses, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        # Calculate R2 score\n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def run_ggh_single_pass(DO, rand_state, n_epochs_selection=30):\n",
    "    \"\"\"\n",
    "    Run GGH single-pass pruning to get high-precision selection.\n",
    "    Returns selected_gids and precision.\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    # Get data parameters\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    inpt_vars = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_shared_features = len(DO.inpt_vars)\n",
    "    n_hypothesis_features = len(DO.miss_vars)\n",
    "    output_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Create dataloader with global_ids\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === STEP 1: Train unbiased model on all hypotheses ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=0.01)\n",
    "    # Train with track_data=False for most epochs, then track_data=True for final epoch\n",
    "    for epoch in range(n_epochs_selection - 1):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    # Final epoch with track_data=True to collect hypothesis analysis\n",
    "    trainer_unbiased.train_epoch(dataloader, n_epochs_selection - 1, track_data=True)\n",
    "    \n",
    "    # === STEP 2: Use Adaptive Context Selection to get top 30% ===\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    all_selections, _ = select_hypotheses_adaptive(trainer_unbiased, DO, anchor_data)\n",
    "    selections = all_selections.copy()  # Both point to same list structure\n",
    "    \n",
    "    # Get top 30%\n",
    "    selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_30_pct = int(len(selections) * 0.30)\n",
    "    top_selections = selections[:top_30_pct]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    \n",
    "    # Track sample_to_gid\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # === STEP 3: Train biased model on top 30% + partial ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, partial_weight=2.0, lr=0.01)\n",
    "    for epoch in range(n_epochs_selection):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # === STEP 4: Score top 30% with biased model, prune bottom 50% ===\n",
    "    # First, score PARTIAL data to build anchors\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=5)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Build loss data for partial samples\n",
    "    partial_loss_data = {gid: partial_analysis[gid]['avg_loss'] \n",
    "                        for gid in partial_analysis if partial_analysis[gid]['avg_loss'] is not None}\n",
    "    \n",
    "    # Compute anchors from PARTIAL data\n",
    "    anchor_data_biased = compute_anchor_data_with_loss(partial_analysis, DO, partial_loss_data)\n",
    "    \n",
    "    # Now score top 30% samples\n",
    "    scorer = RemainingDataScorer(DO, model_biased, top_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=5)\n",
    "    analysis = scorer.get_analysis()\n",
    "    \n",
    "    \n",
    "    # Score each sample using anchors from partial data\n",
    "    scored_samples = []\n",
    "    for sample_idx in top_sample_indices:\n",
    "        gid = sample_to_gid.get(sample_idx)\n",
    "        if not gid:\n",
    "            continue\n",
    "        if gid not in analysis:\n",
    "            continue\n",
    "        if analysis[gid]['avg_gradient'] is None:\n",
    "            continue\n",
    "        \n",
    "        gradient = analysis[gid]['avg_gradient']\n",
    "        loss = analysis[gid]['avg_loss']\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        features = DO.df_train_hypothesis.loc[gid, DO.inpt_vars].values.astype(np.float64)\n",
    "        is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        score = compute_adaptive_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "        scored_samples.append({\n",
    "            'sample_idx': sample_idx, 'gid': gid, 'score': score, 'is_correct': is_correct\n",
    "        })\n",
    "    \n",
    "    \n",
    "    # Sort and keep top 50% (prune bottom 50%)\n",
    "    scored_samples.sort(key=lambda x: x['score'], reverse=True)\n",
    "    n_keep = len(scored_samples) // 2\n",
    "    kept_samples = scored_samples[:n_keep]\n",
    "    \n",
    "    # Calculate precision\n",
    "    n_correct = sum(1 for s in kept_samples if s['is_correct'])\n",
    "    precision = n_correct / len(kept_samples) * 100 if kept_samples else 0\n",
    "    \n",
    "    # Return selected gids\n",
    "    selected_gids = set(s['gid'] for s in kept_samples)\n",
    "    \n",
    "    return selected_gids, precision, partial_correct_gids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON STUDY: GGH vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Data splits: 72% train pool, 16% val, 12% test\")\n",
    "print(f\"  - Partial trains on: ~2.5% (partial data only, 100% correct)\")\n",
    "print(f\"  - GGH trains on: ~2.5% partial + ~15% selected (~73% correct)\")\n",
    "print(f\"Number of runs: 15\")\n",
    "print(f\"Training epochs: 200 (with validation-based epoch selection)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_runs = 15\n",
    "n_epochs = 200\n",
    "results = []\n",
    "\n",
    "for run_idx in range(n_runs):\n",
    "    run_rand_state = 42 + run_idx * 100\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{n_runs} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # === Setup DataOperator with proper splits ===\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get dimensions\n",
    "    hyp_per_sample = DO_run.num_hyp_comb\n",
    "    n_shared_features = len(DO_run.inpt_vars)\n",
    "    n_hypothesis_features = len(DO_run.miss_vars)\n",
    "    output_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # Get partial gids\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_total_hyp = len(DO_run.df_train_hypothesis)\n",
    "    n_samples = n_total_hyp // hyp_per_sample\n",
    "    print(f\"Total: {n_samples} samples, {n_total_hyp} hypotheses\")\n",
    "    print(f\"Partial: {len(partial_gids)} hypotheses ({len(partial_gids)/n_total_hyp*100:.1f}%)\")\n",
    "    \n",
    "    # === Method 1: GGH ===\n",
    "    print(\"\\n--- GGH Method ---\")\n",
    "    ggh_selected_gids, ggh_precision, _ = run_ggh_single_pass(DO_run, run_rand_state)\n",
    "    print(f\"GGH selection: {len(ggh_selected_gids)} hypotheses at {ggh_precision:.1f}% precision\")\n",
    "    print(f\"GGH trains on: {len(ggh_selected_gids) + len(partial_gids)} hypotheses (selection + partial)\")\n",
    "    \n",
    "    # Train GGH model\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(\"Training GGH model (200 epochs)...\")\n",
    "    model_ggh, ggh_best_epoch, ggh_val_losses, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=2.0, lr=0.01, n_epochs=n_epochs\n",
    "    )\n",
    "    \n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Method 2: Partial Only ===\n",
    "    print(\"\\n--- Partial Only ---\")\n",
    "    print(f\"Partial trains on: {len(partial_gids)} hypotheses (partial only)\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16, hypothesis_hidden=32, final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(\"Training Partial model (200 epochs)...\")\n",
    "    model_partial, partial_best_epoch, partial_val_losses, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(),  # No GGH selections, only partial\n",
    "        partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=0.01, n_epochs=n_epochs\n",
    "    )\n",
    "    \n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # === Record results ===\n",
    "    results.append({\n",
    "        'run': run_idx + 1,\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_n_selected': len(ggh_selected_gids),\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'ggh_val_loss': ggh_best_val_loss,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'partial_val_loss': partial_best_val_loss,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON STUDY RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'GGH Prec':<10} {'GGH Loss':<12} {'Partial Loss':<14} {'Δ Loss':<10} {'GGH R2':<10} {'Part R2':<10} {'Δ R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for r in results:\n",
    "    print(f\"{r['run']:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<14.4f} {r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_maes = [r['ggh_test_mae'] for r in results]\n",
    "partial_maes = [r['partial_test_mae'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\nGGH Selection Precision: {np.mean(ggh_precisions):.1f}% ± {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f} ± {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f} ± {np.std(partial_losses):.4f}\")\n",
    "\n",
    "print(f\"\\nTest MAE:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_maes):.4f} ± {np.std(ggh_maes):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_maes):.4f} ± {np.std(partial_maes):.4f}\")\n",
    "\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} ± {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f} ± {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests (paired t-test)\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_mae, p_value_mae = stats.ttest_rel(ggh_maes, partial_maes)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  MAE:  t={t_stat_mae:.3f}, p={p_value_mae:.4f} {'*' if p_value_mae < 0.05 else ''}\")\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_mae = sum(1 for r in results if r['ggh_test_mae'] < r['partial_test_mae'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{n_runs} ({n_ggh_wins_loss/n_runs*100:.1f}%)\")\n",
    "print(f\"  GGH wins (MAE):  {n_ggh_wins_mae}/{n_runs} ({n_ggh_wins_mae/n_runs*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{n_runs} ({n_ggh_wins_r2/n_runs*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(n_runs)\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(n_runs)])\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax2 = axes[1]\n",
    "bp = ax2.boxplot([ggh_losses, partial_losses], labels=['GGH', 'Partial'])\n",
    "ax2.set_ylabel('Test Loss (MSE)')\n",
    "ax2.set_title('Test Loss Distribution')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: GGH Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, n_runs+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('GGH Selection Precision (%)')\n",
    "ax3.set_title('GGH Hypothesis Precision')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/comparison_study_15runs.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement = np.mean([r['improvement_loss'] for r in results])\n",
    "if avg_improvement > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH significantly OUTPERFORMS Partial-only (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average improvement: {avg_improvement:.4f} MSE\")\n",
    "elif avg_improvement < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH (p={p_value_loss:.4f})\")\n",
    "    print(f\"GGH is worse by: {-avg_improvement:.4f} MSE\")\n",
    "else:\n",
    "    print(f\"No significant difference between methods (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average difference: {avg_improvement:+.4f} MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbc2dd-1003-4a2d-8bba-61817330a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
