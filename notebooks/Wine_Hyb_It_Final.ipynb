{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64044e-413b-42bc-a586-e11f03bc9c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HypothesisDataset defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that includes global IDs for tracking.\"\"\"\n",
    "    def __init__(self, DO):\n",
    "        # Input features = inpt_vars + hypothesis column\n",
    "        input_cols = DO.inpt_vars + [f'{DO.miss_vars[0]}_hypothesis']\n",
    "        self.inputs = torch.tensor(\n",
    "            DO.df_train_hypothesis[input_cols].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.targets = torch.tensor(\n",
    "            DO.df_train_hypothesis[DO.target_vars].values, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.global_ids = torch.arange(len(self.inputs))\n",
    "        self.input_cols = input_cols\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.global_ids[idx]\n",
    "\n",
    "print(\"HypothesisDataset defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive_header",
   "metadata": {},
   "source": [
    "## Adaptive Context Selection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined_scoring_header",
   "metadata": {},
   "source": [
    "## Combined Loss + Gradient Scoring (for Iteration 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "combined_scoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scoring utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_combined_score(loss, gradient, features, class_id, anchor_data, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Combine loss and gradient signals for scoring.\n",
    "    \n",
    "    For a truth-biased model:\n",
    "    - Lower loss = more likely correct (aligned with truth)\n",
    "    - Gradient similarity to correct anchor = more likely correct\n",
    "    \n",
    "    Final score = (1 - loss_weight) * gradient_score + loss_weight * (-normalized_loss)\n",
    "    Higher score = more likely correct\n",
    "    \"\"\"\n",
    "    # Gradient score (same as adaptive)\n",
    "    grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "    \n",
    "    # Loss score: lower loss = higher score\n",
    "    # We'll normalize this later when we have all losses\n",
    "    loss_score = -loss  # Negative because lower loss is better\n",
    "    \n",
    "    return {\n",
    "        'grad_score': grad_score,\n",
    "        'loss_score': loss_score,\n",
    "        'raw_loss': loss\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_and_combine_scores(all_scores, loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    Normalize loss scores per class and combine with gradient scores.\n",
    "    \n",
    "    Returns combined scores where higher = more likely correct.\n",
    "    \"\"\"\n",
    "    # Group by class\n",
    "    class_losses = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        if class_id not in class_losses:\n",
    "            class_losses[class_id] = []\n",
    "        class_losses[class_id].append(scores['raw_loss'])\n",
    "    \n",
    "    # Compute per-class mean and std for loss normalization\n",
    "    class_stats = {}\n",
    "    for class_id, losses in class_losses.items():\n",
    "        class_stats[class_id] = {\n",
    "            'mean': np.mean(losses),\n",
    "            'std': np.std(losses) + 1e-8\n",
    "        }\n",
    "    \n",
    "    # Normalize and combine\n",
    "    combined_scores = {}\n",
    "    for sample_idx, (gid, scores) in all_scores.items():\n",
    "        class_id = scores['class_id']\n",
    "        stats = class_stats[class_id]\n",
    "        \n",
    "        # Z-score normalize loss (then negate: lower loss = higher score)\n",
    "        normalized_loss_score = -(scores['raw_loss'] - stats['mean']) / stats['std']\n",
    "        \n",
    "        # Combine: weighted average of gradient and loss scores\n",
    "        combined = (1 - loss_weight) * scores['grad_score'] + loss_weight * normalized_loss_score\n",
    "        \n",
    "        combined_scores[sample_idx] = {\n",
    "            'gid': gid,\n",
    "            'combined_score': combined,\n",
    "            'grad_score': scores['grad_score'],\n",
    "            'loss_score': normalized_loss_score,\n",
    "            'raw_loss': scores['raw_loss'],\n",
    "            'class_id': class_id,\n",
    "            'is_correct': scores['is_correct']\n",
    "        }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "print(\"Combined scoring utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_header",
   "metadata": {},
   "source": [
    "## Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "analysis_utils",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def analyze_threshold_precision(all_selections, title=\"Precision Analysis\", verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze precision at different thresholds.\n",
    "    \n",
    "    all_selections: list of (score, is_correct, sample_idx) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    if not all_selections:\n",
    "        print(\"No selections to analyze\")\n",
    "        return None, None\n",
    "    \n",
    "    # Compute precision at different percentiles\n",
    "    results = []\n",
    "    percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        n_include = max(1, int(len(all_selections) * pct / 100))\n",
    "        top_selections = all_selections[:n_include]\n",
    "        n_correct = sum(1 for _, is_correct, _ in top_selections if is_correct)\n",
    "        precision = n_correct / n_include\n",
    "        results.append({\n",
    "            'percentile': pct,\n",
    "            'n_samples': n_include,\n",
    "            'n_correct': n_correct,\n",
    "            'precision': precision\n",
    "        })\n",
    "    \n",
    "    # Compute precision in score bins\n",
    "    scores = [s[0] for s in all_selections]\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    n_bins = 10\n",
    "    bin_results = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        bin_low = min_score + (max_score - min_score) * i / n_bins\n",
    "        bin_high = min_score + (max_score - min_score) * (i + 1) / n_bins\n",
    "        bin_selections = [(s, c) for s, c, _ in all_selections if bin_low <= s < bin_high]\n",
    "        if bin_selections:\n",
    "            bin_correct = sum(1 for _, c in bin_selections if c)\n",
    "            bin_results.append({\n",
    "                'bin': f'{bin_low:.2f}-{bin_high:.2f}',\n",
    "                'n_samples': len(bin_selections),\n",
    "                'precision': bin_correct / len(bin_selections)\n",
    "            })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(title)\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\nPrecision by Top Percentile (highest scores first):\")\n",
    "        print(\"-\" * 50)\n",
    "        for r in results:\n",
    "            print(f\"Top {r['percentile']:>3}%: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "        \n",
    "        if bin_results:\n",
    "            print(\"\\nPrecision by Score Bin:\")\n",
    "            print(\"-\" * 50)\n",
    "            for r in bin_results:\n",
    "                print(f\"Score {r['bin']}: {r['n_samples']:>4} samples, precision={r['precision']*100:.1f}%\")\n",
    "    \n",
    "    return results, bin_results\n",
    "\n",
    "\n",
    "def select_hypotheses_adaptive(trainer, DO, anchor_data=None):\n",
    "    \"\"\"\n",
    "    Select best hypothesis per sample using adaptive context.\n",
    "    Returns list of (score, is_correct, sample_idx) sorted by score descending.\n",
    "    \"\"\"\n",
    "    if anchor_data is None:\n",
    "        anchor_data = compute_anchor_data(trainer, DO)\n",
    "    \n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    partial_sample_indices = anchor_data['partial_sample_indices']\n",
    "    blacklisted_gids = anchor_data['blacklisted_gids']\n",
    "    \n",
    "    all_selections = []\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_is_correct = False\n",
    "        best_gid = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_adaptive_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return all_selections, anchor_data\n",
    "\n",
    "print(\"Analysis utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration1_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 1: Unbiased Training\n",
    "\n",
    "Train on ALL hypotheses equally. No selection = no feedback loop bias.\n",
    "Use Adaptive Context Selection to score hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "iteration1_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack partial coverage: False\n",
      "Number of training hypotheses: 3453\n",
      "Hypotheses per sample: 3\n",
      "Number of samples: 1151\n",
      "Partial data samples: 28\n"
     ]
    }
   ],
   "source": [
    "# Initialize data\n",
    "set_to_deterministic(rand_state)\n",
    "\n",
    "DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "                  partial_perc, rand_state, device='cpu')\n",
    "DO.problem_type = 'regression'\n",
    "\n",
    "print(f\"Lack partial coverage: {DO.lack_partial_coverage}\")\n",
    "print(f\"Number of training hypotheses: {len(DO.df_train_hypothesis)}\")\n",
    "print(f\"Hypotheses per sample: {DO.num_hyp_comb}\")\n",
    "print(f\"Number of samples: {len(DO.df_train_hypothesis) // DO.num_hyp_comb}\")\n",
    "\n",
    "# Count partial data\n",
    "partial_correct_gids = DO.df_train_hypothesis[\n",
    "    (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "    (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "].index.tolist()\n",
    "print(f\"Partial data samples: {len(partial_correct_gids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "iteration1_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input structure:\n",
      "  Total input size: 4\n",
      "  Shared features: 3\n",
      "  Hypothesis feature: 1\n",
      "\n",
      "Model created: HypothesisAmplifyingModel\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create dataloader\n",
    "    dataset = HypothesisDataset(DO)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Check input structure\n",
    "    input_size = dataset.inputs.shape[1]\n",
    "    n_shared_features = len(inpt_vars)\n",
    "    n_hypothesis_features = 1\n",
    "    \n",
    "    print(f\"\\nInput structure:\")\n",
    "    print(f\"  Total input size: {input_size}\")\n",
    "    print(f\"  Shared features: {n_shared_features}\")\n",
    "    print(f\"  Hypothesis feature: {n_hypothesis_features}\")\n",
    "    \n",
    "    # Create model\n",
    "    model_iter1 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel created: HypothesisAmplifyingModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "iteration1_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 1: Unbiased Training\n",
      "======================================================================\n",
      "Training on ALL hypotheses equally for 60 epochs\n",
      "Tracking gradients in last 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [00:00<00:01, 27.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60: Loss = 0.0235 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 45/60 [00:01<00:00, 27.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/60: Loss = 0.0216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:07<00:00,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60: Loss = 0.0216 (tracking)\n",
      "\n",
      "Iteration 1 complete. Final loss: 0.0216\n",
      "Tracked 3453 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Iteration 1\n",
    "if not DO.lack_partial_coverage:\n",
    "    trainer_iter1 = UnbiasedTrainer(DO, model_iter1, lr=lr)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 1: Unbiased Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training on ALL hypotheses equally for {iter1_epochs} epochs\")\n",
    "    print(f\"Tracking gradients in last {iter1_analysis_epochs} epochs\")\n",
    "    \n",
    "    iter1_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter1_epochs)):\n",
    "        # Track data in last N epochs\n",
    "        track = epoch >= (iter1_epochs - iter1_analysis_epochs)\n",
    "        \n",
    "        loss = trainer_iter1.train_epoch(dataloader, epoch, track_data=track)\n",
    "        iter1_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            status = \"(tracking)\" if track else \"\"\n",
    "            print(f\"Epoch {epoch+1}/{iter1_epochs}: Loss = {loss:.4f} {status}\")\n",
    "    \n",
    "    print(f\"\\nIteration 1 complete. Final loss: {iter1_losses[-1]:.4f}\")\n",
    "    print(f\"Tracked {len(trainer_iter1.loss_history)} hypotheses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "iteration1_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Selection Analysis (Adaptive Context)\n",
      "======================================================================\n",
      "Per-class method selection:\n",
      "  Class 0: grad_sim=-0.990 (good) -> GRADIENT-ONLY\n",
      "  Class 1: grad_sim=-0.612 (good) -> GRADIENT-ONLY\n",
      "  Class 2: grad_sim=-0.993 (good) -> GRADIENT-ONLY\n",
      "======================================================================\n",
      "ITERATION 1: Precision by Threshold\n",
      "======================================================================\n",
      "\n",
      "Precision by Top Percentile (highest scores first):\n",
      "--------------------------------------------------\n",
      "Top  10%:  112 samples, precision=22.3%\n",
      "Top  20%:  224 samples, precision=23.7%\n",
      "Top  30%:  336 samples, precision=27.7%\n",
      "Top  40%:  449 samples, precision=30.1%\n",
      "Top  50%:  561 samples, precision=34.6%\n",
      "Top  60%:  673 samples, precision=38.3%\n",
      "Top  70%:  786 samples, precision=40.2%\n",
      "Top  80%:  898 samples, precision=43.4%\n",
      "Top  90%: 1010 samples, precision=47.3%\n",
      "Top 100%: 1123 samples, precision=48.3%\n",
      "\n",
      "Precision by Score Bin:\n",
      "--------------------------------------------------\n",
      "Score -1.68--1.31:    2 samples, precision=50.0%\n",
      "Score -1.31--0.94:    1 samples, precision=0.0%\n",
      "Score 0.89-1.26:    1 samples, precision=100.0%\n",
      "Score 1.26-1.63:    1 samples, precision=0.0%\n",
      "Score 1.63-2.00: 1117 samples, precision=48.3%\n"
     ]
    }
   ],
   "source": [
    "# Analyze Iteration 1 results\n",
    "if not DO.lack_partial_coverage:\n",
    "    # Get selections with adaptive context\n",
    "    all_selections_iter1, anchor_data_iter1 = select_hypotheses_adaptive(trainer_iter1, DO)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 1: Selection Analysis (Adaptive Context)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print_adaptive_method_summary(anchor_data_iter1, hyp_per_sample)\n",
    "    \n",
    "    # Convert to format for analysis\n",
    "    selections_for_analysis = [(s[0], s[1], s[2]) for s in all_selections_iter1]\n",
    "    results_iter1, _ = analyze_threshold_precision(\n",
    "        selections_for_analysis, \n",
    "        title=\"ITERATION 1: Precision by Threshold\"\n",
    "    )\n",
    "    \n",
    "    # Store precision for comparison\n",
    "    iter1_precision = {r['percentile']: r['precision'] for r in results_iter1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "iteration1_select_top",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 1: Top 30% Selection\n",
      "======================================================================\n",
      "Selected 336 samples (top 30%)\n",
      "Correct: 93 (27.7% precision)\n",
      "Remaining samples: 787\n"
     ]
    }
   ],
   "source": [
    "# Select top N% for Iteration 2\n",
    "if not DO.lack_partial_coverage:\n",
    "    n_total = len(all_selections_iter1)\n",
    "    n_top = max(1, int(n_total * top_percentile / 100))\n",
    "    \n",
    "    top_selections = all_selections_iter1[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    top_gids = set(s[3] for s in top_selections)\n",
    "    \n",
    "    # Remaining samples (not in top N%)\n",
    "    remaining_sample_indices = set(s[2] for s in all_selections_iter1[n_top:])\n",
    "    \n",
    "    # Count correct in top selection\n",
    "    n_correct_top = sum(1 for s in top_selections if s[1])\n",
    "    precision_top = n_correct_top / n_top\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ITERATION 1: Top {top_percentile}% Selection\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Selected {n_top} samples (top {top_percentile}%)\")\n",
    "    print(f\"Correct: {n_correct_top} ({precision_top*100:.1f}% precision)\")\n",
    "    print(f\"Remaining samples: {len(remaining_sample_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration2_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ITERATION 2: Biased Training\n",
    "\n",
    "Train a NEW model on:\n",
    "- Top 30% from Iteration 1 (high precision selections)\n",
    "- Partial data with upweighting (~25% of effective training)\n",
    "\n",
    "This creates a \"truth-biased\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "iteration2_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 2: Biased Training Setup\n",
      "======================================================================\n",
      "Training data:\n",
      "  Selected (top 30%): 336 samples\n",
      "  Partial data: 28 samples\n",
      "  Partial weight: 4.00x\n",
      "  Effective partial: 112.0 (25.0% of training)\n",
      "  Effective total: 448.0\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Get partial data GIDs (correct hypotheses from partial data)\n",
    "    partial_gids = set(anchor_data_iter1['partial_correct_gids'])\n",
    "    n_partial = len(partial_gids)\n",
    "    n_selected = len(top_gids)\n",
    "    \n",
    "    # Calculate weight for partial data\n",
    "    # Target: partial should be ~25% of effective training\n",
    "    # partial_weight * n_partial / (partial_weight * n_partial + n_selected) = 0.25\n",
    "    # Solving: partial_weight = 0.25 * n_selected / (0.75 * n_partial)\n",
    "    partial_weight = (partial_target_ratio * n_selected) / ((1 - partial_target_ratio) * n_partial)\n",
    "    partial_weight = max(1.0, partial_weight)  # At least weight 1\n",
    "    \n",
    "    effective_partial = n_partial * partial_weight\n",
    "    effective_total = effective_partial + n_selected\n",
    "    actual_partial_ratio = effective_partial / effective_total\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 2: Biased Training Setup\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training data:\")\n",
    "    print(f\"  Selected (top {top_percentile}%): {n_selected} samples\")\n",
    "    print(f\"  Partial data: {n_partial} samples\")\n",
    "    print(f\"  Partial weight: {partial_weight:.2f}x\")\n",
    "    print(f\"  Effective partial: {effective_partial:.1f} ({actual_partial_ratio*100:.1f}% of training)\")\n",
    "    print(f\"  Effective total: {effective_total:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "iteration2_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ITERATION 2: Training Biased Model\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:01<00:00, 20.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: Loss = 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2 complete. Final loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Create new model for Iteration 2\n",
    "    set_to_deterministic(rand_state + 1)  # Different seed for variety\n",
    "    \n",
    "    model_iter2 = HypothesisAmplifyingModel(\n",
    "        n_shared_features=n_shared_features,\n",
    "        n_hypothesis_features=n_hypothesis_features,\n",
    "        shared_hidden=16,\n",
    "        hypothesis_hidden=32,\n",
    "        final_hidden=32,\n",
    "        output_size=output_size\n",
    "    )\n",
    "    \n",
    "    # Create biased trainer\n",
    "    trainer_iter2 = BiasedTrainer(\n",
    "        DO, model_iter2, \n",
    "        selected_gids=top_gids,\n",
    "        partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight,\n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ITERATION 2: Training Biased Model\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    iter2_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(iter2_epochs)):\n",
    "        loss = trainer_iter2.train_epoch(dataloader, epoch, track_data=False)\n",
    "        iter2_losses.append(loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{iter2_epochs}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nIteration 2 complete. Final loss: {iter2_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16d0c81f-a098-466d-9b56-03f4b5867440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ITERATION 3: Scoring Remaining Data\n",
      "======================================================================\n",
      "Scoring 787 remaining samples with biased model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ITERATION 3: Scoring Remaining Data\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Scoring {len(remaining_sample_indices)} remaining samples with biased model\")\n",
    "    \n",
    "    # Create scorer\n",
    "    scorer = RemainingDataScorer(DO, model_iter2, remaining_sample_indices)\n",
    "    \n",
    "    # Score remaining data\n",
    "    scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    \n",
    "    # Get analysis\n",
    "    analysis_iter3 = scorer.get_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5249a389-f992-4381-aff3-0f1a9d86f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing anchors from biased model on partial data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n",
      "\n",
      "Biased model anchor similarities:\n",
      "  Class 0: grad_sim = -1.000\n",
      "  Class 1: grad_sim = -0.989\n",
      "  Class 2: grad_sim = -1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DO.lack_partial_coverage:\n",
    "    # Compute new anchors using the biased model's view of partial data\n",
    "    # We need to score partial data with the biased model too\n",
    "    \n",
    "    print(\"\\nComputing anchors from biased model on partial data...\")\n",
    "    \n",
    "    # Score partial data with biased model\n",
    "    partial_sample_indices = anchor_data_iter1['partial_sample_indices']\n",
    "    partial_scorer = RemainingDataScorer(DO, model_iter2, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    \n",
    "    # Build anchor data from partial scores\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Create anchor data structure similar to compute_anchor_data\n",
    "    anchor_data_iter3 = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_similarity_grad': {},\n",
    "        'use_enriched': {},  # For now, use gradient-only for simplicity\n",
    "        'input_cols': inpt_vars,\n",
    "        'partial_correct_gids': anchor_data_iter1['partial_correct_gids'],\n",
    "        'blacklisted_gids': anchor_data_iter1['blacklisted_gids'],\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "    }\n",
    "    \n",
    "    # Compute anchors per class\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads = []\n",
    "        incorrect_grads = []\n",
    "        \n",
    "        for gid in anchor_data_iter1['partial_correct_gids']:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "        \n",
    "        for gid in anchor_data_iter1['blacklisted_gids']:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            anchor_data_iter3['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_iter3['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            # Compute similarity\n",
    "            sim = float(np.dot(\n",
    "                anchor_data_iter3['anchor_correct_grad'][class_id],\n",
    "                anchor_data_iter3['anchor_incorrect_grad'][class_id]\n",
    "            ) / (\n",
    "                np.linalg.norm(anchor_data_iter3['anchor_correct_grad'][class_id]) * \n",
    "                np.linalg.norm(anchor_data_iter3['anchor_incorrect_grad'][class_id]) + 1e-8\n",
    "            ))\n",
    "            anchor_data_iter3['anchor_similarity_grad'][class_id] = sim\n",
    "            anchor_data_iter3['use_enriched'][class_id] = False  # Gradient-only for now\n",
    "    \n",
    "    print(\"\\nBiased model anchor similarities:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        sim = anchor_data_iter3['anchor_similarity_grad'].get(class_id, None)\n",
    "        if sim is not None:\n",
    "            print(f\"  Class {class_id}: grad_sim = {sim:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "tsq1hztx7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\n",
      "======================================================================\n",
      "\n",
      "Scoring Iter1's top 30% (336 samples) with biased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:01<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Scored 336 samples\n",
      "\n",
      "Original top 30%: 336 samples, 27.7% precision\n",
      "\n",
      "Precision after REMOVING lowest-confidence samples:\n",
      "------------------------------------------------------------\n",
      "Remove     Remaining    Correct    Precision    Change    \n",
      "------------------------------------------------------------\n",
      "  0%       336          93           27.7%        +0.0pp\n",
      " 10%       303          82           27.1%        -0.6pp\n",
      " 20%       269          79           29.4%        +1.7pp\n",
      " 30%       236          74           31.4%        +3.7pp\n",
      " 40%       202          68           33.7%        +6.0pp **\n",
      " 50%       168          60           35.7%        +8.0pp **\n",
      "\n",
      "** = >5pp improvement over original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRUNING APPROACH: Use biased model to remove likely-wrong samples from Iter1's top 30%\n",
    "# =============================================================================\n",
    "if not DO.lack_partial_coverage:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRUNING: Score Iter1's top 30% with biased model, remove lowest confidence\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Score Iter1's top 30% samples with the biased model\n",
    "    print(f\"\\nScoring Iter1's top {top_percentile}% ({n_top} samples) with biased model...\")\n",
    "    \n",
    "    # Create scorer for top samples\n",
    "    top_scorer = RemainingDataScorer(DO, model_iter2, top_sample_indices)\n",
    "    top_scorer.compute_scores(dataloader, n_passes=iter3_analysis_epochs)\n",
    "    top_analysis = top_scorer.get_analysis()\n",
    "    \n",
    "    # Also need to score partial data with biased model to get anchors (reuse from before)\n",
    "    # anchor_data_iter3 was already computed\n",
    "    \n",
    "    # Compute combined scores for each top sample\n",
    "    top_scores = []\n",
    "    for sample_idx in top_sample_indices:\n",
    "        start = sample_idx * hyp_per_sample\n",
    "        \n",
    "        # Find the gid that Iter1 selected for this sample\n",
    "        iter1_selected_gid = None\n",
    "        for s in top_selections:\n",
    "            if s[2] == sample_idx:\n",
    "                iter1_selected_gid = s[3]\n",
    "                break\n",
    "        \n",
    "        if iter1_selected_gid is None or iter1_selected_gid not in top_analysis:\n",
    "            continue\n",
    "        \n",
    "        gid = iter1_selected_gid\n",
    "        if top_analysis[gid]['avg_gradient'] is None:\n",
    "            continue\n",
    "            \n",
    "        gradient = top_analysis[gid]['avg_gradient']\n",
    "        loss = top_analysis[gid]['avg_loss']\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        features = DO.df_train_hypothesis.loc[gid, inpt_vars].values.astype(np.float64)\n",
    "        is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        # Compute gradient score using biased model anchors\n",
    "        grad_score = compute_adaptive_score(gradient, features, class_id, anchor_data_iter3)\n",
    "        \n",
    "        # Use negative loss as score (lower loss = higher score)\n",
    "        loss_score = -loss\n",
    "        \n",
    "        # Combined score (can tune weights)\n",
    "        combined = 0.5 * grad_score + 0.5 * loss_score\n",
    "        \n",
    "        top_scores.append({\n",
    "            'sample_idx': sample_idx,\n",
    "            'gid': gid,\n",
    "            'is_correct': is_correct,\n",
    "            'grad_score': grad_score,\n",
    "            'loss': loss,\n",
    "            'combined_score': combined\n",
    "        })\n",
    "    \n",
    "    print(f\"Scored {len(top_scores)} samples\")\n",
    "    \n",
    "    # Sort by combined score (ascending - lowest scores are most likely wrong)\n",
    "    top_scores_sorted = sorted(top_scores, key=lambda x: x['combined_score'])\n",
    "    \n",
    "    # Analyze precision after removing bottom N%\n",
    "    print(f\"\\nOriginal top {top_percentile}%: {n_top} samples, {precision_top*100:.1f}% precision\")\n",
    "    print(f\"\\nPrecision after REMOVING lowest-confidence samples:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Remove':<10} {'Remaining':<12} {'Correct':<10} {'Precision':<12} {'Change':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for remove_pct in [0, 10, 20, 30, 40, 50]:\n",
    "        n_remove = int(len(top_scores_sorted) * remove_pct / 100)\n",
    "        remaining = top_scores_sorted[n_remove:]  # Remove lowest scores\n",
    "        \n",
    "        n_remaining = len(remaining)\n",
    "        n_correct = sum(1 for s in remaining if s['is_correct'])\n",
    "        prec = n_correct / n_remaining * 100 if n_remaining > 0 else 0\n",
    "        change = prec - precision_top * 100\n",
    "        \n",
    "        marker = \" **\" if change > 5 else \"\"\n",
    "        print(f\"{remove_pct:>3}%       {n_remaining:<12} {n_correct:<10} {prec:>6.1f}%      {change:>+6.1f}pp{marker}\")\n",
    "    \n",
    "    print(\"\\n** = >5pp improvement over original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08725mtihlh9",
   "metadata": {},
   "source": [
    "---\n",
    "# Multi-Random-State Precision Analysis\n",
    "\n",
    "Test precision across multiple random states to understand algorithm robustness.\n",
    "Compare different selection strategies:\n",
    "1. **Adaptive Context**: Use enriched (gradient + features) only when gradient separation is poor\n",
    "2. **Always Enriched**: Always use gradient + features\n",
    "3. **Gradient Only**: Only use gradients (ignore features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "je3au7ks60h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "COMPREHENSIVE GRID SEARCH: Pruning Strategies & Hyperparameters\n",
      "==========================================================================================\n",
      "Fixed: Unbiased selection (50 epochs, 1 analysis epoch, enriched scoring)\n",
      "\n",
      "Pruning hyperparameters tested:\n",
      "  - iter2_epochs: [30, 60]\n",
      "  - partial_weight: [2.0 (current), 6.0 (triple)]\n",
      "\n",
      "Scoring strategies:\n",
      "  - grad_only: Gradient similarity only\n",
      "  - loss_only: Loss score only\n",
      "  - combined_0.5: 0.5*grad + 0.5*(-loss)\n",
      "  - inverted_loss: grad + loss (prefer higher loss)\n",
      "  - enriched_only: Gradient + context features (NEW)\n",
      "  - enriched_loss: Enriched + loss (NEW)\n",
      "==========================================================================================\n",
      "\n",
      "Run 1/15 (rand_state=0)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 56.0%\n",
      "\n",
      "Run 2/15 (rand_state=1)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 69.6%\n",
      "\n",
      "Run 3/15 (rand_state=2)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 43.8%\n",
      "\n",
      "Run 4/15 (rand_state=3)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 53.0%\n",
      "\n",
      "Run 5/15 (rand_state=4)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 58.0%\n",
      "\n",
      "Run 6/15 (rand_state=5)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 63.4%\n",
      "\n",
      "Run 7/15 (rand_state=6)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 62.8%\n",
      "\n",
      "Run 8/15 (rand_state=7)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 51.5%\n",
      "\n",
      "Run 9/15 (rand_state=8)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 47.9%\n",
      "\n",
      "Run 10/15 (rand_state=9)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 52.1%\n",
      "\n",
      "Run 11/15 (rand_state=10)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:01<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 54.2%\n",
      "\n",
      "Run 12/15 (rand_state=11)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 39.9%\n",
      "\n",
      "Run 13/15 (rand_state=12)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 50.9%\n",
      "\n",
      "Run 14/15 (rand_state=13)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 52.7%\n",
      "\n",
      "Run 15/15 (rand_state=14)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "Iter1: 48.8%\n",
      "\n",
      "==========================================================================================\n",
      "SUMMARY: Mean Precision by Strategy & Configuration\n",
      "==========================================================================================\n",
      "Baseline (no pruning): 53.6%\n",
      "\n",
      "Config         grad_only     loss_only     combined_0.5  inverted_loss enriched_only enriched_loss \n",
      "---------------------------------------------------------------------------------------------------\n",
      "e30_pw2.0       54.0%(++0.4)  56.1%(++2.5)  55.0%(++1.4)  53.5%(-0.2)  57.8%(++4.2)  57.7%(++4.1) \n",
      "e30_pw6.0       54.2%(++0.5)  55.8%(++2.2)  55.1%(++1.5)  53.8%(++0.1)  57.8%(++4.1)  57.7%(++4.1) \n",
      "e60_pw2.0       54.8%(++1.1)  55.7%(++2.0)  55.8%(++2.2)  53.8%(++0.1)  57.3%(++3.7)  57.3%(++3.7) \n",
      "e60_pw6.0       54.4%(++0.7)  55.5%(++1.9)  55.8%(++2.2)  53.6%(-0.1)  57.7%(++4.0)  57.6%(++4.0) \n",
      "\n",
      "==========================================================================================\n",
      "BEST CONFIGURATIONS\n",
      "==========================================================================================\n",
      "Best for grad_only      : e60_pw2.0 -> 54.8% (+1.1pp vs baseline)\n",
      "Best for loss_only      : e30_pw2.0 -> 56.1% (+2.5pp vs baseline)\n",
      "Best for combined_0.5   : e60_pw6.0 -> 55.8% (+2.2pp vs baseline)\n",
      "Best for inverted_loss  : e60_pw2.0 -> 53.8% (+0.1pp vs baseline)\n",
      "Best for enriched_only  : e30_pw2.0 -> 57.8% (+4.2pp vs baseline)\n",
      "Best for enriched_loss  : e30_pw6.0 -> 57.7% (+4.1pp vs baseline)\n",
      "\n",
      "OVERALL BEST: enriched_only with e30_pw2.0 -> 57.8%\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON: Enriched vs Non-Enriched Strategies\n",
      "==========================================================================================\n",
      "\n",
      "e30_pw2.0:\n",
      "  grad_only:     54.0%\n",
      "  enriched_only: 57.8% (+3.8pp vs grad_only)\n",
      "  combined_0.5:  55.0%\n",
      "  enriched_loss: 57.7% (+2.7pp vs combined)\n",
      "\n",
      "e30_pw6.0:\n",
      "  grad_only:     54.2%\n",
      "  enriched_only: 57.8% (+3.6pp vs grad_only)\n",
      "  combined_0.5:  55.1%\n",
      "  enriched_loss: 57.7% (+2.6pp vs combined)\n",
      "\n",
      "e60_pw2.0:\n",
      "  grad_only:     54.8%\n",
      "  enriched_only: 57.3% (+2.6pp vs grad_only)\n",
      "  combined_0.5:  55.8%\n",
      "  enriched_loss: 57.3% (+1.5pp vs combined)\n",
      "\n",
      "e60_pw6.0:\n",
      "  grad_only:     54.4%\n",
      "  enriched_only: 57.7% (+3.3pp vs grad_only)\n",
      "  combined_0.5:  55.8%\n",
      "  enriched_loss: 57.6% (+1.8pp vs combined)\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON: Partial Weight Impact (best strategy per weight)\n",
      "==========================================================================================\n",
      "\n",
      "Partial Weight = 2.0:\n",
      "  30 epochs: enriched_only -> 57.8%\n",
      "  60 epochs: enriched_loss -> 57.3%\n",
      "\n",
      "Partial Weight = 6.0:\n",
      "  30 epochs: enriched_only -> 57.8%\n",
      "  60 epochs: enriched_only -> 57.7%\n",
      "\n",
      "==========================================================================================\n",
      "DETAILED: Per-Run Best Configuration\n",
      "==========================================================================================\n",
      "State    Iter1      Best Config          Best Strat       Prec       Improve   \n",
      "------------------------------------------------------------------------------------\n",
      "0          56.0%    e30_pw6.0            grad_only          62.5%      +6.5pp\n",
      "1          69.6%    e60_pw2.0            combined_0.5       75.1%      +5.5pp\n",
      "2          43.8%    e30_pw2.0            enriched_only      47.2%      +3.5pp\n",
      "3          53.0%    e30_pw2.0            enriched_loss      57.6%      +4.6pp\n",
      "4          58.0%    e60_pw6.0            enriched_only      66.9%      +8.9pp\n",
      "5          63.4%    e60_pw6.0            loss_only          68.4%      +5.0pp\n",
      "6          62.8%    e30_pw2.0            combined_0.5       69.9%      +7.1pp\n",
      "7          51.5%    e30_pw2.0            enriched_only      52.8%      +1.3pp\n",
      "8          47.9%    e30_pw2.0            grad_only          52.4%      +4.5pp\n",
      "9          52.1%    e30_pw2.0            enriched_only      59.9%      +7.8pp\n",
      "10         54.2%    e30_pw2.0            enriched_only      61.0%      +6.8pp\n",
      "11         39.9%    e60_pw6.0            inverted_loss      43.9%      +4.0pp\n",
      "12         50.9%    e60_pw2.0            grad_only          55.0%      +4.1pp\n",
      "13         52.7%    e60_pw2.0            inverted_loss      59.9%      +7.2pp\n",
      "14         48.8%    e30_pw2.0            enriched_only      56.1%      +7.3pp\n",
      "\n",
      "==========================================================================================\n",
      "PRUNING QUALITY: Correct vs Incorrect Pruned (averaged)\n",
      "==========================================================================================\n",
      "\n",
      "Config          Strategy         Pruned C     Pruned I     Accuracy    \n",
      "-------------------------------------------------------------------\n",
      "TOP 10 by pruning accuracy (% of pruned that were incorrect):\n",
      "e30_pw2.0       enriched_only        24.7        42.3        63.2%\n",
      "e30_pw6.0       enriched_only        24.8        42.2        63.0%\n",
      "e30_pw6.0       enriched_loss        24.9        42.1        62.9%\n",
      "e30_pw2.0       enriched_loss        24.9        42.1        62.8%\n",
      "e60_pw6.0       enriched_only        25.1        41.9        62.6%\n",
      "e60_pw6.0       enriched_loss        25.3        41.7        62.3%\n",
      "e60_pw2.0       enriched_only        25.9        41.1        61.3%\n",
      "e60_pw2.0       enriched_loss        25.9        41.1        61.3%\n",
      "e30_pw2.0       loss_only            29.2        37.8        56.4%\n",
      "e30_pw6.0       loss_only            30.0        37.0        55.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE DIAGNOSTIC: Pruning Strategy & Hyperparameter Grid Search\n",
    "# =============================================================================\n",
    "# Test different:\n",
    "#   - Scoring strategies (grad, loss, enriched, combinations)\n",
    "#   - Biased training epochs (30, 60)\n",
    "#   - Analysis epochs for pruning (1, 5)\n",
    "#   - Partial weights (2.0, 6.0)\n",
    "# Unbiased selection stage stays fixed (50 epochs, 1 analysis epoch)\n",
    "# =============================================================================\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import grad  # Ensure grad is not overwritten\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute score using ALWAYS ENRICHED method (gradient + features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def select_hypotheses_enriched(trainer, DO, anchor_data):\n",
    "    \"\"\"Select best hypothesis per sample using ALWAYS ENRICHED scoring.\"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    partial_sample_indices = anchor_data['partial_sample_indices']\n",
    "    blacklisted_gids = anchor_data['blacklisted_gids']\n",
    "    \n",
    "    all_selections = []\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_is_correct = False\n",
    "        best_gid = None\n",
    "        best_class = None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "                best_class = class_id\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid, best_class))\n",
    "    \n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    return all_selections\n",
    "\n",
    "\n",
    "def run_pruning_grid_search(rand_state, iter1_epochs=50, iter1_analysis_epochs=1,\n",
    "                            top_percentile=30, prune_percentile=20, scoring_passes=3):\n",
    "    \"\"\"\n",
    "    Run full grid search over pruning hyperparameters.\n",
    "    Unbiased selection is fixed. Only pruning stage varies.\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    DO_test = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, rand_state, device='cpu'\n",
    "    )\n",
    "    DO_test.problem_type = 'regression'\n",
    "    \n",
    "    hyp_per_sample = DO_test.num_hyp_comb\n",
    "    n_shared = len(DO_test.inpt_vars)\n",
    "    n_hyp = len(DO_test.miss_vars)\n",
    "    out_size = len(DO_test.target_vars)\n",
    "    inpt_vars_list = DO_test.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO_test.df_train_hypothesis[\n",
    "        (DO_test.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_test.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO_test.df_train_hypothesis[\n",
    "        (DO_test.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_test.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Create dataloader\n",
    "    test_batch_size = 100 * hyp_per_sample\n",
    "    input_cols = DO_test.inpt_vars + [var + '_hypothesis' for var in DO_test.miss_vars]\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO_test.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO_test.df_train_hypothesis[DO_test.target_vars].values, dtype=torch.float32),\n",
    "        torch.arange(len(DO_test.df_train_hypothesis))\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=test_batch_size, shuffle=True)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ITERATION 1: Unbiased training + enriched selection (FIXED)\n",
    "    # =========================================================================\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO_test, model_unbiased, lr=0.001)\n",
    "    \n",
    "    for epoch in range(iter1_epochs - iter1_analysis_epochs):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    for epoch in range(iter1_epochs - iter1_analysis_epochs, iter1_epochs):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO_test)\n",
    "    all_selections = select_hypotheses_enriched(trainer_unbiased, DO_test, anchor_data)\n",
    "    \n",
    "    n_top = int(len(all_selections) * top_percentile / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    n_correct_iter1 = sum(1 for s in top_selections if s[1])\n",
    "    precision_iter1 = n_correct_iter1 / len(top_selections) * 100\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ITERATION 2: Grid search over pruning configurations\n",
    "    # =========================================================================\n",
    "    iter2_epochs_options = [30, 60]\n",
    "    partial_weight_options = [2.0, 6.0]\n",
    "    analysis_epochs_options = [1, 5]\n",
    "    \n",
    "    all_config_results = {}\n",
    "    \n",
    "    for iter2_epochs, partial_weight in product(iter2_epochs_options, partial_weight_options):\n",
    "        config_key = f\"e{iter2_epochs}_pw{partial_weight}\"\n",
    "        \n",
    "        # Train biased model with this config\n",
    "        set_to_deterministic(rand_state + 100)\n",
    "        model_biased = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n",
    "        \n",
    "        top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "        trainer_biased = BiasedTrainer(DO_test, model_biased, selected_gids=top_gids_set,\n",
    "                                       partial_gids=partial_correct_gids, \n",
    "                                       partial_weight=partial_weight, lr=0.001)\n",
    "        \n",
    "        # Train and track gradients for different analysis epochs\n",
    "        max_analysis = max(analysis_epochs_options)\n",
    "        for epoch in range(iter2_epochs - max_analysis):\n",
    "            trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "        for epoch in range(iter2_epochs - max_analysis, iter2_epochs):\n",
    "            trainer_biased.train_epoch(dataloader, epoch, track_data=True)\n",
    "        \n",
    "        # Build enriched anchors from biased model (for enriched scoring)\n",
    "        partial_scorer = RemainingDataScorer(DO_test, model_biased, partial_sample_indices)\n",
    "        partial_scorer.compute_scores(dataloader, n_passes=scoring_passes)\n",
    "        partial_analysis = partial_scorer.get_analysis()\n",
    "        \n",
    "        anchor_data_iter2 = {\n",
    "            'anchor_correct_grad': {},\n",
    "            'anchor_incorrect_grad': {},\n",
    "            'anchor_correct_enriched': {},\n",
    "            'anchor_incorrect_enriched': {},\n",
    "            'feature_norm_params': {},\n",
    "        }\n",
    "        \n",
    "        # Compute gradient scale for feature normalization\n",
    "        all_grads = [partial_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                     if gid in partial_analysis and partial_analysis[gid]['avg_gradient'] is not None]\n",
    "        if all_grads:\n",
    "            grad_scale = np.mean([np.linalg.norm(g) for g in all_grads])\n",
    "        else:\n",
    "            grad_scale = 1.0\n",
    "        anchor_data_iter2['grad_scale'] = grad_scale\n",
    "        \n",
    "        for class_id in range(hyp_per_sample):\n",
    "            correct_grads = []\n",
    "            incorrect_grads = []\n",
    "            correct_features = []\n",
    "            incorrect_features = []\n",
    "            \n",
    "            for gid in partial_correct_gids:\n",
    "                if gid in partial_analysis and DO_test.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                    if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                        correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                        features = DO_test.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "                        correct_features.append(features)\n",
    "            \n",
    "            for gid in blacklisted_gids:\n",
    "                if gid in partial_analysis and DO_test.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                    if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                        incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                        features = DO_test.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "                        incorrect_features.append(features)\n",
    "            \n",
    "            if correct_grads and incorrect_grads:\n",
    "                anchor_data_iter2['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "                anchor_data_iter2['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "                \n",
    "                # Compute feature normalization and enriched anchors\n",
    "                all_features = correct_features + incorrect_features\n",
    "                feat_mean = np.mean(all_features, axis=0)\n",
    "                feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "                anchor_data_iter2['feature_norm_params'][class_id] = {\n",
    "                    'mean': feat_mean, 'std': feat_std, 'scale': grad_scale\n",
    "                }\n",
    "                \n",
    "                # Enriched anchors\n",
    "                correct_enriched = []\n",
    "                for g, f in zip(correct_grads, correct_features):\n",
    "                    f_norm = (f - feat_mean) / feat_std * grad_scale\n",
    "                    correct_enriched.append(np.concatenate([g, f_norm]))\n",
    "                \n",
    "                incorrect_enriched = []\n",
    "                for g, f in zip(incorrect_grads, incorrect_features):\n",
    "                    f_norm = (f - feat_mean) / feat_std * grad_scale\n",
    "                    incorrect_enriched.append(np.concatenate([g, f_norm]))\n",
    "                \n",
    "                anchor_data_iter2['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "                anchor_data_iter2['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Score top selections with biased model\n",
    "        top_scorer = RemainingDataScorer(DO_test, model_biased, top_sample_indices)\n",
    "        top_scorer.compute_scores(dataloader, n_passes=scoring_passes)\n",
    "        top_analysis = top_scorer.get_analysis()\n",
    "        \n",
    "        # Collect scores for each sample\n",
    "        sample_scores = []\n",
    "        for sample_idx in top_sample_indices:\n",
    "            gid = sample_to_gid.get(sample_idx)\n",
    "            if gid is None or gid not in top_analysis or top_analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = top_analysis[gid]['avg_gradient']\n",
    "            loss = top_analysis[gid]['avg_loss']\n",
    "            class_id = DO_test.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            is_correct = DO_test.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "            features = DO_test.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            \n",
    "            # Compute gradient-only score\n",
    "            anchor_c = anchor_data_iter2['anchor_correct_grad'].get(class_id)\n",
    "            anchor_i = anchor_data_iter2['anchor_incorrect_grad'].get(class_id)\n",
    "            \n",
    "            if anchor_c is not None:\n",
    "                sim_c = float(np.dot(gradient, anchor_c) / (np.linalg.norm(gradient) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "                sim_i = float(np.dot(gradient, anchor_i) / (np.linalg.norm(gradient) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "                grad_score = sim_c - sim_i\n",
    "            else:\n",
    "                grad_score = 0.0\n",
    "            \n",
    "            # Compute enriched score (gradient + features)\n",
    "            anchor_c_enr = anchor_data_iter2['anchor_correct_enriched'].get(class_id)\n",
    "            anchor_i_enr = anchor_data_iter2['anchor_incorrect_enriched'].get(class_id)\n",
    "            norm_params = anchor_data_iter2['feature_norm_params'].get(class_id)\n",
    "            \n",
    "            if anchor_c_enr is not None and norm_params is not None:\n",
    "                f_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "                enriched_vec = np.concatenate([gradient, f_norm])\n",
    "                sim_c_enr = float(np.dot(enriched_vec, anchor_c_enr) / (np.linalg.norm(enriched_vec) * np.linalg.norm(anchor_c_enr) + 1e-8))\n",
    "                sim_i_enr = float(np.dot(enriched_vec, anchor_i_enr) / (np.linalg.norm(enriched_vec) * np.linalg.norm(anchor_i_enr) + 1e-8)) if anchor_i_enr is not None else 0.0\n",
    "                enriched_score = sim_c_enr - sim_i_enr\n",
    "            else:\n",
    "                enriched_score = grad_score  # Fallback\n",
    "            \n",
    "            sample_scores.append({\n",
    "                'sample_idx': sample_idx,\n",
    "                'gid': gid,\n",
    "                'is_correct': is_correct,\n",
    "                'grad_score': grad_score,\n",
    "                'enriched_score': enriched_score,\n",
    "                'loss': loss,\n",
    "            })\n",
    "        \n",
    "        n_remove = int(len(sample_scores) * prune_percentile / 100)\n",
    "        \n",
    "        # Test all scoring strategies\n",
    "        strategies = {\n",
    "            'grad_only': lambda s: s['grad_score'],\n",
    "            'loss_only': lambda s: -s['loss'],\n",
    "            'combined_0.5': lambda s: 0.5 * s['grad_score'] + 0.5 * (-s['loss']),\n",
    "            'inverted_loss': lambda s: s['grad_score'] + s['loss'],\n",
    "            'enriched_only': lambda s: s['enriched_score'],\n",
    "            'enriched_loss': lambda s: 0.5 * s['enriched_score'] + 0.5 * (-s['loss']),\n",
    "        }\n",
    "        \n",
    "        config_results = {}\n",
    "        for strat_name, score_fn in strategies.items():\n",
    "            sorted_samples = sorted(sample_scores, key=score_fn)\n",
    "            pruned = sorted_samples[:n_remove]\n",
    "            kept = sorted_samples[n_remove:]\n",
    "            \n",
    "            pruned_correct = sum(1 for s in pruned if s['is_correct'])\n",
    "            pruned_incorrect = len(pruned) - pruned_correct\n",
    "            kept_correct = sum(1 for s in kept if s['is_correct'])\n",
    "            \n",
    "            precision = kept_correct / len(kept) * 100 if kept else 0\n",
    "            \n",
    "            config_results[strat_name] = {\n",
    "                'precision': precision,\n",
    "                'pruned_correct': pruned_correct,\n",
    "                'pruned_incorrect': pruned_incorrect,\n",
    "            }\n",
    "        \n",
    "        all_config_results[config_key] = config_results\n",
    "    \n",
    "    return {\n",
    "        'precision_iter1': precision_iter1,\n",
    "        'n_top': len(top_selections),\n",
    "        'configs': all_config_results\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN GRID SEARCH: 15 random states\n",
    "# =============================================================================\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPREHENSIVE GRID SEARCH: Pruning Strategies & Hyperparameters\")\n",
    "print(\"=\" * 90)\n",
    "print(\"Fixed: Unbiased selection (50 epochs, 1 analysis epoch, enriched scoring)\")\n",
    "print(\"\\nPruning hyperparameters tested:\")\n",
    "print(\"  - iter2_epochs: [30, 60]\")\n",
    "print(\"  - partial_weight: [2.0 (current), 6.0 (triple)]\")\n",
    "print(\"\\nScoring strategies:\")\n",
    "print(\"  - grad_only: Gradient similarity only\")\n",
    "print(\"  - loss_only: Loss score only\")\n",
    "print(\"  - combined_0.5: 0.5*grad + 0.5*(-loss)\")\n",
    "print(\"  - inverted_loss: grad + loss (prefer higher loss)\")\n",
    "print(\"  - enriched_only: Gradient + context features (NEW)\")\n",
    "print(\"  - enriched_loss: Enriched + loss (NEW)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "test_rand_states = list(range(15))  # Full 15 runs\n",
    "all_results = []\n",
    "\n",
    "for i, rs in enumerate(test_rand_states):\n",
    "    print(f\"\\nRun {i+1}/{len(test_rand_states)} (rand_state={rs})...\", end=\" \", flush=True)\n",
    "    result = run_pruning_grid_search(rs)\n",
    "    all_results.append(result)\n",
    "    print(f\"Iter1: {result['precision_iter1']:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# AGGREGATE RESULTS\n",
    "# =============================================================================\n",
    "# Build configs dynamically from actual options\n",
    "iter2_epochs_options = [30, 60]\n",
    "partial_weight_options = [2.0, 6.0]\n",
    "configs = [f'e{e}_pw{pw}' for e in iter2_epochs_options for pw in partial_weight_options]\n",
    "strategies = ['grad_only', 'loss_only', 'combined_0.5', 'inverted_loss', 'enriched_only', 'enriched_loss']\n",
    "\n",
    "# Compute mean precision for each config x strategy combination\n",
    "mean_results = {}\n",
    "for config in configs:\n",
    "    mean_results[config] = {}\n",
    "    for strat in strategies:\n",
    "        precs = [r['configs'][config][strat]['precision'] for r in all_results]\n",
    "        mean_results[config][strat] = {\n",
    "            'mean': np.mean(precs),\n",
    "            'std': np.std(precs)\n",
    "        }\n",
    "\n",
    "baseline_mean = np.mean([r['precision_iter1'] for r in all_results])\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY TABLE: Best config for each strategy\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"SUMMARY: Mean Precision by Strategy & Configuration\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"Baseline (no pruning): {baseline_mean:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Header\n",
    "print(f\"{'Config':<15}\", end=\"\")\n",
    "for strat in strategies:\n",
    "    print(f\"{strat:<14}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 99)\n",
    "\n",
    "# Data rows\n",
    "for config in configs:\n",
    "    print(f\"{config:<15}\", end=\"\")\n",
    "    for strat in strategies:\n",
    "        val = mean_results[config][strat]['mean']\n",
    "        diff = val - baseline_mean\n",
    "        marker = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"{val:>5.1f}%({marker}{diff:>+4.1f})\", end=\" \")\n",
    "    print()\n",
    "\n",
    "# =============================================================================\n",
    "# FIND BEST OVERALL CONFIGURATION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"BEST CONFIGURATIONS\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "best_overall = None\n",
    "best_overall_prec = 0\n",
    "best_per_strategy = {}\n",
    "\n",
    "for strat in strategies:\n",
    "    best_config = None\n",
    "    best_prec = 0\n",
    "    for config in configs:\n",
    "        prec = mean_results[config][strat]['mean']\n",
    "        if prec > best_prec:\n",
    "            best_prec = prec\n",
    "            best_config = config\n",
    "        if prec > best_overall_prec:\n",
    "            best_overall_prec = prec\n",
    "            best_overall = (config, strat)\n",
    "    best_per_strategy[strat] = (best_config, best_prec)\n",
    "    print(f\"Best for {strat:<15}: {best_config} -> {best_prec:.1f}% ({best_prec - baseline_mean:+.1f}pp vs baseline)\")\n",
    "\n",
    "print(f\"\\nOVERALL BEST: {best_overall[1]} with {best_overall[0]} -> {best_overall_prec:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARISON: Enriched vs Non-Enriched\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"COMPARISON: Enriched vs Non-Enriched Strategies\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n{config}:\")\n",
    "    grad_prec = mean_results[config]['grad_only']['mean']\n",
    "    enriched = mean_results[config]['enriched_only']['mean']\n",
    "    combined = mean_results[config]['combined_0.5']['mean']\n",
    "    enriched_loss = mean_results[config]['enriched_loss']['mean']\n",
    "    \n",
    "    print(f\"  grad_only:     {grad_prec:.1f}%\")\n",
    "    print(f\"  enriched_only: {enriched:.1f}% ({enriched - grad_prec:+.1f}pp vs grad_only)\")\n",
    "    print(f\"  combined_0.5:  {combined:.1f}%\")\n",
    "    print(f\"  enriched_loss: {enriched_loss:.1f}% ({enriched_loss - combined:+.1f}pp vs combined)\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARISON: Partial Weight Impact\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"COMPARISON: Partial Weight Impact (best strategy per weight)\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "for pw in [2.0, 6.0]:\n",
    "    print(f\"\\nPartial Weight = {pw}:\")\n",
    "    for epochs in [30, 60]:\n",
    "        config = f\"e{epochs}_pw{pw}\"\n",
    "        best_strat = max(strategies, key=lambda s: mean_results[config][s]['mean'])\n",
    "        best_prec = mean_results[config][best_strat]['mean']\n",
    "        print(f\"  {epochs} epochs: {best_strat} -> {best_prec:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETAILED: Per-Run Best\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"DETAILED: Per-Run Best Configuration\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'State':<8} {'Iter1':<10} {'Best Config':<20} {'Best Strat':<16} {'Prec':<10} {'Improve':<10}\")\n",
    "print(\"-\" * 84)\n",
    "\n",
    "for i, rs in enumerate(test_rand_states):\n",
    "    r = all_results[i]\n",
    "    iter1 = r['precision_iter1']\n",
    "    \n",
    "    # Find best config + strategy combination\n",
    "    best_config, best_strat, best_prec = None, None, 0\n",
    "    for config in configs:\n",
    "        for strat in strategies:\n",
    "            prec = r['configs'][config][strat]['precision']\n",
    "            if prec > best_prec:\n",
    "                best_prec = prec\n",
    "                best_config = config\n",
    "                best_strat = strat\n",
    "    \n",
    "    improve = best_prec - iter1\n",
    "    print(f\"{rs:<8} {iter1:>6.1f}%    {best_config:<20} {best_strat:<16} {best_prec:>6.1f}%    {improve:>+6.1f}pp\")\n",
    "\n",
    "# =============================================================================\n",
    "# PRUNING QUALITY ANALYSIS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"PRUNING QUALITY: Correct vs Incorrect Pruned (averaged)\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "print(f\"\\n{'Config':<15} {'Strategy':<16} {'Pruned C':<12} {'Pruned I':<12} {'Accuracy':<12}\")\n",
    "print(\"-\" * 67)\n",
    "\n",
    "# Show best 5 configurations by pruning accuracy\n",
    "config_strat_accuracy = []\n",
    "for config in configs:\n",
    "    for strat in strategies:\n",
    "        pruned_c = np.mean([r['configs'][config][strat]['pruned_correct'] for r in all_results])\n",
    "        pruned_i = np.mean([r['configs'][config][strat]['pruned_incorrect'] for r in all_results])\n",
    "        accuracy = pruned_i / (pruned_c + pruned_i) * 100 if (pruned_c + pruned_i) > 0 else 0\n",
    "        config_strat_accuracy.append((config, strat, pruned_c, pruned_i, accuracy))\n",
    "\n",
    "# Sort by accuracy descending\n",
    "config_strat_accuracy.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "print(\"TOP 10 by pruning accuracy (% of pruned that were incorrect):\")\n",
    "for config, strat, pc, pi, acc in config_strat_accuracy[:10]:\n",
    "    print(f\"{config:<15} {strat:<16} {pc:>8.1f}    {pi:>8.1f}    {acc:>8.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c9f0d-921a-422b-8850-507664f2cd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76530a0-0a00-42c1-ba47-a1ace6ad40f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd7c61-c4e9-4a4b-b23b-558d013e1718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43735d-e52d-46e3-b3fb-518bfc6b4d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706a81-16f3-4aea-927d-bdf021bf1feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75bb7f-1aec-47b1-a289-ff0c05768a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cc007-b387-4750-8343-4ff71240db67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efdbdf-cc67-44cc-a0a7-af755b2e66e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b96bd-49d6-4f54-ab74-265c6ef5d46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451552b6-3967-4419-b306-593f236feeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e356e7-3bbe-4ba6-870f-c907b7924ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd5b2e-2e9f-4fc4-b169-3cda728a5f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db72db-40d0-4667-883c-cdee264cb6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb136ab1-d003-4c41-af6f-369cc7670699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f9f7a-2fde-4567-b589-24bd93df30d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803d985-429b-4a82-a650-51266b2e27fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lft4j7pu59",
   "metadata": {},
   "source": [
    "---\n",
    "# Corrected Iterative Cycle: Gradient-Based with Loss as Context\n",
    "\n",
    "The previous loss-only approach failed (precision decreased from 67.6% to 52.9%).\n",
    "\n",
    "**Correct approach:**\n",
    "- Use **adaptive context selection** (gradient-only OR gradient+context per class)\n",
    "- Include **LOSS** in the enriched context (alongside features)\n",
    "- Classes with good gradient separation → use gradient-only\n",
    "- Classes with poor gradient separation → use gradient + features + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "okadf6fgly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions with loss context defined.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data_with_loss(analysis, DO, loss_data):\n",
    "    \"\"\"\n",
    "    Compute anchors that include loss in the enriched representation.\n",
    "    \n",
    "    For each class:\n",
    "    - Compute gradient-only anchors (same as before)\n",
    "    - Compute enriched anchors: gradient + features + normalized_loss\n",
    "    - Decide which to use based on gradient anchor similarity\n",
    "    \"\"\"\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Storage\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    feature_norm_params = {}\n",
    "    loss_norm_params = {}\n",
    "    \n",
    "    # Get gradient scale for normalization\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect data for correct hypotheses\n",
    "        correct_grads, correct_features, correct_losses = [], [], []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                correct_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                correct_losses.append(loss_data[gid])\n",
    "        \n",
    "        # Collect data for incorrect hypotheses\n",
    "        incorrect_grads, incorrect_features, incorrect_losses = [], [], []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None and gid in loss_data:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                incorrect_features.append(DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64))\n",
    "                incorrect_losses.append(loss_data[gid])\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "        \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Convert to arrays\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        correct_losses = np.array(correct_losses, dtype=np.float64)\n",
    "        incorrect_losses = np.array(incorrect_losses, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Normalize losses\n",
    "        all_losses = np.concatenate([correct_losses, incorrect_losses])\n",
    "        loss_mean = np.mean(all_losses)\n",
    "        loss_std = np.std(all_losses) + 1e-8\n",
    "        loss_norm_params[class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "        \n",
    "        # Negate loss: lower loss = higher value (more likely correct)\n",
    "        correct_losses_norm = -((correct_losses - loss_mean) / loss_std) * grad_scale\n",
    "        incorrect_losses_norm = -((incorrect_losses - loss_mean) / loss_std) * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm, correct_losses_norm.reshape(-1, 1)])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm, incorrect_losses_norm.reshape(-1, 1)])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'loss_norm_params': loss_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method with loss included in enriched context.\n",
    "    \n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features + loss) for classes with poor gradient separation\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Normalize features\n",
    "        feat_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if feat_params:\n",
    "            features_norm = (features - feat_params['mean']) / feat_params['std'] * feat_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        \n",
    "        # Normalize loss (negated: lower loss = higher value)\n",
    "        loss_params = anchor_data['loss_norm_params'].get(class_id)\n",
    "        if loss_params:\n",
    "            loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "        else:\n",
    "            loss_norm = -loss\n",
    "        \n",
    "        # Enriched = gradient + features + loss\n",
    "        enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "print(\"Functions with loss context defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drao865a84",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmark Hyperparameters\n",
    "\n",
    "Configure key parameters for the GGH comparison study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "yopyxinv9wk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK HYPERPARAMETERS\n",
      "======================================================================\n",
      "\n",
      "Run Configuration:\n",
      "  Number of runs: 15\n",
      "  Random states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "\n",
      "GGH Iterative Method:\n",
      "  Iter1 (Unbiased): 60 epochs, last 5 tracked\n",
      "  Iter1 Selection: top 30% by gradient score\n",
      "  Iter2 (Biased): 30 epochs, partial_weight=2.0\n",
      "  Iter3 Pruning: remove bottom 30% (keep top 70%)\n",
      "  Scoring: 5 passes, grad_loss_weight=0.5\n",
      "\n",
      "Final Model Training:\n",
      "  Epochs: 200\n",
      "  Learning rate: 0.01\n",
      "  Partial weight: 2.0\n",
      "\n",
      "Expected final selection:\n",
      "  Top 30% → keep 70% → final ~21.0% of samples\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "# Tune these parameters to test different configurations of the GGH method.\n",
    "# The benchmark comparison will use these values.\n",
    "\n",
    "# === Run Configuration ===\n",
    "BENCHMARK_N_RUNS = 15                    # Number of random states to test\n",
    "BENCHMARK_RAND_STATES = list(range(15))  # Random states: 0 to 14\n",
    "\n",
    "# === GGH Iterative Method Parameters ===\n",
    "# Iteration 1: Unbiased training\n",
    "GGH_ITER1_EPOCHS = 60                    # Total epochs for unbiased training\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5            # Last N epochs tracked for gradient analysis\n",
    "GGH_TOP_PERCENTILE = 30                  # % of samples selected by unbiased model\n",
    "\n",
    "# Iteration 2: Biased training  \n",
    "GGH_ITER2_EPOCHS = 30                    # Epochs for biased model training\n",
    "GGH_ITER2_PARTIAL_WEIGHT = 2.0           # Weight for partial data in biased training\n",
    "\n",
    "# Iteration 3: Pruning\n",
    "GGH_PRUNE_PERCENTILE = 30                # % of bottom samples to remove from top selection\n",
    "GGH_SCORING_PASSES = 5                   # Number of scoring passes for gradient estimation\n",
    "GGH_GRAD_LOSS_WEIGHT = 0.5               # Weight for combining grad score and loss score\n",
    "\n",
    "# === Final Model Training ===\n",
    "BENCHMARK_FINAL_EPOCHS = 200             # Epochs for final model training\n",
    "BENCHMARK_PARTIAL_WEIGHT = 2.0           # Partial weight for final GGH model training\n",
    "BENCHMARK_LR = 0.01                      # Learning rate\n",
    "\n",
    "# === Model Architecture (usually fixed) ===\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "# === Computed Values ===\n",
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nRun Configuration:\")\n",
    "print(f\"  Number of runs: {BENCHMARK_N_RUNS}\")\n",
    "print(f\"  Random states: {BENCHMARK_RAND_STATES}\")\n",
    "\n",
    "print(f\"\\nGGH Iterative Method:\")\n",
    "print(f\"  Iter1 (Unbiased): {GGH_ITER1_EPOCHS} epochs, last {GGH_ITER1_ANALYSIS_EPOCHS} tracked\")\n",
    "print(f\"  Iter1 Selection: top {GGH_TOP_PERCENTILE}% by gradient score\")\n",
    "print(f\"  Iter2 (Biased): {GGH_ITER2_EPOCHS} epochs, partial_weight={GGH_ITER2_PARTIAL_WEIGHT}\")\n",
    "print(f\"  Iter3 Pruning: remove bottom {GGH_PRUNE_PERCENTILE}% (keep top {100-GGH_PRUNE_PERCENTILE}%)\")\n",
    "print(f\"  Scoring: {GGH_SCORING_PASSES} passes, grad_loss_weight={GGH_GRAD_LOSS_WEIGHT}\")\n",
    "\n",
    "print(f\"\\nFinal Model Training:\")\n",
    "print(f\"  Epochs: {BENCHMARK_FINAL_EPOCHS}\")\n",
    "print(f\"  Learning rate: {BENCHMARK_LR}\")\n",
    "print(f\"  Partial weight: {BENCHMARK_PARTIAL_WEIGHT}\")\n",
    "\n",
    "print(f\"\\nExpected final selection:\")\n",
    "top_pct = GGH_TOP_PERCENTILE / 100\n",
    "kept_pct = 1 - (GGH_PRUNE_PERCENTILE / 100)\n",
    "final_pct = top_pct * kept_pct * 100\n",
    "print(f\"  Top {GGH_TOP_PERCENTILE}% → keep {(1-GGH_PRUNE_PERCENTILE/100)*100:.0f}% → final ~{final_pct:.1f}% of samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5vehgbcrq24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON STUDY: Optimized GGH vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method:\n",
      "  Iter1: 50 epochs unbiased, 1 tracked, Enriched selection (top 30%)\n",
      "  Iter2: 30 epochs biased (pw=2.0), Enriched+Loss pruning (remove 20%)\n",
      "  Final: Train on selected + partial (pw=2.0)\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, validation-based epoch selection, same architecture\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=0)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:01<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 59.5% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=48, val_loss=0.0185, test_loss=0.0176, test_mae=0.1016, R2=0.2688\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=9, val_loss=0.0194, test_loss=0.0169, test_mae=0.1102, R2=0.2971\n",
      "\n",
      ">>> Improvement: Loss=+0.0007, MAE=-0.0086, R2=-0.0283\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=1)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 70.6% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0178, test_loss=0.0181, test_mae=0.1100, R2=0.3525\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=7, val_loss=0.0197, test_loss=0.0205, test_mae=0.1198, R2=0.2692\n",
      "\n",
      ">>> Improvement: Loss=-0.0023, MAE=-0.0098, R2=+0.0833\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=2)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 47.2% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=20, val_loss=0.0237, test_loss=0.0166, test_mae=0.0957, R2=0.3285\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=64, val_loss=0.0225, test_loss=0.0172, test_mae=0.1006, R2=0.3048\n",
      "\n",
      ">>> Improvement: Loss=-0.0006, MAE=-0.0049, R2=+0.0237\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=3)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 57.6% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=1, val_loss=0.0156, test_loss=0.0233, test_mae=0.1190, R2=0.1853\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0175, test_loss=0.0242, test_mae=0.1225, R2=0.1565\n",
      "\n",
      ">>> Improvement: Loss=-0.0008, MAE=-0.0035, R2=+0.0288\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=4)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 65.8% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=26, val_loss=0.0216, test_loss=0.0195, test_mae=0.1044, R2=0.1787\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=7, val_loss=0.0200, test_loss=0.0171, test_mae=0.1020, R2=0.2810\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=+0.0025, R2=-0.1023\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=5)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 65.8% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=61, val_loss=0.0171, test_loss=0.0206, test_mae=0.1113, R2=0.1484\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=39, val_loss=0.0168, test_loss=0.0195, test_mae=0.1085, R2=0.1937\n",
      "\n",
      ">>> Improvement: Loss=+0.0011, MAE=+0.0028, R2=-0.0453\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=6)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 68.4% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0189, test_loss=0.0171, test_mae=0.1040, R2=0.2938\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=17, val_loss=0.0194, test_loss=0.0190, test_mae=0.1132, R2=0.2174\n",
      "\n",
      ">>> Improvement: Loss=-0.0019, MAE=-0.0092, R2=+0.0764\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=7)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 52.0% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=8, val_loss=0.0221, test_loss=0.0210, test_mae=0.1096, R2=0.2587\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=31, val_loss=0.0175, test_loss=0.0186, test_mae=0.1054, R2=0.3417\n",
      "\n",
      ">>> Improvement: Loss=+0.0023, MAE=+0.0041, R2=-0.0829\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=8)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 50.6% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=0, val_loss=0.0224, test_loss=0.0177, test_mae=0.1040, R2=0.2991\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0212, test_loss=0.0174, test_mae=0.1013, R2=0.3102\n",
      "\n",
      ">>> Improvement: Loss=+0.0003, MAE=+0.0028, R2=-0.0110\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=9)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 59.9% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=110, val_loss=0.0213, test_loss=0.0288, test_mae=0.1165, R2=0.0990\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0169, test_loss=0.0250, test_mae=0.1255, R2=0.2186\n",
      "\n",
      ">>> Improvement: Loss=+0.0038, MAE=-0.0090, R2=-0.1196\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=10)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 60.6% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=57, val_loss=0.0185, test_loss=0.0257, test_mae=0.1173, R2=-0.0311\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=23, val_loss=0.0222, test_loss=0.0213, test_mae=0.1122, R2=0.1436\n",
      "\n",
      ">>> Improvement: Loss=+0.0044, MAE=+0.0050, R2=-0.1747\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=11)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 39.4% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=71, val_loss=0.0205, test_loss=0.0214, test_mae=0.1225, R2=0.2932\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=7, val_loss=0.0224, test_loss=0.0219, test_mae=0.1219, R2=0.2790\n",
      "\n",
      ">>> Improvement: Loss=-0.0004, MAE=+0.0006, R2=+0.0143\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=12)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 53.2% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=27, val_loss=0.0271, test_loss=0.0259, test_mae=0.1142, R2=0.1460\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0231, test_loss=0.0220, test_mae=0.1074, R2=0.2749\n",
      "\n",
      ">>> Improvement: Loss=+0.0039, MAE=+0.0068, R2=-0.1289\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=13)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 59.1% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=0, val_loss=0.0312, test_loss=0.0360, test_mae=0.1543, R2=-0.5594\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0225, test_loss=0.0194, test_mae=0.1142, R2=0.1592\n",
      "\n",
      ">>> Improvement: Loss=+0.0166, MAE=+0.0401, R2=-0.7187\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=14)\n",
      "============================================================\n",
      "Running GGH selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 3/3 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1008 hypotheses from 336 samples\n",
      "  GGH precision: 56.1% (269 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=22, val_loss=0.0231, test_loss=0.0167, test_mae=0.0967, R2=0.2632\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=181, val_loss=0.0244, test_loss=0.0203, test_mae=0.1157, R2=0.1063\n",
      "\n",
      ">>> Improvement: Loss=-0.0036, MAE=-0.0189, R2=+0.1569\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: All Runs\n",
      "================================================================================\n",
      "\n",
      "Run   State  GGH Prec   GGH R2     Partial R2   Δ R2      \n",
      "---------------------------------------------------------------\n",
      "1     0        59.5%      0.2688      0.2971     -0.0283\n",
      "2     1        70.6%      0.3525      0.2692     +0.0833\n",
      "3     2        47.2%      0.3285      0.3048     +0.0237\n",
      "4     3        57.6%      0.1853      0.1565     +0.0288\n",
      "5     4        65.8%      0.1787      0.2810     -0.1023\n",
      "6     5        65.8%      0.1484      0.1937     -0.0453\n",
      "7     6        68.4%      0.2938      0.2174     +0.0764\n",
      "8     7        52.0%      0.2587      0.3417     -0.0829\n",
      "9     8        50.6%      0.2991      0.3102     -0.0110\n",
      "10    9        59.9%      0.0990      0.2186     -0.1196\n",
      "11    10       60.6%     -0.0311      0.1436     -0.1747\n",
      "12    11       39.4%      0.2932      0.2790     +0.0143\n",
      "13    12       53.2%      0.1460      0.2749     -0.1289\n",
      "14    13       59.1%     -0.5594      0.1592     -0.7187\n",
      "15    14       56.1%      0.2632      0.1063     +0.1569\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH mean:     0.1683 (+/- 0.2177)\n",
      "  Partial mean: 0.2369 (+/- 0.0691)\n",
      "  Difference:   -0.0686\n",
      "  Paired t-test: t=-1.320, p=0.2080 \n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH mean:     0.0217 (+/- 0.0053)\n",
      "  Partial mean: 0.0200 (+/- 0.0024)\n",
      "  Difference:   +0.0017\n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (R2):   6/15 (40.0%)\n",
      "  GGH wins (Loss): 6/15 (40.0%)\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "Partial outperforms GGH by 0.0686 R2 on average\n",
      "The difference is NOT statistically significant (p=0.2080)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON STUDY: Optimized GGH vs Partial-Only (15 Random States)\n",
    "# =============================================================================\n",
    "# GGH Method:\n",
    "#   1. Unbiased training (50 epochs, 1 analysis) + Enriched selection (top 30%)\n",
    "#   2. Biased training (30 epochs, partial_weight=2.0) + Enriched+Loss pruning\n",
    "#   3. Final model trained on pruned selection + partial\n",
    "# Partial: Only partial data (~2.5%)\n",
    "# Both use same final model architecture, validation-based epoch selection\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_RAND_STATES = [42 + i * 100 for i in range(15)]  # Match Wine_Hybrid_Iterative\n",
    "BENCHMARK_FINAL_EPOCHS = 200\n",
    "BENCHMARK_LR = 0.01\n",
    "BENCHMARK_PARTIAL_WEIGHT = 2.0\n",
    "\n",
    "# GGH Parameters (optimized from grid search)\n",
    "GGH_ITER1_EPOCHS = 50\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 1\n",
    "GGH_TOP_PERCENTILE = 30\n",
    "GGH_ITER2_EPOCHS = 30\n",
    "GGH_ITER2_PARTIAL_WEIGHT = 2.0\n",
    "GGH_PRUNE_PERCENTILE = 20\n",
    "GGH_SCORING_PASSES = 3\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def compute_enriched_score_benchmark(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def run_optimized_ggh(DO, rand_state):\n",
    "    \"\"\"\n",
    "    Run optimized GGH method:\n",
    "    1. Unbiased training + Enriched selection\n",
    "    2. Biased training + Enriched+Loss pruning\n",
    "    Returns selected_gids, precision, partial_correct_gids\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=300)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training + Enriched selection ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=0.001)\n",
    "    \n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # Compute anchors and select with enriched method\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    all_selections = []\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_is_correct, best_gid, best_class = -np.inf, False, None, None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_enriched_score_benchmark(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "                best_class = class_id\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid, best_class))\n",
    "    \n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * GGH_TOP_PERCENTILE / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # === ITERATION 2: Biased training + Enriched+Loss pruning ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                             MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                             MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, \n",
    "                                   partial_weight=GGH_ITER2_PARTIAL_WEIGHT, lr=0.001)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # Build enriched anchors from biased model\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    anchor_data_iter2 = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    all_grads = [partial_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in partial_analysis and partial_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_iter2['grad_scale'] = grad_scale\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            anchor_data_iter2['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_iter2['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_iter2['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_enriched = [np.concatenate([g, (f - feat_mean) / feat_std * grad_scale]) \n",
    "                               for g, f in zip(correct_grads, correct_features)]\n",
    "            incorrect_enriched = [np.concatenate([g, (f - feat_mean) / feat_std * grad_scale]) \n",
    "                                 for g, f in zip(incorrect_grads, incorrect_features)]\n",
    "            \n",
    "            anchor_data_iter2['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_iter2['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    # Score top selections with biased model using enriched+loss\n",
    "    top_scorer = RemainingDataScorer(DO, model_biased, top_sample_indices)\n",
    "    top_scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    top_analysis = top_scorer.get_analysis()\n",
    "    \n",
    "    sample_scores = []\n",
    "    for sample_idx in top_sample_indices:\n",
    "        gid = sample_to_gid.get(sample_idx)\n",
    "        if gid is None or gid not in top_analysis or top_analysis[gid]['avg_gradient'] is None:\n",
    "            continue\n",
    "        \n",
    "        gradient = top_analysis[gid]['avg_gradient']\n",
    "        loss = top_analysis[gid]['avg_loss']\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "        \n",
    "        # Compute enriched score\n",
    "        enriched_score = compute_enriched_score_benchmark(gradient, features, class_id, anchor_data_iter2)\n",
    "        \n",
    "        # Enriched + Loss scoring (0.5 * enriched + 0.5 * (-loss))\n",
    "        combined_score = 0.5 * enriched_score + 0.5 * (-loss)\n",
    "        \n",
    "        sample_scores.append({\n",
    "            'sample_idx': sample_idx,\n",
    "            'gid': gid,\n",
    "            'is_correct': is_correct,\n",
    "            'combined_score': combined_score\n",
    "        })\n",
    "    \n",
    "    # Prune bottom X%\n",
    "    sample_scores.sort(key=lambda x: x['combined_score'])\n",
    "    n_remove = int(len(sample_scores) * GGH_PRUNE_PERCENTILE / 100)\n",
    "    kept = sample_scores[n_remove:]\n",
    "    \n",
    "    n_correct = sum(1 for s in kept if s['is_correct'])\n",
    "    precision = n_correct / len(kept) * 100 if kept else 0\n",
    "    selected_gids = set(s['gid'] for s in kept)\n",
    "    \n",
    "    return selected_gids, precision, partial_correct_gids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON STUDY: Optimized GGH vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method:\")\n",
    "print(f\"  Iter1: {GGH_ITER1_EPOCHS} epochs unbiased, {GGH_ITER1_ANALYSIS_EPOCHS} tracked, Enriched selection (top {GGH_TOP_PERCENTILE}%)\")\n",
    "print(f\"  Iter2: {GGH_ITER2_EPOCHS} epochs biased (pw={GGH_ITER2_PARTIAL_WEIGHT}), Enriched+Loss pruning (remove {GGH_PRUNE_PERCENTILE}%)\")\n",
    "print(f\"  Final: Train on selected + partial (pw={BENCHMARK_PARTIAL_WEIGHT})\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {BENCHMARK_FINAL_EPOCHS} epochs, validation-based epoch selection, same architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, run_rand_state in enumerate(BENCHMARK_RAND_STATES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup DataOperator\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === Run GGH selection ===\n",
    "    print(\"Running GGH selection...\")\n",
    "    ggh_selected_gids, ggh_precision, _ = run_optimized_ggh(DO_run, run_rand_state)\n",
    "    print(f\"  GGH precision: {ggh_precision:.1f}% ({len(ggh_selected_gids)} samples)\")\n",
    "    \n",
    "    # === Train GGH final model ===\n",
    "    print(f\"Training GGH model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=BENCHMARK_PARTIAL_WEIGHT, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    print(f\"Training Partial model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(), partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_loss': ggh_test_loss - partial_test_loss,\n",
    "        'improvement_mae': ggh_test_mae - partial_test_mae,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, \"\n",
    "          f\"MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: All Runs\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n{'Run':<5} {'State':<6} {'GGH Prec':<10} {'GGH R2':<10} {'Partial R2':<12} {'Δ R2':<10}\")\n",
    "print(\"-\" * 63)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['rand_state']:<6} {r['ggh_precision']:>6.1f}%    \"\n",
    "          f\"{r['ggh_test_r2']:>8.4f}  {r['partial_test_r2']:>10.4f}    {r['improvement_r2']:>+8.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH mean:     {np.mean(ggh_r2s):.4f} (+/- {np.std(ggh_r2s):.4f})\")\n",
    "print(f\"  Partial mean: {np.mean(partial_r2s):.4f} (+/- {np.std(partial_r2s):.4f})\")\n",
    "print(f\"  Difference:   {np.mean(ggh_r2s) - np.mean(partial_r2s):+.4f}\")\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "print(f\"  Paired t-test: t={t_stat:.3f}, p={p_value:.4f} {'*' if p_value < 0.05 else ''}\")\n",
    "\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH mean:     {np.mean(ggh_losses):.4f} (+/- {np.std(ggh_losses):.4f})\")\n",
    "print(f\"  Partial mean: {np.mean(partial_losses):.4f} (+/- {np.std(partial_losses):.4f})\")\n",
    "print(f\"  Difference:   {np.mean(ggh_losses) - np.mean(partial_losses):+.4f}\")\n",
    "\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{BENCHMARK_N_RUNS} ({n_ggh_wins_r2/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{BENCHMARK_N_RUNS} ({n_ggh_wins_loss/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "if np.mean(ggh_r2s) > np.mean(partial_r2s):\n",
    "    print(f\"GGH outperforms Partial by {np.mean(ggh_r2s) - np.mean(partial_r2s):.4f} R2 on average\")\n",
    "else:\n",
    "    print(f\"Partial outperforms GGH by {np.mean(partial_r2s) - np.mean(ggh_r2s):.4f} R2 on average\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"The difference is statistically significant (p={p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"The difference is NOT statistically significant (p={p_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9fbbc2dd-1003-4a2d-8bba-61817330a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH (No Pruning) vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method (Simplified):\n",
      "  Unbiased: 50 epochs, 1 tracked, Enriched selection (top 30%)\n",
      "  NO pruning step - use all top 30% directly\n",
      "  Final: Train on selected + partial (pw=2.0)\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, lr=0.01, validation-based epoch selection\n",
      "Random states: [42, 142, 242]... (matching Wine_Hybrid_Iterative)\n",
      "================================================================================\n",
      "\n",
      "Run 1/15 (rand_state=42)...\n",
      "  GGH selection: 36.6% precision (336 samples)\n",
      "  GGH R2: 0.3221, Partial R2: 0.3394, Δ: -0.0173\n",
      "\n",
      "Run 2/15 (rand_state=142)...\n",
      "  GGH selection: 54.5% precision (336 samples)\n",
      "  GGH R2: 0.1813, Partial R2: 0.2048, Δ: -0.0235\n",
      "\n",
      "Run 3/15 (rand_state=242)...\n",
      "  GGH selection: 53.6% precision (336 samples)\n",
      "  GGH R2: 0.2928, Partial R2: 0.2850, Δ: +0.0078\n",
      "\n",
      "Run 4/15 (rand_state=342)...\n",
      "  GGH selection: 47.6% precision (336 samples)\n",
      "  GGH R2: 0.3144, Partial R2: 0.1972, Δ: +0.1173\n",
      "\n",
      "Run 5/15 (rand_state=442)...\n",
      "  GGH selection: 46.1% precision (336 samples)\n",
      "  GGH R2: 0.2308, Partial R2: 0.1838, Δ: +0.0470\n",
      "\n",
      "Run 6/15 (rand_state=542)...\n",
      "  GGH selection: 64.0% precision (336 samples)\n",
      "  GGH R2: 0.1590, Partial R2: 0.1901, Δ: -0.0311\n",
      "\n",
      "Run 7/15 (rand_state=642)...\n",
      "  GGH selection: 40.8% precision (336 samples)\n",
      "  GGH R2: 0.1793, Partial R2: 0.1099, Δ: +0.0694\n",
      "\n",
      "Run 8/15 (rand_state=742)...\n",
      "  GGH selection: 46.7% precision (336 samples)\n",
      "  GGH R2: 0.2198, Partial R2: 0.2889, Δ: -0.0691\n",
      "\n",
      "Run 9/15 (rand_state=842)...\n",
      "  GGH selection: 53.0% precision (336 samples)\n",
      "  GGH R2: 0.2518, Partial R2: 0.2050, Δ: +0.0468\n",
      "\n",
      "Run 10/15 (rand_state=942)...\n",
      "  GGH selection: 59.5% precision (336 samples)\n",
      "  GGH R2: 0.1552, Partial R2: 0.1681, Δ: -0.0129\n",
      "\n",
      "Run 11/15 (rand_state=1042)...\n",
      "  GGH selection: 43.2% precision (336 samples)\n",
      "  GGH R2: 0.1589, Partial R2: 0.0496, Δ: +0.1094\n",
      "\n",
      "Run 12/15 (rand_state=1142)...\n",
      "  GGH selection: 53.3% precision (336 samples)\n",
      "  GGH R2: 0.1665, Partial R2: 0.0234, Δ: +0.1431\n",
      "\n",
      "Run 13/15 (rand_state=1242)...\n",
      "  GGH selection: 49.7% precision (336 samples)\n",
      "  GGH R2: 0.1595, Partial R2: 0.2037, Δ: -0.0442\n",
      "\n",
      "Run 14/15 (rand_state=1342)...\n",
      "  GGH selection: 42.3% precision (336 samples)\n",
      "  GGH R2: 0.2538, Partial R2: -0.0483, Δ: +0.3020\n",
      "\n",
      "Run 15/15 (rand_state=1442)...\n",
      "  GGH selection: 64.0% precision (336 samples)\n",
      "  GGH R2: 0.3252, Partial R2: 0.3227, Δ: +0.0026\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: GGH (No Pruning) vs Partial-Only\n",
      "================================================================================\n",
      "\n",
      "Run   State    GGH Prec   GGH R2     Partial R2   Δ R2      \n",
      "-----------------------------------------------------------------\n",
      "1     42         36.6%      0.3221      0.3394     -0.0173\n",
      "2     142        54.5%      0.1813      0.2048     -0.0235\n",
      "3     242        53.6%      0.2928      0.2850     +0.0078\n",
      "4     342        47.6%      0.3144      0.1972     +0.1173\n",
      "5     442        46.1%      0.2308      0.1838     +0.0470\n",
      "6     542        64.0%      0.1590      0.1901     -0.0311\n",
      "7     642        40.8%      0.1793      0.1099     +0.0694\n",
      "8     742        46.7%      0.2198      0.2889     -0.0691\n",
      "9     842        53.0%      0.2518      0.2050     +0.0468\n",
      "10    942        59.5%      0.1552      0.1681     -0.0129\n",
      "11    1042       43.2%      0.1589      0.0496     +0.1094\n",
      "12    1142       53.3%      0.1665      0.0234     +0.1431\n",
      "13    1242       49.7%      0.1595      0.2037     -0.0442\n",
      "14    1342       42.3%      0.2538     -0.0483     +0.3020\n",
      "15    1442       64.0%      0.3252      0.3227     +0.0026\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH mean:     0.2247 (+/- 0.0627)\n",
      "  Partial mean: 0.1815 (+/- 0.1063)\n",
      "  Difference:   +0.0431\n",
      "  Paired t-test: t=1.751, p=0.1018 \n",
      "\n",
      "Win Rate: GGH wins 9/15 (60.0%)\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "GGH (No Pruning) outperforms Partial by 0.0431 R2 on average\n",
      "The difference is NOT statistically significant (p=0.1018)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: GGH (No Pruning) vs Partial-Only\n",
    "# =============================================================================\n",
    "# GGH Method (Simplified - No Pruning):\n",
    "#   1. Unbiased training (50 epochs, 1 analysis) + Enriched selection (top 30%)\n",
    "#   2. Final model trained on top 30% + partial (pw=2.0)\n",
    "# Partial: Only partial data (~2.5%)\n",
    "# Both use same final model architecture, validation-based epoch selection\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "N_RUNS = 15\n",
    "RAND_STATES = [42 + i * 100 for i in range(15)]  # Match Wine_Hybrid_Iterative\n",
    "FINAL_EPOCHS = 200\n",
    "LR = 0.01\n",
    "FINAL_PARTIAL_WEIGHT = 2.0\n",
    "\n",
    "# GGH Parameters (no pruning)\n",
    "ITER1_EPOCHS = 50\n",
    "ITER1_ANALYSIS_EPOCHS = 1\n",
    "TOP_PERCENTILE = 30\n",
    "\n",
    "\n",
    "def run_ggh_no_pruning(DO, rand_state):\n",
    "    \"\"\"\n",
    "    Run GGH selection WITHOUT pruning.\n",
    "    Returns selected_gids (top 30%) and precision.\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=300)\n",
    "    \n",
    "    # === Unbiased training + Enriched selection ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=0.001)\n",
    "    \n",
    "    for epoch in range(ITER1_EPOCHS - ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    for epoch in range(ITER1_EPOCHS - ITER1_ANALYSIS_EPOCHS, ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    all_selections = []\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_is_correct, best_gid = -np.inf, False, None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            score = compute_enriched_score_benchmark(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, best_gid))\n",
    "    \n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * TOP_PERCENTILE / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    \n",
    "    n_correct = sum(1 for s in top_selections if s[1])\n",
    "    precision = n_correct / len(top_selections) * 100 if top_selections else 0\n",
    "    selected_gids = set(s[2] for s in top_selections)\n",
    "    \n",
    "    return selected_gids, precision\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN BENCHMARK\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: GGH (No Pruning) vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method (Simplified):\")\n",
    "print(f\"  Unbiased: {ITER1_EPOCHS} epochs, {ITER1_ANALYSIS_EPOCHS} tracked, Enriched selection (top {TOP_PERCENTILE}%)\")\n",
    "print(f\"  NO pruning step - use all top {TOP_PERCENTILE}% directly\")\n",
    "print(f\"  Final: Train on selected + partial (pw={FINAL_PARTIAL_WEIGHT})\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {FINAL_EPOCHS} epochs, lr={LR}, validation-based epoch selection\")\n",
    "print(f\"Random states: {RAND_STATES[:3]}... (matching Wine_Hybrid_Iterative)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, rand_state in enumerate(RAND_STATES):\n",
    "    print(f\"\\nRun {run_idx+1}/{N_RUNS} (rand_state={rand_state})...\")\n",
    "    \n",
    "    set_to_deterministic(rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === GGH Selection (no pruning) ===\n",
    "    ggh_selected_gids, ggh_precision = run_ggh_no_pruning(DO_run, rand_state)\n",
    "    print(f\"  GGH selection: {ggh_precision:.1f}% precision ({len(ggh_selected_gids)} samples)\")\n",
    "    \n",
    "    # === Train GGH final model ===\n",
    "    set_to_deterministic(rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer,\n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=FINAL_PARTIAL_WEIGHT, lr=LR, n_epochs=FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    set_to_deterministic(rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(), partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=LR, n_epochs=FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    \n",
    "    results.append({\n",
    "        'rand_state': rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_n_selected': len(ggh_selected_gids),\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "    })\n",
    "    \n",
    "    print(f\"  GGH R2: {ggh_test_r2:.4f}, Partial R2: {partial_test_r2:.4f}, Δ: {ggh_test_r2 - partial_test_r2:+.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: GGH (No Pruning) vs Partial-Only\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n{'Run':<5} {'State':<8} {'GGH Prec':<10} {'GGH R2':<10} {'Partial R2':<12} {'Δ R2':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['rand_state']:<8} {r['ggh_precision']:>6.1f}%    \"\n",
    "          f\"{r['ggh_test_r2']:>8.4f}  {r['partial_test_r2']:>10.4f}    {r['ggh_test_r2'] - r['partial_test_r2']:>+8.4f}\")\n",
    "\n",
    "# Statistics\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH mean:     {np.mean(ggh_r2s):.4f} (+/- {np.std(ggh_r2s):.4f})\")\n",
    "print(f\"  Partial mean: {np.mean(partial_r2s):.4f} (+/- {np.std(partial_r2s):.4f})\")\n",
    "print(f\"  Difference:   {np.mean(ggh_r2s) - np.mean(partial_r2s):+.4f}\")\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "print(f\"  Paired t-test: t={t_stat:.3f}, p={p_value:.4f} {'*' if p_value < 0.05 else ''}\")\n",
    "\n",
    "n_ggh_wins = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate: GGH wins {n_ggh_wins}/{N_RUNS} ({n_ggh_wins/N_RUNS*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "if np.mean(ggh_r2s) > np.mean(partial_r2s):\n",
    "    print(f\"GGH (No Pruning) outperforms Partial by {np.mean(ggh_r2s) - np.mean(partial_r2s):.4f} R2 on average\")\n",
    "else:\n",
    "    print(f\"Partial outperforms GGH (No Pruning) by {np.mean(partial_r2s) - np.mean(ggh_r2s):.4f} R2 on average\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"The difference is statistically significant (p={p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"The difference is NOT statistically significant (p={p_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ggh_expansion_enriched",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method (Expansion Strategy):\n",
      "  Iter1: 60 epochs unbiased (lr=0.01), last 5 tracked\n",
      "  Iter1: Enriched selection (gradient + features) -> top 30%\n",
      "  Iter2: 30 epochs biased (lr=0.01, pw=2.0) on top 30% + partial\n",
      "  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\n",
      "  Final: Train on expansion selection + partial (pw=2.0)\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, validation-based epoch selection, same architecture\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 33.9%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 58.0%\n",
      "  Final GGH precision: 58.0% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=6, val_loss=0.0178, test_loss=0.0174, test_mae=0.0968, R2=0.3351\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0176, test_loss=0.0172, test_mae=0.0982, R2=0.3394\n",
      "\n",
      ">>> Improvement: Loss=-0.0001, MAE=+0.0015, R2=-0.0043\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 56.2%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 50.3%\n",
      "  Final GGH precision: 50.3% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=17, val_loss=0.0184, test_loss=0.0197, test_mae=0.1123, R2=0.1953\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0179, test_loss=0.0194, test_mae=0.1102, R2=0.2048\n",
      "\n",
      ">>> Improvement: Loss=-0.0002, MAE=-0.0021, R2=-0.0095\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 50.9%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 43.8%\n",
      "  Final GGH precision: 43.8% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=175, val_loss=0.0199, test_loss=0.0184, test_mae=0.1057, R2=0.2321\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=88, val_loss=0.0196, test_loss=0.0171, test_mae=0.1036, R2=0.2850\n",
      "\n",
      ">>> Improvement: Loss=-0.0013, MAE=-0.0021, R2=-0.0529\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 47.0%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 48.5%\n",
      "  Final GGH precision: 48.5% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=194, val_loss=0.0187, test_loss=0.0201, test_mae=0.1127, R2=0.1397\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0210, test_loss=0.0187, test_mae=0.1101, R2=0.1972\n",
      "\n",
      ">>> Improvement: Loss=-0.0013, MAE=-0.0026, R2=-0.0575\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 42.0%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 58.0%\n",
      "  Final GGH precision: 58.0% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=6, val_loss=0.0217, test_loss=0.0175, test_mae=0.1005, R2=0.2968\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0244, test_loss=0.0203, test_mae=0.1061, R2=0.1838\n",
      "\n",
      ">>> Improvement: Loss=+0.0028, MAE=+0.0056, R2=+0.1130\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 60.7%\n",
      "  Final GGH precision: 60.7% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=11, val_loss=0.0170, test_loss=0.0177, test_mae=0.0988, R2=0.2592\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=52, val_loss=0.0174, test_loss=0.0194, test_mae=0.1114, R2=0.1901\n",
      "\n",
      ">>> Improvement: Loss=+0.0017, MAE=+0.0126, R2=+0.0691\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 40.8%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 50.3%\n",
      "  Final GGH precision: 50.3% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=15, val_loss=0.0206, test_loss=0.0182, test_mae=0.1035, R2=0.2292\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0233, test_loss=0.0210, test_mae=0.1154, R2=0.1099\n",
      "\n",
      ">>> Improvement: Loss=+0.0028, MAE=+0.0119, R2=+0.1193\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=742)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 51.2%\n",
      "  Final GGH precision: 51.2% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=7, val_loss=0.0194, test_loss=0.0193, test_mae=0.1110, R2=0.2592\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0192, test_loss=0.0185, test_mae=0.1102, R2=0.2889\n",
      "\n",
      ">>> Improvement: Loss=-0.0008, MAE=-0.0008, R2=-0.0297\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=842)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 46.7%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 66.1%\n",
      "  Final GGH precision: 66.1% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0178, test_loss=0.0258, test_mae=0.1217, R2=0.1598\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=14, val_loss=0.0159, test_loss=0.0244, test_mae=0.1217, R2=0.2050\n",
      "\n",
      ">>> Improvement: Loss=-0.0014, MAE=+0.0000, R2=-0.0452\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=942)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 59.5%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 36.9%\n",
      "  Final GGH precision: 36.9% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=49, val_loss=0.0177, test_loss=0.0247, test_mae=0.1217, R2=0.1424\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=6, val_loss=0.0169, test_loss=0.0240, test_mae=0.1214, R2=0.1681\n",
      "\n",
      ">>> Improvement: Loss=-0.0007, MAE=-0.0002, R2=-0.0257\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=1042)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.2%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 41.4%\n",
      "  Final GGH precision: 41.4% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=0, val_loss=0.0214, test_loss=0.0206, test_mae=0.1133, R2=0.1445\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0224, test_loss=0.0229, test_mae=0.1148, R2=0.0496\n",
      "\n",
      ">>> Improvement: Loss=+0.0023, MAE=+0.0015, R2=+0.0949\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=1142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 46.7%\n",
      "  Final GGH precision: 46.7% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=12, val_loss=0.0193, test_loss=0.0194, test_mae=0.1160, R2=0.1684\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=86, val_loss=0.0199, test_loss=0.0228, test_mae=0.1204, R2=0.0234\n",
      "\n",
      ">>> Improvement: Loss=+0.0034, MAE=+0.0044, R2=+0.1450\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=1242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 52.4%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 52.4%\n",
      "  Final GGH precision: 52.4% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=27, val_loss=0.0213, test_loss=0.0175, test_mae=0.1081, R2=0.2894\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=32, val_loss=0.0223, test_loss=0.0196, test_mae=0.1125, R2=0.2037\n",
      "\n",
      ">>> Improvement: Loss=+0.0021, MAE=+0.0044, R2=+0.0857\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=1342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 44.3%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 53.6%\n",
      "  Final GGH precision: 53.6% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=5, val_loss=0.0187, test_loss=0.0151, test_mae=0.0993, R2=0.3377\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=192, val_loss=0.0241, test_loss=0.0240, test_mae=0.1330, R2=-0.0483\n",
      "\n",
      ">>> Improvement: Loss=+0.0088, MAE=+0.0337, R2=+0.3860\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=1442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%\n",
      "  Iter3 (expansion) remaining scored: 787, top 336 precision: 45.2%\n",
      "  Final GGH precision: 45.2% (336 samples)\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=13, val_loss=0.0180, test_loss=0.0171, test_mae=0.1054, R2=0.3393\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=8, val_loss=0.0175, test_loss=0.0176, test_mae=0.1060, R2=0.3227\n",
      "\n",
      ">>> Improvement: Loss=+0.0004, MAE=+0.0006, R2=+0.0166\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   GGH Prec   GGH Loss     Part Loss    Δ Loss     GGH R2     Part R2    Δ R2      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     58.0      % 0.0174       0.0172          -0.0001 0.3351     0.3394        -0.0043\n",
      "2     50.3      % 0.0197       0.0194          -0.0002 0.1953     0.2048        -0.0095\n",
      "3     43.8      % 0.0184       0.0171          -0.0013 0.2321     0.2850        -0.0529\n",
      "4     48.5      % 0.0201       0.0187          -0.0013 0.1397     0.1972        -0.0575\n",
      "5     58.0      % 0.0175       0.0203          +0.0028 0.2968     0.1838        +0.1130\n",
      "6     60.7      % 0.0177       0.0194          +0.0017 0.2592     0.1901        +0.0691\n",
      "7     50.3      % 0.0182       0.0210          +0.0028 0.2292     0.1099        +0.1193\n",
      "8     51.2      % 0.0193       0.0185          -0.0008 0.2592     0.2889        -0.0297\n",
      "9     66.1      % 0.0258       0.0244          -0.0014 0.1598     0.2050        -0.0452\n",
      "10    36.9      % 0.0247       0.0240          -0.0007 0.1424     0.1681        -0.0257\n",
      "11    41.4      % 0.0206       0.0229          +0.0023 0.1445     0.0496        +0.0949\n",
      "12    46.7      % 0.0194       0.0228          +0.0034 0.1684     0.0234        +0.1450\n",
      "13    52.4      % 0.0175       0.0196          +0.0021 0.2894     0.2037        +0.0857\n",
      "14    53.6      % 0.0151       0.0240          +0.0088 0.3377     -0.0483       +0.3860\n",
      "15    45.2      % 0.0171       0.0176          +0.0004 0.3393     0.3227        +0.0166\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "GGH Expansion Precision: 50.9% ± 7.4%\n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH:     0.0192 ± 0.0027\n",
      "  Partial: 0.0205 ± 0.0025\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH:     0.2352 ± 0.0716\n",
      "  Partial: 0.1815 ± 0.1063\n",
      "\n",
      "Statistical Tests (paired t-test):\n",
      "  Loss: t=-1.770, p=0.0985 \n",
      "  R2:   t=1.807, p=0.0922 \n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (Loss): 8/15 (53.3%)\n",
      "  GGH wins (R2):   8/15 (53.3%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "No significant difference in Loss (p=0.0985)\n",
      "No significant difference in R2 (p=0.0922)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: GGH Expansion Strategy (Like Wine_Hybrid_Iterative) + Enriched Scoring\n",
    "# =============================================================================\n",
    "# GGH Method:\n",
    "#   1. Unbiased training (60 epochs, last 5 tracked) + Enriched selection (top 30%)\n",
    "#   2. Biased training (30 epochs, lr=0.01) on top 30% + partial\n",
    "#   3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select top from remaining\n",
    "#   4. Final model trained on expansion selection + partial\n",
    "# Partial: Only partial data (~2.5%)\n",
    "# Both use same final model architecture, validation-based epoch selection\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION (Matching Wine_Hybrid_Iterative) ===\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_RAND_STATES = [42 + i * 100 for i in range(15)]\n",
    "BENCHMARK_FINAL_EPOCHS = 200\n",
    "BENCHMARK_LR = 0.01\n",
    "BENCHMARK_PARTIAL_WEIGHT = 2.0  # Final model partial weight\n",
    "\n",
    "# GGH Parameters (Matching Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_EPOCHS = 60              # 60 epochs (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5      # Last 5 tracked (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_TOP_PERCENTILE = 30            # Top 30%\n",
    "GGH_ITER2_EPOCHS = 30              # 30 epochs biased\n",
    "GGH_ITER2_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER2_PARTIAL_WEIGHT = 2.0     # partial_weight=2.0\n",
    "GGH_SCORING_PASSES = 5             # 5 passes (like Wine_Hybrid_Iterative)\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute enriched score with loss included (gradient + features + loss).\n",
    "    For expansion scoring on remaining samples.\n",
    "    \"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    loss_params = anchor_data.get('loss_norm_params', {}).get(class_id)\n",
    "    grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "    \n",
    "    # Normalize features\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    # Normalize loss (negated: lower loss = higher value)\n",
    "    if loss_params:\n",
    "        loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "    else:\n",
    "        loss_norm = -loss * grad_scale\n",
    "    \n",
    "    # Enriched = gradient + features + loss\n",
    "    enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "    \n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def run_ggh_expansion(DO, rand_state):\n",
    "    \"\"\"\n",
    "    Run GGH method with EXPANSION strategy (like Wine_Hybrid_Iterative) but with Enriched scoring.\n",
    "    \n",
    "    1. Unbiased training (60 epochs, track last 5) + Enriched selection -> top 30%\n",
    "    2. Biased training on top 30% + partial (30 epochs)\n",
    "    3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select best from remaining\n",
    "    \n",
    "    Returns selected_gids, precision, partial_correct_gids\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training (60 epochs, track last 5) + Enriched selection ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n",
    "    \n",
    "    # Train without tracking for first (60-5)=55 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # Track last 5 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # Compute anchors for ENRICHED selection\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    # Select using ENRICHED method (gradient + features)\n",
    "    all_selections = []\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_is_correct, best_gid, best_class = -np.inf, False, None, None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED scoring (gradient + features)\n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "                best_class = class_id\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid, best_class))\n",
    "    \n",
    "    # Sort and get top 30%\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * GGH_TOP_PERCENTILE / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # Iter1 precision\n",
    "    iter1_correct = sum(1 for s in top_selections if s[1])\n",
    "    iter1_precision = iter1_correct / len(top_selections) * 100 if top_selections else 0\n",
    "    \n",
    "    # === ITERATION 2: Biased training on top 30% + partial ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                             MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                             MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, \n",
    "                                   partial_weight=GGH_ITER2_PARTIAL_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # === ITERATION 3: EXPANSION - Score REMAINING 70% with Enriched+Loss ===\n",
    "    all_sample_indices = set(range(n_samples))\n",
    "    remaining_sample_indices = all_sample_indices - top_sample_indices - partial_sample_indices\n",
    "    \n",
    "    # First, score partial data to build anchors with loss\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Build anchors with loss from partial data\n",
    "    anchor_data_biased = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},  # Will include loss\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "        'loss_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    # Compute normalization parameters\n",
    "    all_grads = [partial_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in partial_analysis and partial_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_biased['grad_scale'] = grad_scale\n",
    "    \n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        correct_losses, incorrect_losses = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    correct_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    incorrect_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            # Gradient anchors\n",
    "            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            # Feature normalization\n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n",
    "            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n",
    "            \n",
    "            # Loss normalization\n",
    "            all_losses = correct_losses + incorrect_losses\n",
    "            loss_mean = np.mean(all_losses)\n",
    "            loss_std = np.std(all_losses) + 1e-8\n",
    "            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n",
    "            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n",
    "            \n",
    "            # Enriched anchors = gradient + features + loss\n",
    "            correct_enriched = [np.concatenate([g, f, [l]]) \n",
    "                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n",
    "            incorrect_enriched = [np.concatenate([g, f, [l]]) \n",
    "                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n",
    "            \n",
    "            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    # Score REMAINING 70%\n",
    "    scorer = RemainingDataScorer(DO, model_biased, remaining_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    remaining_analysis = scorer.get_analysis()\n",
    "    \n",
    "    # Score each hypothesis in remaining samples using Enriched+Loss\n",
    "    remaining_scored = []\n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        start_gid = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_gid = None\n",
    "        best_is_correct = False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start_gid + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in remaining_analysis or remaining_analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = remaining_analysis[gid]['avg_gradient']\n",
    "            loss = remaining_analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED + LOSS scoring\n",
    "            score = compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            remaining_scored.append({\n",
    "                'sample_idx': sample_idx,\n",
    "                'gid': best_gid,\n",
    "                'score': best_score,\n",
    "                'is_correct': best_is_correct\n",
    "            })\n",
    "    \n",
    "    # Sort remaining by score and take top selections (same count as iter1 top)\n",
    "    remaining_scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "    n_take_from_remaining = min(len(remaining_scored), n_top)\n",
    "    top_remaining = remaining_scored[:n_take_from_remaining]\n",
    "    \n",
    "    # Calculate precision on top of remaining (EXPANSION result)\n",
    "    iter3_correct = sum(1 for s in top_remaining if s['is_correct'])\n",
    "    iter3_precision = iter3_correct / len(top_remaining) * 100 if top_remaining else 0\n",
    "    \n",
    "    # Final selection: use iter3 selections (from remaining 70%)\n",
    "    selected_gids = set(s['gid'] for s in top_remaining)\n",
    "    \n",
    "    print(f\"  Iter1 top {GGH_TOP_PERCENTILE}% precision: {iter1_precision:.1f}%\")\n",
    "    print(f\"  Iter3 (expansion) remaining scored: {len(remaining_scored)}, top {n_take_from_remaining} precision: {iter3_precision:.1f}%\")\n",
    "    \n",
    "    return selected_gids, iter3_precision, partial_correct_gids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method (Expansion Strategy):\")\n",
    "print(f\"  Iter1: {GGH_ITER1_EPOCHS} epochs unbiased (lr={GGH_ITER1_LR}), last {GGH_ITER1_ANALYSIS_EPOCHS} tracked\")\n",
    "print(f\"  Iter1: Enriched selection (gradient + features) -> top {GGH_TOP_PERCENTILE}%\")\n",
    "print(f\"  Iter2: {GGH_ITER2_EPOCHS} epochs biased (lr={GGH_ITER2_LR}, pw={GGH_ITER2_PARTIAL_WEIGHT}) on top 30% + partial\")\n",
    "print(f\"  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\")\n",
    "print(f\"  Final: Train on expansion selection + partial (pw={BENCHMARK_PARTIAL_WEIGHT})\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {BENCHMARK_FINAL_EPOCHS} epochs, validation-based epoch selection, same architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, run_rand_state in enumerate(BENCHMARK_RAND_STATES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup DataOperator\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === Run GGH Expansion selection ===\n",
    "    print(\"Running GGH Expansion selection...\")\n",
    "    ggh_selected_gids, ggh_precision, _ = run_ggh_expansion(DO_run, run_rand_state)\n",
    "    print(f\"  Final GGH precision: {ggh_precision:.1f}% ({len(ggh_selected_gids)} samples)\")\n",
    "    \n",
    "    # === Train GGH final model ===\n",
    "    print(f\"Training GGH model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=BENCHMARK_PARTIAL_WEIGHT, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    print(f\"Training Partial model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(), partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,  # Positive = GGH better\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, \"\n",
    "          f\"MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'GGH Prec':<10} {'GGH Loss':<12} {'Part Loss':<12} {'Δ Loss':<10} {'GGH R2':<10} {'Part R2':<10} {'Δ R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<12.4f} \"\n",
    "          f\"{r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGGH Expansion Precision: {np.mean(ggh_precisions):.1f}% ± {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f} ± {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f} ± {np.std(partial_losses):.4f}\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} ± {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f} ± {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{BENCHMARK_N_RUNS} ({n_ggh_wins_loss/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{BENCHMARK_N_RUNS} ({n_ggh_wins_r2/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH Expansion vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 2: R2 comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, ggh_r2s, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, partial_r2s, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Test R2')\n",
    "ax2.set_title('Test R2: GGH Expansion vs Partial')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 3: GGH Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, BENCHMARK_N_RUNS+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('GGH Expansion Precision (%)')\n",
    "ax3.set_title('GGH Hypothesis Precision (from Remaining 70%)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_expansion_enriched_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement_loss = np.mean([r['improvement_loss'] for r in results])\n",
    "avg_improvement_r2 = np.mean([r['improvement_r2'] for r in results])\n",
    "if avg_improvement_loss > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on Loss (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average loss improvement: {avg_improvement_loss:.4f} MSE\")\n",
    "elif avg_improvement_loss < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on Loss (p={p_value_loss:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in Loss (p={p_value_loss:.4f})\")\n",
    "\n",
    "if avg_improvement_r2 > 0 and p_value_r2 < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on R2 (p={p_value_r2:.4f})\")\n",
    "    print(f\"Average R2 improvement: {avg_improvement_r2:.4f}\")\n",
    "elif avg_improvement_r2 < 0 and p_value_r2 < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on R2 (p={p_value_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in R2 (p={p_value_r2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55119061-e18b-4606-a63e-23e798c35b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
