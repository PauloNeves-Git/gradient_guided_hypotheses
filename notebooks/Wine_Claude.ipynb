{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine_Claude: Optimizing Gradient Guided Hypotheses\n",
    "\n",
    "This notebook explores configurations to achieve gradient selection performance\n",
    "that significantly exceeds baseline methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms, clean_final_analysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Claude\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "hypothesis = [[9.35, 10, 11.5, 15]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "\n",
    "# Initialize inspector\n",
    "INSPECT = Inspector(results_path, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_experiment(use_info, DO, INSPECT, batch_size, hidden_size, output_size, num_epochs, rand_state, results_path,\n",
    "                    dropout=0.05, lr=0.004, nu=0.1, normalize_grads_contx=False, use_context=True, \n",
    "                    final_analysis=False, freqperc_cutoff=0.25):\n",
    "    \n",
    "    AM = AlgoModulators(DO, lr=lr, nu=nu, normalize_grads_contx=normalize_grads_contx, \n",
    "                        use_context=use_context, freqperc_cutoff=freqperc_cutoff)\n",
    "    dataloader = DO.prep_dataloader(use_info, batch_size)\n",
    "    model = initialize_model(DO, dataloader, hidden_size, rand_state, dropout=dropout)\n",
    "    \n",
    "    TVM = TrainValidationManager(use_info, num_epochs, dataloader, batch_size, rand_state, \n",
    "                                  results_path, final_analysis=final_analysis)\n",
    "    TVM.train_model(DO, AM, model, final_analysis=final_analysis)\n",
    "    INSPECT.save_train_val_logs(DO, AM, TVM, model, final_analysis=final_analysis)\n",
    "    \n",
    "    return DO, TVM, model\n",
    "\n",
    "def multi_experiments(total_runs, use_info, num_epochs, data_path, inpt_vars, target_vars, miss_vars, \n",
    "                      hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size, \n",
    "                      results_path, hyperparameters, final_analysis=True):\n",
    "    \n",
    "    clean_final_analysis(results_path, use_info)\n",
    "    progress_bar = tqdm(total=total_runs)\n",
    "    \n",
    "    for r_state in range(2000):\n",
    "        set_to_deterministic(r_state)\n",
    "        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n",
    "                          partial_perc, r_state, device=\"cpu\")\n",
    "        DO.problem_type = 'regression'\n",
    "        \n",
    "        if not DO.lack_partial_coverage:\n",
    "            full_experiment(use_info, DO, INSPECT, batch_size, hidden_size, output_size, num_epochs, \n",
    "                           r_state, results_path,\n",
    "                           dropout=hyperparameters[\"dropout\"][\"value\"],\n",
    "                           lr=hyperparameters[\"lr\"][\"value\"],\n",
    "                           nu=hyperparameters[\"nu\"][\"value\"],\n",
    "                           normalize_grads_contx=hyperparameters[\"normalize_grads_contx\"][\"value\"],\n",
    "                           use_context=hyperparameters[\"use_context\"][\"value\"],\n",
    "                           final_analysis=final_analysis,\n",
    "                           freqperc_cutoff=hyperparameters.get(\"freqperc_cutoff\", {\"value\": 0.25})[\"value\"])\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        if progress_bar.n == total_runs:\n",
    "            break\n",
    "    \n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_header",
   "metadata": {},
   "source": [
    "## Step 1: Run Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_baselines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num_loops = 15\n",
    "partial_perc = 0.015  # Testing with 1.5% partial data\n",
    "\n",
    "# Baseline hyperparameters\n",
    "baseline_hyperparams = {\n",
    "    \"lr\": {\"value\": 0.001},\n",
    "    \"dropout\": {\"value\": 0.05},\n",
    "    \"nu\": {\"value\": 0.1},\n",
    "    \"normalize_grads_contx\": {\"value\": False},\n",
    "    \"use_context\": {\"value\": True},\n",
    "    \"freqperc_cutoff\": {\"value\": 0.25}\n",
    "}\n",
    "\n",
    "batch_size = 100 * len(hypothesis[0])\n",
    "num_epochs_baseline = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_partial_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run partial info baseline\n",
    "multi_experiments(num_loops, \"partial info\", num_epochs_baseline, data_path, inpt_vars, target_vars, \n",
    "                  miss_vars, hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size, \n",
    "                  results_path, baseline_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_known_only",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run use known only baseline\n",
    "multi_experiments(num_loops, \"use known only\", num_epochs_baseline, data_path, inpt_vars, target_vars,\n",
    "                  miss_vars, hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size,\n",
    "                  results_path, baseline_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_full_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run full info (oracle upper bound)\n",
    "multi_experiments(num_loops, \"full info\", num_epochs_baseline, data_path, inpt_vars, target_vars,\n",
    "                  miss_vars, hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size,\n",
    "                  results_path, baseline_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check baseline results\n",
    "df_baseline, df_baseline_notavg = INSPECT.create_test_comparison_table(\n",
    "    data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "    partial_perc, batch_size, best_imput=\"\"\n",
    ")\n",
    "print(\"Baseline Results:\")\n",
    "print(df_baseline[[\"Method\", \"avg_r2_score\", \"std_r2_score\", \"avg_mse\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypothesis_header",
   "metadata": {},
   "source": [
    "## Step 2: Optimize Gradient Selection (Use Hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized_hyperparams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized hyperparameters for gradient selection\n",
    "# Key changes: lower nu (more permissive), normalize gradients, adjusted learning rate\n",
    "optimized_hyperparams = {\n",
    "    \"lr\": {\"value\": 0.002},\n",
    "    \"dropout\": {\"value\": 0.05},\n",
    "    \"nu\": {\"value\": 0.15},  # More permissive to capture more correct hypotheses\n",
    "    \"normalize_grads_contx\": {\"value\": True},  # Normalize for better separation\n",
    "    \"use_context\": {\"value\": True},\n",
    "    \"freqperc_cutoff\": {\"value\": 0.20}  # Lower cutoff, be more inclusive\n",
    "}\n",
    "\n",
    "num_epochs_hypothesis = 60  # Fewer epochs for hypothesis selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run gradient selection with optimized parameters\n",
    "multi_experiments(num_loops, \"use hypothesis\", num_epochs_hypothesis, data_path, inpt_vars, target_vars,\n",
    "                  miss_vars, hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size,\n",
    "                  results_path, optimized_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results with hypothesis\n",
    "df_results, df_results_notavg = INSPECT.create_test_comparison_table(\n",
    "    data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "    partial_perc, batch_size, best_imput=\"\"\n",
    ")\n",
    "print(\"Results with Gradient Selection:\")\n",
    "print(df_results[[\"Method\", \"avg_r2_score\", \"std_r2_score\", \"avg_mse\"]])\n",
    "\n",
    "# Calculate improvement\n",
    "if \"use hypothesis\" in df_results[\"Method\"].values:\n",
    "    hyp_r2 = df_results[df_results[\"Method\"] == \"use hypothesis\"][\"avg_r2_score\"].values[0]\n",
    "    partial_r2 = df_results[df_results[\"Method\"] == \"partial info\"][\"avg_r2_score\"].values[0]\n",
    "    known_r2 = df_results[df_results[\"Method\"] == \"use known only\"][\"avg_r2_score\"].values[0]\n",
    "    best_baseline = max(partial_r2, known_r2)\n",
    "    improvement = (hyp_r2 - best_baseline) * 100\n",
    "    print(f\"\\nImprovement over best baseline: {improvement:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_header",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter search for best configuration\n",
    "def quick_test(nu, lr, freqperc_cutoff, normalize, num_epochs, partial_perc, n_runs=5):\n",
    "    \"\"\"Quick test with fewer runs to find promising configurations.\"\"\"\n",
    "    hyperparams = {\n",
    "        \"lr\": {\"value\": lr},\n",
    "        \"dropout\": {\"value\": 0.05},\n",
    "        \"nu\": {\"value\": nu},\n",
    "        \"normalize_grads_contx\": {\"value\": normalize},\n",
    "        \"use_context\": {\"value\": True},\n",
    "        \"freqperc_cutoff\": {\"value\": freqperc_cutoff}\n",
    "    }\n",
    "    \n",
    "    test_results = []\n",
    "    for r_state in range(100):\n",
    "        set_to_deterministic(r_state)\n",
    "        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "                          partial_perc, r_state, device=\"cpu\")\n",
    "        DO.problem_type = 'regression'\n",
    "        \n",
    "        if not DO.lack_partial_coverage:\n",
    "            DO, TVM, model = full_experiment(\"use hypothesis\", DO, INSPECT, batch_size, hidden_size, \n",
    "                                              output_size, num_epochs, r_state, results_path,\n",
    "                                              dropout=0.05, lr=lr, nu=nu,\n",
    "                                              normalize_grads_contx=normalize,\n",
    "                                              use_context=True, final_analysis=False,\n",
    "                                              freqperc_cutoff=freqperc_cutoff)\n",
    "            \n",
    "            best_model = load_model(DO, TVM.weights_save_path, batch_size)\n",
    "            test_r2 = INSPECT.calculate_val_r2score(DO, TVM, best_model, data=\"test\")\n",
    "            test_results.append(test_r2)\n",
    "            \n",
    "            if len(test_results) >= n_runs:\n",
    "                break\n",
    "    \n",
    "    return np.mean(test_results), np.std(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over key parameters\n",
    "print(\"Running parameter search...\")\n",
    "results = []\n",
    "\n",
    "for nu in [0.05, 0.10, 0.20, 0.3]:\n",
    "    for lr in [0.001, 0.0025]:\n",
    "        for freqperc in [0.15, 0.25, 0.75]:\n",
    "            for normalize in [True, False]:\n",
    "                for epochs in [50]:\n",
    "                    mean_r2, std_r2 = quick_test(nu, lr, freqperc, normalize, epochs, partial_perc=0.015, n_runs=3)\n",
    "                    results.append({\n",
    "                        'nu': nu, 'lr': lr, 'freqperc': freqperc, \n",
    "                        'normalize': normalize, 'epochs': epochs,\n",
    "                        'mean_r2': mean_r2, 'std_r2': std_r2\n",
    "                    })\n",
    "                    print(f\"nu={nu}, lr={lr}, freq={freqperc}, norm={normalize}, ep={epochs}: R2={mean_r2:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nTop 10 configurations:\")\n",
    "print(results_df.nlargest(10, 'mean_r2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best configuration found\n",
    "best_config = results_df.nlargest(1, 'mean_r2').iloc[0]\n",
    "print(f\"Best configuration:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_header",
   "metadata": {},
   "source": [
    "## Step 4: Final Benchmark (15 loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final hyperparameters based on search\n",
    "final_hyperparams = {\n",
    "    \"lr\": {\"value\": best_config['lr']},\n",
    "    \"dropout\": {\"value\": 0.05},\n",
    "    \"nu\": {\"value\": best_config['nu']},\n",
    "    \"normalize_grads_contx\": {\"value\": best_config['normalize']},\n",
    "    \"use_context\": {\"value\": True},\n",
    "    \"freqperc_cutoff\": {\"value\": best_config['freqperc']}\n",
    "}\n",
    "\n",
    "final_epochs = int(best_config['epochs'])\n",
    "print(f\"Running final benchmark with: {final_hyperparams}\")\n",
    "print(f\"Epochs: {final_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_hypothesis_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run final hypothesis experiment\n",
    "multi_experiments(num_loops, \"use hypothesis\", final_epochs, data_path, inpt_vars, target_vars,\n",
    "                  miss_vars, hypothesis, partial_perc, INSPECT, batch_size, hidden_size, output_size,\n",
    "                  results_path, final_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "df_final, df_final_notavg = INSPECT.create_test_comparison_table(\n",
    "    data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "    partial_perc, batch_size, best_imput=\"\"\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(df_final[[\"Method\", \"avg_r2_score\", \"std_r2_score\", \"avg_mse\", \"avg_mae\"]])\n",
    "\n",
    "# Calculate and display improvement\n",
    "hyp_r2 = df_final[df_final[\"Method\"] == \"use hypothesis\"][\"avg_r2_score\"].values[0]\n",
    "partial_r2 = df_final[df_final[\"Method\"] == \"partial info\"][\"avg_r2_score\"].values[0]\n",
    "known_r2 = df_final[df_final[\"Method\"] == \"use known only\"][\"avg_r2_score\"].values[0]\n",
    "full_r2 = df_final[df_final[\"Method\"] == \"full info\"][\"avg_r2_score\"].values[0]\n",
    "\n",
    "best_baseline = max(partial_r2, known_r2)\n",
    "improvement = (hyp_r2 - best_baseline) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"IMPROVEMENT ANALYSIS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Use Hypothesis R2: {hyp_r2:.4f}\")\n",
    "print(f\"Best Baseline R2:  {best_baseline:.4f} ({'partial info' if partial_r2 > known_r2 else 'use known only'})\")\n",
    "print(f\"Full Info R2:      {full_r2:.4f} (oracle upper bound)\")\n",
    "print(f\"\")\n",
    "print(f\"Improvement over best baseline: {improvement:.2f} percentage points\")\n",
    "print(f\"Gap to oracle closed: {((hyp_r2 - best_baseline) / (full_r2 - best_baseline)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-run results\n",
    "print(\"\\nPer-run R2 scores:\")\n",
    "print(df_final_notavg[[\"Method\", \"rand_states\", \"avg_r2_score\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
