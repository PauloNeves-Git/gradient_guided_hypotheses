{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine_Claude: Targeted Parameter Search for GGH\n",
    "\n",
    "This notebook systematically tests different parameter configurations to find\n",
    "settings where gradient-guided hypothesis selection significantly outperforms baselines.\n",
    "\n",
    "**Goal**: Find configuration where `use hypothesis` R2 score is at least 4.5 percentage points\n",
    "higher than the best baseline (`partial info` or `use known only`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms, clean_final_analysis\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Claude\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Claude\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "hypothesis = [[9.35, 10, 11.5, 15]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "batch_size = 100 * len(hypothesis[0])  # 400\n",
    "output_size = len(target_vars)\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['use hypothesis', 'partial info', 'use known only', 'full info']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "# Initialize inspector\n",
    "INSPECT = Inspector(results_path, hidden_size)\n",
    "print(f\"Results will be saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "test_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test function defined.\n"
     ]
    }
   ],
   "source": [
    "def test_config(use_info, partial_perc, num_epochs, lr, nu, normalize, freqperc, \n",
    "                dropout=0.05, n_runs=5, verbose=False, use_enhanced_context=False):\n",
    "    \"\"\"\n",
    "    Test a configuration and return R2 scores across multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        use_enhanced_context: If True, adds hypothesis class ID (one-hot) to enriched vectors\n",
    "                             to better distinguish between hypotheses.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    valid_runs = 0\n",
    "    \n",
    "    for r_state in range(500):  # Try up to 500 random states to get n_runs valid ones\n",
    "        set_to_deterministic(r_state)\n",
    "        \n",
    "        DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "                          partial_perc, r_state, device='cpu')\n",
    "        DO.problem_type = 'regression'\n",
    "        \n",
    "        if not DO.lack_partial_coverage:\n",
    "            AM = AlgoModulators(DO, lr=lr, nu=nu, normalize_grads_contx=normalize,\n",
    "                               use_context=True, freqperc_cutoff=freqperc,\n",
    "                               use_enhanced_context=use_enhanced_context)\n",
    "            dataloader = DO.prep_dataloader(use_info, batch_size)\n",
    "            model = initialize_model(DO, dataloader, hidden_size, r_state, dropout=dropout)\n",
    "            \n",
    "            TVM = TrainValidationManager(use_info, num_epochs, dataloader, batch_size,\n",
    "                                         r_state, results_path, final_analysis=False)\n",
    "            TVM.train_model(DO, AM, model, final_analysis=False)\n",
    "            \n",
    "            # Load best model and evaluate\n",
    "            model.load_state_dict(torch.load(TVM.weights_save_path))\n",
    "            model.eval()\n",
    "            \n",
    "            if use_info in ['use hypothesis', 'partial info', 'full info']:\n",
    "                test_pred = model(DO.full_test_input_tensor)\n",
    "            else:\n",
    "                test_pred = model(DO.known_test_input_tensor)\n",
    "            \n",
    "            test_true = DO.df_test[target_vars].values\n",
    "            r2 = r2_score(test_true, test_pred.detach().numpy())\n",
    "            results.append(r2)\n",
    "            valid_runs += 1\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Run {valid_runs} (seed={r_state}): R2={r2:.4f}\")\n",
    "            \n",
    "            if valid_runs >= n_runs:\n",
    "                break\n",
    "    \n",
    "    return np.mean(results), np.std(results), results\n",
    "\n",
    "print(\"Test function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_header",
   "metadata": {},
   "source": [
    "## Step 1: Establish Baselines\n",
    "\n",
    "Test both `partial_perc=0.015` and `partial_perc=0.025` to find which gives better opportunity for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baselines_015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINES with partial_perc = 0.015\n",
      "============================================================\n",
      "\n",
      "Testing partial info...\n",
      "  partial info: 0.1790 +/- 0.0835\n",
      "  Individual: ['0.0220', '0.2613', '0.1854', '0.1905', '0.2360']\n",
      "\n",
      "Testing use known only...\n",
      "  use known only: 0.1758 +/- 0.0479\n",
      "  Individual: ['0.2019', '0.2316', '0.1561', '0.0929', '0.1963']\n",
      "\n",
      "Best baseline: 0.1790\n",
      "Target (baseline + 4.5pp): 0.2240\n"
     ]
    }
   ],
   "source": [
    "# Baselines with partial_perc = 0.015 (1.5% complete data)\n",
    "partial_perc = 0.015\n",
    "n_baseline_runs = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BASELINES with partial_perc = {partial_perc}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Testing partial info...\")\n",
    "p_mean_015, p_std_015, p_results_015 = test_config(\n",
    "    'partial info', partial_perc, 200, 0.001, 0.1, False, 0.25, n_runs=n_baseline_runs\n",
    ")\n",
    "print(f\"  partial info: {p_mean_015:.4f} +/- {p_std_015:.4f}\")\n",
    "print(f\"  Individual: {[f'{r:.4f}' for r in p_results_015]}\")\n",
    "\n",
    "print(\"\\nTesting use known only...\")\n",
    "k_mean_015, k_std_015, k_results_015 = test_config(\n",
    "    'use known only', partial_perc, 200, 0.001, 0.1, False, 0.25, n_runs=n_baseline_runs\n",
    ")\n",
    "print(f\"  use known only: {k_mean_015:.4f} +/- {k_std_015:.4f}\")\n",
    "print(f\"  Individual: {[f'{r:.4f}' for r in k_results_015]}\")\n",
    "\n",
    "best_baseline_015 = max(p_mean_015, k_mean_015)\n",
    "target_015 = best_baseline_015 + 0.045\n",
    "print(f\"\\nBest baseline: {best_baseline_015:.4f}\")\n",
    "print(f\"Target (baseline + 4.5pp): {target_015:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baselines_025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINES with partial_perc = 0.025\n",
      "============================================================\n",
      "\n",
      "Testing partial info...\n",
      "  partial info: 0.1902 +/- 0.0485\n",
      "  Individual: ['0.1530', '0.2789', '0.2042', '0.1662', '0.1487']\n",
      "\n",
      "Testing use known only...\n",
      "  use known only: 0.1809 +/- 0.0511\n",
      "  Individual: ['0.2019', '0.2316', '0.1561', '0.2222', '0.0929']\n",
      "\n",
      "Best baseline: 0.1902\n",
      "Target (baseline + 4.5pp): 0.2352\n"
     ]
    }
   ],
   "source": [
    "# Baselines with partial_perc = 0.025 (2.5% complete data)\n",
    "partial_perc = 0.025\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BASELINES with partial_perc = {partial_perc}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Testing partial info...\")\n",
    "p_mean_025, p_std_025, p_results_025 = test_config(\n",
    "    'partial info', partial_perc, 200, 0.001, 0.1, False, 0.25, n_runs=n_baseline_runs\n",
    ")\n",
    "print(f\"  partial info: {p_mean_025:.4f} +/- {p_std_025:.4f}\")\n",
    "print(f\"  Individual: {[f'{r:.4f}' for r in p_results_025]}\")\n",
    "\n",
    "print(\"\\nTesting use known only...\")\n",
    "k_mean_025, k_std_025, k_results_025 = test_config(\n",
    "    'use known only', partial_perc, 200, 0.001, 0.1, False, 0.25, n_runs=n_baseline_runs\n",
    ")\n",
    "print(f\"  use known only: {k_mean_025:.4f} +/- {k_std_025:.4f}\")\n",
    "print(f\"  Individual: {[f'{r:.4f}' for r in k_results_025]}\")\n",
    "\n",
    "best_baseline_025 = max(p_mean_025, k_mean_025)\n",
    "target_025 = best_baseline_025 + 0.045\n",
    "print(f\"\\nBest baseline: {best_baseline_025:.4f}\")\n",
    "print(f\"Target (baseline + 4.5pp): {target_025:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baseline_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "partial_perc=0.015: best baseline=0.1790, target=0.2240\n",
      "partial_perc=0.025: best baseline=0.1902, target=0.2352\n"
     ]
    }
   ],
   "source": [
    "# Summary of baselines\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\npartial_perc=0.015: best baseline={best_baseline_015:.4f}, target={target_015:.4f}\")\n",
    "print(f\"partial_perc=0.025: best baseline={best_baseline_025:.4f}, target={target_025:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search_header",
   "metadata": {},
   "source": [
    "## Step 2: Targeted Parameter Search\n",
    "\n",
    "Test promising configurations for `use hypothesis` method.\n",
    "\n",
    "**Key parameters:**\n",
    "- `nu`: OneClassSVM parameter (lower = more permissive selection)\n",
    "- `lr`: Learning rate\n",
    "- `freqperc_cutoff`: Frequency threshold for final selection\n",
    "- `normalize_grads_contx`: Whether to normalize gradients + context\n",
    "- `num_epochs`: Training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d791fb-0db7-45fd-8a9c-7aa8e7d5d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Baseline-like configs (no enhanced context)\n",
    "    (0.10, 0.001, 0.25, False, 60, False, \"baseline\"),\n",
    "    \n",
    "    # Vary nu (selection restrictiveness)\n",
    "    (0.05, 0.001, 0.25, False, 60, False, \"low nu\"),\n",
    "    (0.08, 0.001, 0.25, False, 60, False, \"med-low nu\"),\n",
    "    (0.15, 0.001, 0.25, False, 60, False, \"med-high nu\"),\n",
    "    (0.20, 0.001, 0.25, False, 60, False, \"high nu\"),\n",
    "    \n",
    "    # Vary learning rate\n",
    "    (0.10, 0.002, 0.25, False, 60, False, \"higher lr\"),\n",
    "    (0.10, 0.004, 0.25, False, 60, False, \"high lr\"),\n",
    "    (0.10, 0.0005, 0.25, False, 60, False, \"low lr\"),\n",
    "    \n",
    "    # Vary frequency cutoff\n",
    "    (0.10, 0.001, 0.15, False, 60, False, \"low freq cutoff\"),\n",
    "    (0.10, 0.001, 0.20, False, 60, False, \"med-low freq cutoff\"),\n",
    "    (0.10, 0.001, 0.33, False, 60, False, \"high freq cutoff\"),\n",
    "    \n",
    "    # With normalization\n",
    "    (0.10, 0.001, 0.25, True, 60, False, \"with normalize\"),\n",
    "    (0.10, 0.002, 0.25, True, 60, False, \"normalize + higher lr\"),\n",
    "    \n",
    "    # Vary epochs\n",
    "    (0.10, 0.001, 0.25, False, 40, False, \"40 epochs\"),\n",
    "    (0.10, 0.001, 0.25, False, 80, False, \"more epochs\"),\n",
    "    \n",
    "    # Combined promising configs\n",
    "    (0.08, 0.002, 0.20, False, 60, False, \"combined 1\"),\n",
    "    (0.08, 0.002, 0.20, True, 60, False, \"combined 2 + norm\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parameter_configs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations to test: 2\n",
      "  - Standard configs: 0\n",
      "  - Enhanced context configs: 2\n"
     ]
    }
   ],
   "source": [
    "# Define configurations to test\n",
    "# Format: (nu, lr, freqperc_cutoff, normalize, epochs, use_enhanced_context, description)\n",
    "configs_to_test = [\n",
    "    \n",
    "    # ==========================================\n",
    "    # ENHANCED CONTEXT CONFIGURATIONS\n",
    "    # (adds hypothesis class ID to enriched vectors)\n",
    "    # This addresses the diagnostic finding that 75% of input features\n",
    "    # are identical across hypotheses, diluting the gradient signal\n",
    "    # ==========================================\n",
    "    (0.10, 0.001, 0.25, False, 50, True, \"enhanced context\"),\n",
    "    (0.10, 0.001, 0.25, True, 50, True, \"enhanced + normalize\"),\n",
    "    #(0.08, 0.001, 0.25, False, 60, True, \"enhanced + low nu\"),\n",
    "    #(0.15, 0.001, 0.25, False, 60, True, \"enhanced + high nu\"),\n",
    "    #(0.10, 0.002, 0.25, False, 60, True, \"enhanced + higher lr\"),\n",
    "    #(0.10, 0.001, 0.20, False, 60, True, \"enhanced + low freq\"),\n",
    "    #(0.08, 0.002, 0.20, True, 60, True, \"enhanced combined\"),\n",
    "    #(0.10, 0.001, 0.25, True, 80, True, \"enhanced + norm + epochs\"),\n",
    "]\n",
    "\n",
    "print(f\"Total configurations to test: {len(configs_to_test)}\")\n",
    "print(f\"  - Standard configs: {sum(1 for c in configs_to_test if not c[5])}\")\n",
    "print(f\"  - Enhanced context configs: {sum(1 for c in configs_to_test if c[5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search_015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARAMETER SEARCH with partial_perc = 0.015\n",
      "Best baseline: 0.1790, Target: 0.2240\n",
      "============================================================\n",
      "\n",
      "[1/2] Testing enhanced context... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:34<00:00, 15.08s/it]\n",
      "100%|██████████| 50/50 [12:35<00:00, 15.10s/it]\n",
      "100%|██████████| 50/50 [12:38<00:00, 15.17s/it]\n",
      "100%|██████████| 50/50 [12:40<00:00, 15.21s/it]\n",
      "100%|██████████| 50/50 [12:40<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2=0.1525 (-2.65pp) \n",
      "[2/2] Testing enhanced + normalize... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:37<00:00, 15.15s/it]\n",
      " 34%|███▍      | 17/50 [04:28<08:40, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradients were selected, training will cease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 19/50 [04:47<07:58, 15.42s/it]"
     ]
    }
   ],
   "source": [
    "# Search with partial_perc = 0.015\n",
    "partial_perc = 0.015\n",
    "best_baseline = best_baseline_015\n",
    "target = target_015\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PARAMETER SEARCH with partial_perc = {partial_perc}\")\n",
    "print(f\"Best baseline: {best_baseline:.4f}, Target: {target:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "results_015 = []\n",
    "best_r2_015 = 0\n",
    "best_config_015 = None\n",
    "\n",
    "for i, (nu, lr, freqperc, normalize, epochs, enhanced_ctx, desc) in enumerate(configs_to_test):\n",
    "    print(f\"[{i+1}/{len(configs_to_test)}] Testing {desc}...\", end=\" \")\n",
    "    \n",
    "    h_mean, h_std, h_results = test_config(\n",
    "        'use hypothesis', partial_perc, epochs, lr, nu, normalize, freqperc, \n",
    "        n_runs=5, use_enhanced_context=enhanced_ctx\n",
    "    )\n",
    "    \n",
    "    improvement = (h_mean - best_baseline) * 100\n",
    "    results_015.append({\n",
    "        'desc': desc, 'nu': nu, 'lr': lr, 'freqperc': freqperc,\n",
    "        'normalize': normalize, 'epochs': epochs, 'enhanced_ctx': enhanced_ctx,\n",
    "        'mean_r2': h_mean, 'std_r2': h_std, 'improvement_pp': improvement\n",
    "    })\n",
    "    \n",
    "    marker = \"***\" if improvement >= 4.5 else (\"**\" if improvement >= 2.0 else (\"*\" if improvement > 0 else \"\"))\n",
    "    print(f\"R2={h_mean:.4f} ({improvement:+.2f}pp) {marker}\")\n",
    "    \n",
    "    if h_mean > best_r2_015:\n",
    "        best_r2_015 = h_mean\n",
    "        best_config_015 = (nu, lr, freqperc, normalize, epochs, enhanced_ctx, desc)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST for partial_perc={partial_perc}:\")\n",
    "print(f\"  R2 = {best_r2_015:.4f} (improvement: {(best_r2_015-best_baseline)*100:.2f}pp)\")\n",
    "print(f\"  Config: {best_config_015}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search_025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with partial_perc = 0.025\n",
    "partial_perc = 0.025\n",
    "best_baseline = best_baseline_025\n",
    "target = target_025\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PARAMETER SEARCH with partial_perc = {partial_perc}\")\n",
    "print(f\"Best baseline: {best_baseline:.4f}, Target: {target:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "results_025 = []\n",
    "best_r2_025 = 0\n",
    "best_config_025 = None\n",
    "\n",
    "for i, (nu, lr, freqperc, normalize, epochs, enhanced_ctx, desc) in enumerate(configs_to_test):\n",
    "    print(f\"[{i+1}/{len(configs_to_test)}] Testing {desc}...\", end=\" \")\n",
    "    \n",
    "    h_mean, h_std, h_results = test_config(\n",
    "        'use hypothesis', partial_perc, epochs, lr, nu, normalize, freqperc, \n",
    "        n_runs=5, use_enhanced_context=enhanced_ctx\n",
    "    )\n",
    "    \n",
    "    improvement = (h_mean - best_baseline) * 100\n",
    "    results_025.append({\n",
    "        'desc': desc, 'nu': nu, 'lr': lr, 'freqperc': freqperc,\n",
    "        'normalize': normalize, 'epochs': epochs, 'enhanced_ctx': enhanced_ctx,\n",
    "        'mean_r2': h_mean, 'std_r2': h_std, 'improvement_pp': improvement\n",
    "    })\n",
    "    \n",
    "    marker = \"***\" if improvement >= 4.5 else (\"**\" if improvement >= 2.0 else (\"*\" if improvement > 0 else \"\"))\n",
    "    print(f\"R2={h_mean:.4f} ({improvement:+.2f}pp) {marker}\")\n",
    "    \n",
    "    if h_mean > best_r2_025:\n",
    "        best_r2_025 = h_mean\n",
    "        best_config_025 = (nu, lr, freqperc, normalize, epochs, enhanced_ctx, desc)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST for partial_perc={partial_perc}:\")\n",
    "print(f\"  R2 = {best_r2_025:.4f} (improvement: {(best_r2_025-best_baseline_025)*100:.2f}pp)\")\n",
    "print(f\"  Config: {best_config_025}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results as tables\n",
    "print(\"\\nResults for partial_perc=0.015:\")\n",
    "df_015 = pd.DataFrame(results_015).sort_values('mean_r2', ascending=False)\n",
    "print(df_015[['desc', 'nu', 'lr', 'freqperc', 'normalize', 'enhanced_ctx', 'epochs', 'mean_r2', 'improvement_pp']].head(12).to_string())\n",
    "\n",
    "print(\"\\n\\nResults for partial_perc=0.025:\")\n",
    "df_025 = pd.DataFrame(results_025).sort_values('mean_r2', ascending=False)\n",
    "print(df_025[['desc', 'nu', 'lr', 'freqperc', 'normalize', 'enhanced_ctx', 'epochs', 'mean_r2', 'improvement_pp']].head(12).to_string())\n",
    "\n",
    "# Compare enhanced vs non-enhanced\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED CONTEXT vs STANDARD COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for pp, df in [(\"0.015\", df_015), (\"0.025\", df_025)]:\n",
    "    enhanced = df[df['enhanced_ctx'] == True]['mean_r2'].mean()\n",
    "    standard = df[df['enhanced_ctx'] == False]['mean_r2'].mean()\n",
    "    print(f\"partial_perc={pp}: Enhanced avg R2={enhanced:.4f}, Standard avg R2={standard:.4f}, diff={(enhanced-standard)*100:+.2f}pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_header",
   "metadata": {},
   "source": [
    "## Step 3: Validate Best Configuration (15 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which partial_perc showed better improvement\n",
    "improvement_015 = (best_r2_015 - best_baseline_015) * 100\n",
    "improvement_025 = (best_r2_025 - best_baseline_025) * 100\n",
    "\n",
    "if improvement_015 > improvement_025:\n",
    "    final_partial_perc = 0.015\n",
    "    final_config = best_config_015\n",
    "    final_baseline = best_baseline_015\n",
    "else:\n",
    "    final_partial_perc = 0.025\n",
    "    final_config = best_config_025\n",
    "    final_baseline = best_baseline_025\n",
    "\n",
    "nu, lr, freqperc, normalize, epochs, enhanced_ctx, desc = final_config\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL VALIDATION (15 runs)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nSelected: partial_perc={final_partial_perc}\")\n",
    "print(f\"Config: nu={nu}, lr={lr}, freqperc={freqperc}, normalize={normalize}, epochs={epochs}, enhanced_context={enhanced_ctx}\")\n",
    "print(f\"Description: {desc}\")\n",
    "print(f\"\\nRunning 15 validation runs...\")\n",
    "\n",
    "h_mean_final, h_std_final, h_results_final = test_config(\n",
    "    'use hypothesis', final_partial_perc, epochs, lr, nu, normalize, freqperc, \n",
    "    n_runs=15, verbose=True, use_enhanced_context=enhanced_ctx\n",
    ")\n",
    "\n",
    "final_improvement = (h_mean_final - final_baseline) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Use Hypothesis R2: {h_mean_final:.4f} +/- {h_std_final:.4f}\")\n",
    "print(f\"Best Baseline R2:  {final_baseline:.4f}\")\n",
    "print(f\"Improvement:       {final_improvement:.2f} percentage points\")\n",
    "print(f\"Target achieved:   {'YES!' if final_improvement >= 4.5 else 'Not yet'}\")\n",
    "print(f\"Enhanced context:  {enhanced_ctx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
