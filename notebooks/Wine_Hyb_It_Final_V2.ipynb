{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine GGH Expansion Benchmark (Cleaned)\n",
    "\n",
    "This notebook contains only the necessary code to run the GGH expansion benchmark.\n",
    "\n",
    "**Results**: Partial R2 ≈ 0.18, GGH R2 ≈ 0.235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_header",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## GGH Expansion Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ggh_expansion_enriched",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method (Expansion Strategy):\n",
      "  Iter1: 60 epochs unbiased (lr=0.01), last 5 tracked\n",
      "  Iter1: Enriched selection (gradient + features) -> top 30%\n",
      "  Iter2: 30 epochs biased (lr=0.01, pw=2.0) on top 30% + partial\n",
      "  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\n",
      "  Final: Train on expansion selection + partial (pw=2.0)\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, validation-based epoch selection, same architecture\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 33.9%, class dist: {0: 51, 1: 105, 2: 180}\n",
      "  Iter3 top 20% (of remaining) precision: 63.1%, class dist: {0: 101, 1: 56, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 43.2%, class dist: {0: 152, 1: 161, 2: 180}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=14, val_loss=0.0181, test_loss=0.0181, test_mae=0.0936, R2=0.3081\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0176, test_loss=0.0172, test_mae=0.0982, R2=0.3394\n",
      "\n",
      ">>> Improvement: Loss=-0.0008, MAE=+0.0046, R2=-0.0313\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 56.2%, class dist: {0: 120, 1: 17, 2: 199}\n",
      "  Iter3 top 20% (of remaining) precision: 52.2%, class dist: {0: 91, 1: 66, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 55.0%, class dist: {0: 211, 1: 83, 2: 199}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=31, val_loss=0.0190, test_loss=0.0213, test_mae=0.1109, R2=0.1300\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0179, test_loss=0.0194, test_mae=0.1102, R2=0.2048\n",
      "\n",
      ">>> Improvement: Loss=-0.0018, MAE=-0.0007, R2=-0.0748\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 50.9%, class dist: {0: 68, 1: 0, 2: 268}\n",
      "  Iter3 top 20% (of remaining) precision: 36.9%, class dist: {0: 46, 1: 111, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 46.5%, class dist: {0: 114, 1: 111, 2: 268}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=13, val_loss=0.0199, test_loss=0.0174, test_mae=0.1022, R2=0.2729\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=88, val_loss=0.0196, test_loss=0.0171, test_mae=0.1036, R2=0.2850\n",
      "\n",
      ">>> Improvement: Loss=-0.0003, MAE=+0.0015, R2=-0.0121\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 47.0%, class dist: {0: 85, 1: 183, 2: 68}\n",
      "  Iter3 top 20% (of remaining) precision: 51.0%, class dist: {0: 43, 1: 114, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.3%, class dist: {0: 128, 1: 297, 2: 68}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=18, val_loss=0.0161, test_loss=0.0165, test_mae=0.0987, R2=0.2915\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0210, test_loss=0.0187, test_mae=0.1101, R2=0.1972\n",
      "\n",
      ">>> Improvement: Loss=+0.0022, MAE=+0.0114, R2=+0.0943\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 42.0%, class dist: {0: 71, 1: 161, 2: 104}\n",
      "  Iter3 top 20% (of remaining) precision: 63.1%, class dist: {0: 137, 1: 5, 2: 15}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.7%, class dist: {0: 208, 1: 166, 2: 119}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=9, val_loss=0.0219, test_loss=0.0176, test_mae=0.1011, R2=0.2940\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0244, test_loss=0.0203, test_mae=0.1061, R2=0.1838\n",
      "\n",
      ">>> Improvement: Loss=+0.0027, MAE=+0.0051, R2=+0.1102\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%, class dist: {0: 141, 1: 31, 2: 164}\n",
      "  Iter3 top 20% (of remaining) precision: 70.1%, class dist: {0: 130, 1: 27, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 64.3%, class dist: {0: 271, 1: 58, 2: 164}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=27, val_loss=0.0164, test_loss=0.0186, test_mae=0.1027, R2=0.2219\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=52, val_loss=0.0174, test_loss=0.0194, test_mae=0.1114, R2=0.1901\n",
      "\n",
      ">>> Improvement: Loss=+0.0008, MAE=+0.0086, R2=+0.0318\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 40.8%, class dist: {0: 82, 1: 147, 2: 107}\n",
      "  Iter3 top 20% (of remaining) precision: 59.9%, class dist: {0: 71, 1: 58, 2: 28}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 46.9%, class dist: {0: 153, 1: 205, 2: 135}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=12, val_loss=0.0208, test_loss=0.0188, test_mae=0.1086, R2=0.2032\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0233, test_loss=0.0210, test_mae=0.1154, R2=0.1099\n",
      "\n",
      ">>> Improvement: Loss=+0.0022, MAE=+0.0068, R2=+0.0933\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=742)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%, class dist: {0: 121, 1: 31, 2: 184}\n",
      "  Iter3 top 20% (of remaining) precision: 53.5%, class dist: {0: 80, 1: 77, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 54.4%, class dist: {0: 201, 1: 108, 2: 184}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=9, val_loss=0.0201, test_loss=0.0210, test_mae=0.1138, R2=0.1930\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0192, test_loss=0.0185, test_mae=0.1102, R2=0.2889\n",
      "\n",
      ">>> Improvement: Loss=-0.0025, MAE=-0.0036, R2=-0.0959\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=842)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 46.7%, class dist: {0: 66, 1: 156, 2: 114}\n",
      "  Iter3 top 20% (of remaining) precision: 65.0%, class dist: {0: 81, 1: 28, 2: 48}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 52.5%, class dist: {0: 147, 1: 184, 2: 162}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=4, val_loss=0.0170, test_loss=0.0244, test_mae=0.1156, R2=0.2054\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=14, val_loss=0.0159, test_loss=0.0244, test_mae=0.1217, R2=0.2050\n",
      "\n",
      ">>> Improvement: Loss=+0.0000, MAE=+0.0061, R2=+0.0004\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=942)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 59.5%, class dist: {0: 246, 1: 90, 2: 0}\n",
      "  Iter3 top 20% (of remaining) precision: 31.8%, class dist: {0: 19, 1: 132, 2: 6}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 50.7%, class dist: {0: 265, 1: 222, 2: 6}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=9, val_loss=0.0147, test_loss=0.0225, test_mae=0.1179, R2=0.2169\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=6, val_loss=0.0169, test_loss=0.0240, test_mae=0.1214, R2=0.1681\n",
      "\n",
      ">>> Improvement: Loss=+0.0014, MAE=+0.0035, R2=+0.0488\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=1042)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.2%, class dist: {0: 170, 1: 12, 2: 154}\n",
      "  Iter3 top 20% (of remaining) precision: 35.7%, class dist: {0: 0, 1: 109, 2: 48}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.3%, class dist: {0: 170, 1: 121, 2: 202}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=167, val_loss=0.0198, test_loss=0.0204, test_mae=0.1168, R2=0.1502\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0224, test_loss=0.0229, test_mae=0.1148, R2=0.0496\n",
      "\n",
      ">>> Improvement: Loss=+0.0024, MAE=-0.0020, R2=+0.1006\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=1142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%, class dist: {0: 167, 1: 88, 2: 81}\n",
      "  Iter3 top 20% (of remaining) precision: 53.5%, class dist: {0: 36, 1: 121, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 54.4%, class dist: {0: 203, 1: 209, 2: 81}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=25, val_loss=0.0183, test_loss=0.0200, test_mae=0.1127, R2=0.1426\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=86, val_loss=0.0199, test_loss=0.0228, test_mae=0.1204, R2=0.0234\n",
      "\n",
      ">>> Improvement: Loss=+0.0028, MAE=+0.0077, R2=+0.1192\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=1242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 52.4%, class dist: {0: 66, 1: 142, 2: 128}\n",
      "  Iter3 top 20% (of remaining) precision: 54.1%, class dist: {0: 46, 1: 111, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 52.9%, class dist: {0: 112, 1: 253, 2: 128}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=43, val_loss=0.0197, test_loss=0.0184, test_mae=0.1064, R2=0.2548\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=32, val_loss=0.0223, test_loss=0.0196, test_mae=0.1125, R2=0.2037\n",
      "\n",
      ">>> Improvement: Loss=+0.0013, MAE=+0.0060, R2=+0.0512\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=1342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 44.3%, class dist: {0: 174, 1: 121, 2: 41}\n",
      "  Iter3 top 20% (of remaining) precision: 59.9%, class dist: {0: 71, 1: 25, 2: 61}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 49.3%, class dist: {0: 245, 1: 146, 2: 102}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=40, val_loss=0.0178, test_loss=0.0154, test_mae=0.1036, R2=0.3251\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=192, val_loss=0.0241, test_loss=0.0240, test_mae=0.1330, R2=-0.0483\n",
      "\n",
      ">>> Improvement: Loss=+0.0085, MAE=+0.0295, R2=+0.3734\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=1442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%, class dist: {0: 211, 1: 125, 2: 0}\n",
      "  Iter3 top 20% (of remaining) precision: 42.7%, class dist: {0: 3, 1: 72, 2: 82}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 55.6%, class dist: {0: 214, 1: 197, 2: 82}\n",
      "  Dynamic partial_weight: 5.87\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=15, val_loss=0.0171, test_loss=0.0166, test_mae=0.1055, R2=0.3601\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=8, val_loss=0.0175, test_loss=0.0176, test_mae=0.1060, R2=0.3227\n",
      "\n",
      ">>> Improvement: Loss=+0.0010, MAE=+0.0004, R2=+0.0375\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   GGH Prec   GGH Loss     Part Loss    Δ Loss     GGH R2     Part R2    Δ R2      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     43.2      % 0.0181       0.0172          -0.0008 0.3081     0.3394        -0.0313\n",
      "2     55.0      % 0.0213       0.0194          -0.0018 0.1300     0.2048        -0.0748\n",
      "3     46.5      % 0.0174       0.0171          -0.0003 0.2729     0.2850        -0.0121\n",
      "4     48.3      % 0.0165       0.0187          +0.0022 0.2915     0.1972        +0.0943\n",
      "5     48.7      % 0.0176       0.0203          +0.0027 0.2940     0.1838        +0.1102\n",
      "6     64.3      % 0.0186       0.0194          +0.0008 0.2219     0.1901        +0.0318\n",
      "7     46.9      % 0.0188       0.0210          +0.0022 0.2032     0.1099        +0.0933\n",
      "8     54.4      % 0.0210       0.0185          -0.0025 0.1930     0.2889        -0.0959\n",
      "9     52.5      % 0.0244       0.0244          +0.0000 0.2054     0.2050        +0.0004\n",
      "10    50.7      % 0.0225       0.0240          +0.0014 0.2169     0.1681        +0.0488\n",
      "11    48.3      % 0.0204       0.0229          +0.0024 0.1502     0.0496        +0.1006\n",
      "12    54.4      % 0.0200       0.0228          +0.0028 0.1426     0.0234        +0.1192\n",
      "13    52.9      % 0.0184       0.0196          +0.0013 0.2548     0.2037        +0.0512\n",
      "14    49.3      % 0.0154       0.0240          +0.0085 0.3251     -0.0483       +0.3734\n",
      "15    55.6      % 0.0166       0.0176          +0.0010 0.3601     0.3227        +0.0375\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "GGH Expansion Precision: 51.4% ± 4.9%\n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH:     0.0191 ± 0.0024\n",
      "  Partial: 0.0205 ± 0.0025\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH:     0.2380 ± 0.0675\n",
      "  Partial: 0.1815 ± 0.1063\n",
      "\n",
      "Statistical Tests (paired t-test):\n",
      "  Loss: t=-1.993, p=0.0661 \n",
      "  R2:   t=1.989, p=0.0666 \n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (Loss): 11/15 (73.3%)\n",
      "  GGH wins (R2):   11/15 (73.3%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "No significant difference in Loss (p=0.0661)\n",
      "No significant difference in R2 (p=0.0666)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: GGH Expansion Strategy (Like Wine_Hybrid_Iterative) + Enriched Scoring\n",
    "# =============================================================================\n",
    "# GGH Method:\n",
    "#   1. Unbiased training (60 epochs, last 5 tracked) + Enriched selection (top 30%)\n",
    "#   2. Biased training (30 epochs, lr=0.01) on top 30% + partial\n",
    "#   3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select top from remaining\n",
    "#   4. Final model trained on expansion selection + partial\n",
    "# Partial: Only partial data (~2.5%)\n",
    "# Both use same final model architecture, validation-based epoch selection\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION (Matching Wine_Hybrid_Iterative) ===\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_RAND_STATES = [42 + i * 100 for i in range(15)]\n",
    "BENCHMARK_FINAL_EPOCHS = 200\n",
    "BENCHMARK_LR = 0.01\n",
    "BENCHMARK_PARTIAL_WEIGHT = 2.0  # Final model partial weight\n",
    "\n",
    "# GGH Parameters (Matching Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_EPOCHS = 60              # 60 epochs (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5      # Last 5 tracked (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_TOP_PERCENTILE = 30            # Top 30%\n",
    "GGH_ITER2_EPOCHS = 30              # 30 epochs biased\n",
    "GGH_ITER2_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER2_PARTIAL_WEIGHT = 2.0     # partial_weight=2.0\n",
    "GGH_SCORING_PASSES = 5             # 5 passes (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER3_TOP_PERCENTILE = 20      # Top 20% from remaining (unseen) samples\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute enriched score with loss included (gradient + features + loss).\n",
    "    For expansion scoring on remaining samples.\n",
    "    \"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    loss_params = anchor_data.get('loss_norm_params', {}).get(class_id)\n",
    "    grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "    \n",
    "    # Normalize features\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    # Normalize loss (negated: lower loss = higher value)\n",
    "    if loss_params:\n",
    "        loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "    else:\n",
    "        loss_norm = -loss * grad_scale\n",
    "    \n",
    "    # Enriched = gradient + features + loss\n",
    "    enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "    \n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def run_ggh_expansion(DO, rand_state):\n",
    "    \"\"\"\n",
    "    Run GGH method with EXPANSION strategy (like Wine_Hybrid_Iterative) but with Enriched scoring.\n",
    "    \n",
    "    1. Unbiased training (60 epochs, track last 5) + Enriched selection -> top 30%\n",
    "    2. Biased training on top 30% + partial (30 epochs)\n",
    "    3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select best from remaining\n",
    "    \n",
    "    Returns selected_gids, precision, partial_correct_gids\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training (60 epochs, track last 5) + Enriched selection ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n",
    "    \n",
    "    # Train without tracking for first (60-5)=55 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # Track last 5 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # Compute anchors for ENRICHED selection\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    # Select using ENRICHED method (gradient + features)\n",
    "    all_selections = []\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_is_correct, best_gid, best_class = -np.inf, False, None, None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED scoring (gradient + features)\n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "                best_class = class_id\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid, best_class))\n",
    "    \n",
    "    # Sort and get top 30%\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * GGH_TOP_PERCENTILE / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # Iter1 precision\n",
    "    iter1_correct = sum(1 for s in top_selections if s[1])\n",
    "    iter1_precision = iter1_correct / len(top_selections) * 100 if top_selections else 0\n",
    "    \n",
    "    # Track iter1 class distribution\n",
    "    iter1_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_selections:\n",
    "        gid = s[3]\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        iter1_class_counts[class_id] += 1\n",
    "    \n",
    "    # === ITERATION 2: Biased training on top 30% + partial ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                             MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                             MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, \n",
    "                                   partial_weight=GGH_ITER2_PARTIAL_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # === ITERATION 3: EXPANSION - Score REMAINING 70% with Enriched+Loss ===\n",
    "    all_sample_indices = set(range(n_samples))\n",
    "    remaining_sample_indices = all_sample_indices - top_sample_indices - partial_sample_indices\n",
    "    \n",
    "    # First, score partial data to build anchors with loss\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Build anchors with loss from partial data\n",
    "    anchor_data_biased = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},  # Will include loss\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "        'loss_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    # Compute normalization parameters\n",
    "    all_grads = [partial_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in partial_analysis and partial_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_biased['grad_scale'] = grad_scale\n",
    "    \n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        correct_losses, incorrect_losses = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    correct_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    incorrect_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            # Gradient anchors\n",
    "            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            # Feature normalization\n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n",
    "            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n",
    "            \n",
    "            # Loss normalization\n",
    "            all_losses = correct_losses + incorrect_losses\n",
    "            loss_mean = np.mean(all_losses)\n",
    "            loss_std = np.std(all_losses) + 1e-8\n",
    "            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n",
    "            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n",
    "            \n",
    "            # Enriched anchors = gradient + features + loss\n",
    "            correct_enriched = [np.concatenate([g, f, [l]]) \n",
    "                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n",
    "            incorrect_enriched = [np.concatenate([g, f, [l]]) \n",
    "                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n",
    "            \n",
    "            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    # Score REMAINING 70%\n",
    "    scorer = RemainingDataScorer(DO, model_biased, remaining_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    remaining_analysis = scorer.get_analysis()\n",
    "    \n",
    "    # Score each hypothesis in remaining samples using Enriched+Loss\n",
    "    remaining_scored = []\n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        start_gid = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_gid = None\n",
    "        best_is_correct = False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start_gid + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in remaining_analysis or remaining_analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = remaining_analysis[gid]['avg_gradient']\n",
    "            loss = remaining_analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED + LOSS scoring\n",
    "            score = compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            remaining_scored.append({\n",
    "                'sample_idx': sample_idx,\n",
    "                'gid': best_gid,\n",
    "                'score': best_score,\n",
    "                'is_correct': best_is_correct\n",
    "            })\n",
    "    \n",
    "    # Sort remaining by score and take top 20% (unseen from Iter1)\n",
    "    remaining_scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "    n_take_from_remaining = int(len(remaining_scored) * GGH_ITER3_TOP_PERCENTILE / 100)\n",
    "    top_remaining = remaining_scored[:n_take_from_remaining]\n",
    "    \n",
    "    # Calculate precision on top of remaining\n",
    "    iter3_correct = sum(1 for s in top_remaining if s['is_correct'])\n",
    "    iter3_precision = iter3_correct / len(top_remaining) * 100 if top_remaining else 0\n",
    "    \n",
    "    # Track iter3 class distribution\n",
    "    iter3_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_remaining:\n",
    "        class_id = DO.df_train_hypothesis.iloc[s['gid']]['hyp_class_id']\n",
    "        iter3_class_counts[class_id] += 1\n",
    "    \n",
    "    # COMBINE Iter1 + Iter3 selections\n",
    "    iter1_gids = set(s[3] for s in top_selections)\n",
    "    iter3_gids = set(s['gid'] for s in top_remaining)\n",
    "    selected_gids = iter1_gids | iter3_gids\n",
    "    \n",
    "    # Calculate combined precision\n",
    "    combined_correct = sum(1 for s in top_selections if s[1]) + iter3_correct\n",
    "    combined_precision = combined_correct / len(selected_gids) * 100 if selected_gids else 0\n",
    "    \n",
    "    # Combined class distribution\n",
    "    combined_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for gid in selected_gids:\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        combined_class_counts[class_id] += 1\n",
    "    \n",
    "    print(f\"  Iter1 top {GGH_TOP_PERCENTILE}% precision: {iter1_precision:.1f}%, class dist: {iter1_class_counts}\")\n",
    "    print(f\"  Iter3 top {GGH_ITER3_TOP_PERCENTILE}% (of remaining) precision: {iter3_precision:.1f}%, class dist: {iter3_class_counts}\")\n",
    "    print(f\"  Combined: {len(selected_gids)} samples ({len(iter1_gids)} iter1 + {len(iter3_gids)} iter3), precision: {combined_precision:.1f}%, class dist: {combined_class_counts}\")\n",
    "    \n",
    "    return selected_gids, combined_precision, partial_correct_gids, iter1_class_counts, iter3_class_counts, combined_class_counts\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method (Expansion Strategy):\")\n",
    "print(f\"  Iter1: {GGH_ITER1_EPOCHS} epochs unbiased (lr={GGH_ITER1_LR}), last {GGH_ITER1_ANALYSIS_EPOCHS} tracked\")\n",
    "print(f\"  Iter1: Enriched selection (gradient + features) -> top {GGH_TOP_PERCENTILE}%\")\n",
    "print(f\"  Iter2: {GGH_ITER2_EPOCHS} epochs biased (lr={GGH_ITER2_LR}, pw={GGH_ITER2_PARTIAL_WEIGHT}) on top 30% + partial\")\n",
    "print(f\"  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\")\n",
    "print(f\"  Final: Train on expansion selection + partial (pw={BENCHMARK_PARTIAL_WEIGHT})\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {BENCHMARK_FINAL_EPOCHS} epochs, validation-based epoch selection, same architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, run_rand_state in enumerate(BENCHMARK_RAND_STATES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup DataOperator\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === Run GGH Expansion selection ===\n",
    "    print(\"Running GGH Expansion selection...\")\n",
    "    ggh_selected_gids, ggh_precision, _, iter1_cls, iter3_cls, combined_cls = run_ggh_expansion(DO_run, run_rand_state)\n",
    "    \n",
    "    # Calculate dynamic partial weight to maintain ~25% partial in training\n",
    "    n_combined = len(ggh_selected_gids)\n",
    "    partial_weight_dynamic = 0.25 * n_combined / (0.75 * len(partial_gids))\n",
    "    print(f\"  Dynamic partial_weight: {partial_weight_dynamic:.2f}\")\n",
    "    \n",
    "    # === Train GGH final model ===\n",
    "    print(f\"Training GGH model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight_dynamic, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    print(f\"Training Partial model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(), partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,  # Positive = GGH better\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, \"\n",
    "          f\"MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'GGH Prec':<10} {'GGH Loss':<12} {'Part Loss':<12} {'Δ Loss':<10} {'GGH R2':<10} {'Part R2':<10} {'Δ R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<12.4f} \"\n",
    "          f\"{r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGGH Expansion Precision: {np.mean(ggh_precisions):.1f}% ± {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f} ± {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f} ± {np.std(partial_losses):.4f}\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} ± {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f} ± {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{BENCHMARK_N_RUNS} ({n_ggh_wins_loss/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{BENCHMARK_N_RUNS} ({n_ggh_wins_r2/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH Expansion vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 2: R2 comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, ggh_r2s, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, partial_r2s, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Test R2')\n",
    "ax2.set_title('Test R2: GGH Expansion vs Partial')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 3: GGH Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, BENCHMARK_N_RUNS+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('GGH Expansion Precision (%)')\n",
    "ax3.set_title('GGH Hypothesis Precision (from Remaining 70%)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_expansion_enriched_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement_loss = np.mean([r['improvement_loss'] for r in results])\n",
    "avg_improvement_r2 = np.mean([r['improvement_r2'] for r in results])\n",
    "if avg_improvement_loss > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on Loss (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average loss improvement: {avg_improvement_loss:.4f} MSE\")\n",
    "elif avg_improvement_loss < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on Loss (p={p_value_loss:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in Loss (p={p_value_loss:.4f})\")\n",
    "\n",
    "if avg_improvement_r2 > 0 and p_value_r2 < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on R2 (p={p_value_r2:.4f})\")\n",
    "    print(f\"Average R2 improvement: {avg_improvement_r2:.4f}\")\n",
    "elif avg_improvement_r2 < 0 and p_value_r2 < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on R2 (p={p_value_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in R2 (p={p_value_r2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01483701-5176-4433-9e91-7d159ae81b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
