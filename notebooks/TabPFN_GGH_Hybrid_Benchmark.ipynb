{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TabPFN + GGH Hybrid Imputation Benchmark\n",
    "\n",
    "## Research Goal\n",
    "Combine the strengths of **TabPFN** (pre-trained transformer with learned priors) and **GGH** (gradient-guided hypothesis selection with domain knowledge) into a hybrid imputation method.\n",
    "\n",
    "## Two-Stage Hybrid Approach\n",
    "1. **Stage 1: TabPFN Initial Imputation**\n",
    "   - Map training data to nearest hypothesis class\n",
    "   - TabPFN predicts among K hypothesis classes (domain-constrained)\n",
    "   - Get class probabilities for confidence estimation\n",
    "\n",
    "2. **Stage 2: GGH Gradient Refinement**\n",
    "   - Train downstream model briefly with Stage 1 imputations\n",
    "   - Compute gradients for partial data (known correct)\n",
    "   - Build per-class anchors (correct vs incorrect)\n",
    "   - For low-confidence samples: combine TabPFN probs with GGH anchor similarity\n",
    "\n",
    "## Expected Outcome\n",
    "- TabPFN+GGH Hybrid should outperform TabPFN alone (adds domain knowledge)\n",
    "- May match or exceed GGH by leveraging TabPFN's learned priors as warm start\n",
    "- Biggest gains on samples where TabPFN is uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport sys\nsys.path.insert(0, '../')\nsys.path.insert(0, '../GGH')\n\nfrom GGH.data_ops import DataOperator\nfrom GGH.selection_algorithms import AlgoModulators\nfrom GGH.models import initialize_model\nfrom GGH.train_val_loop import TrainValidationManager\nfrom GGH.inspector import Inspector\nfrom scipy import stats\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.autograd import grad\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef set_to_deterministic(rand_state):\n    import random\n    random.seed(rand_state)\n    np.random.seed(rand_state)\n    torch.manual_seed(rand_state)\n    torch.set_num_threads(1)\n    torch.use_deterministic_algorithms(True)\n\nprint(\"Imports successful!\")\n\n# GPU Detection\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Photocell Degradation\n",
      "Hypothesis values: [0.03 0.11 0.2  0.32 0.43 0.6 ]\n",
      "Number of hypotheses: 6\n",
      "Partial percentage: 3.0%\n",
      "Benchmark runs: 15\n",
      "Epochs: 600\n",
      "\n",
      "Hybrid parameters:\n",
      "  Brief training epochs: 30\n",
      "  Confidence threshold: 0.6\n",
      "Results will be saved to: ../saved_results/TabPFN_GGH_Hybrid\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA CONFIGURATION - Photocell Degradation Dataset\n",
    "# =============================================================================\n",
    "data_path = '../data/dataset_photo_pce10/data.csv'\n",
    "results_path = \"../saved_results/TabPFN_GGH_Hybrid\"\n",
    "\n",
    "# Variables\n",
    "inpt_vars = ['P3HT', 'PTB7-Th']\n",
    "target_vars = ['Degradation']\n",
    "miss_vars = ['PCBM']\n",
    "\n",
    "# Hypothesis values (6 PCBM concentration values)\n",
    "hypothesis = [[0.03, 0.11, 0.20, 0.32, 0.43, 0.6]]\n",
    "HYPOTHESIS_VALUES = np.array(hypothesis[0])\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])  # 6 hypotheses\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.03  # 3% complete data\n",
    "dropout = 0.05\n",
    "lr = 0.004\n",
    "nu = 0.1\n",
    "\n",
    "# Benchmark parameters\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_EPOCHS = 600\n",
    "\n",
    "# TabPFN+GGH Hybrid parameters\n",
    "HYBRID_BRIEF_EPOCHS = 30           # Brief training for gradient computation\n",
    "HYBRID_CONFIDENCE_THRESHOLD = 0.6  # Re-refine samples where TabPFN max prob < 60%\n",
    "HYBRID_LR = 0.01                   # Learning rate for brief training\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset: Photocell Degradation\")\n",
    "print(f\"Hypothesis values: {HYPOTHESIS_VALUES}\")\n",
    "print(f\"Number of hypotheses: {hyp_per_sample}\")\n",
    "print(f\"Partial percentage: {partial_perc*100}%\")\n",
    "print(f\"Benchmark runs: {BENCHMARK_N_RUNS}\")\n",
    "print(f\"Epochs: {BENCHMARK_EPOCHS}\")\n",
    "print(f\"\\nHybrid parameters:\")\n",
    "print(f\"  Brief training epochs: {HYBRID_BRIEF_EPOCHS}\")\n",
    "print(f\"  Confidence threshold: {HYBRID_CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"Results will be saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tabpfn_constrained_header",
   "metadata": {},
   "source": [
    "## TabPFN Constrained to Hypothesis Values\n",
    "\n",
    "First, we implement a TabPFN imputer that is constrained to only output hypothesis values (fair comparison with GGH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tabpfn_constrained",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNConstrainedImputer defined.\n"
     ]
    }
   ],
   "source": [
    "class TabPFNConstrainedImputer:\n",
    "    \"\"\"TabPFN imputer constrained to hypothesis values only.\n",
    "    \n",
    "    Unlike standard TabPFN which predicts among all unique values in training data,\n",
    "    this version maps training values to nearest hypothesis and predicts among those.\n",
    "    This gives TabPFN the same domain knowledge as GGH for fair comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hypothesis_values, rand_state, device=None):\n",
    "        self.hypothesis_values = np.array(hypothesis_values)\n",
    "        self.rand_state = rand_state\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        from tabpfn import TabPFNClassifier\n",
    "        self.tabpfn_cls = TabPFNClassifier\n",
    "        \n",
    "    def fit_transform(self, matrix):\n",
    "        \"\"\"Impute missing values, constrained to hypothesis values.\n",
    "        \n",
    "        Returns:\n",
    "            imputed_values: Array of imputed values (hypothesis values only)\n",
    "            probabilities: Class probabilities from TabPFN\n",
    "            missing_mask: Boolean mask of which rows were imputed\n",
    "        \"\"\"\n",
    "        np.random.seed(self.rand_state)\n",
    "        torch.manual_seed(self.rand_state)\n",
    "        \n",
    "        mask = np.isnan(matrix)\n",
    "        if not mask.any():\n",
    "            return matrix[:, -1], None, np.zeros(len(matrix), dtype=bool)\n",
    "        \n",
    "        column = np.argwhere(np.sum(mask, axis=0) > 0)[0, 0]\n",
    "        \n",
    "        X = np.delete(matrix, column, axis=1)\n",
    "        y = matrix[:, column]\n",
    "        \n",
    "        known_mask = ~mask[:, column]\n",
    "        X_train, y_train = X[known_mask], y[known_mask]\n",
    "        X_missing = X[mask[:, column]]\n",
    "        \n",
    "        # Map y_train to nearest hypothesis class\n",
    "        y_train_mapped = np.array([\n",
    "            self.hypothesis_values[np.argmin(np.abs(self.hypothesis_values - v))]\n",
    "            for v in y_train\n",
    "        ])\n",
    "        \n",
    "        # Encode as class indices (0 to K-1)\n",
    "        y_train_encoded = np.array([\n",
    "            np.argmin(np.abs(self.hypothesis_values - v)) for v in y_train_mapped\n",
    "        ])\n",
    "        \n",
    "        # Train TabPFN\n",
    "        try:\n",
    "            model = self.tabpfn_cls(device=self.device)\n",
    "        except TypeError:\n",
    "            model = self.tabpfn_cls()\n",
    "        \n",
    "        model.fit(X_train, y_train_encoded)\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        predictions = model.predict(X_missing)\n",
    "        probs = model.predict_proba(X_missing)\n",
    "        \n",
    "        # Map predictions back to hypothesis values\n",
    "        imputed_values = self.hypothesis_values[predictions]\n",
    "        \n",
    "        return imputed_values, probs, mask[:, column]\n",
    "\n",
    "print(\"TabPFNConstrainedImputer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid_header",
   "metadata": {},
   "source": [
    "## TabPFN + GGH Hybrid Imputer\n",
    "\n",
    "The main innovation: combine TabPFN's predictions with GGH's gradient-based anchor similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hybrid_imputer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNGGHHybridImputer defined.\n"
     ]
    }
   ],
   "source": "class HypothesisAmplifyingModel(nn.Module):\n    \"\"\"Neural network for gradient computation in GGH refinement.\"\"\"\n    def __init__(self, n_shared_features, n_hypothesis_features=1, \n                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n        super().__init__()\n        \n        self.shared_path = nn.Sequential(\n            nn.Linear(n_shared_features, shared_hidden),\n            nn.ReLU(),\n        )\n        \n        self.hypothesis_path = nn.Sequential(\n            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n            nn.ReLU(),\n            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n            nn.ReLU(),\n        )\n        \n        combined_size = shared_hidden + hypothesis_hidden\n        self.final_path = nn.Sequential(\n            nn.Linear(combined_size, final_hidden),\n            nn.ReLU(),\n            nn.Linear(final_hidden, output_size)\n        )\n        \n        self.n_shared = n_shared_features\n        \n    def forward(self, x):\n        shared_features = x[:, :self.n_shared]\n        hypothesis_feature = x[:, self.n_shared:]\n        \n        shared_emb = self.shared_path(shared_features)\n        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n        \n        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n        return self.final_path(combined)\n\n\nclass TabPFNGGHHybridImputer:\n    \"\"\"Two-stage TabPFN + GGH hybrid imputation.\n    \n    Stage 1: TabPFN imputes with hypothesis constraints, returns probabilities\n    Stage 2: For low-confidence samples, refine using GGH anchor similarity\n    \"\"\"\n    \n    def __init__(self, hypothesis_values, rand_state, device=None,\n                 brief_epochs=30, confidence_threshold=0.6, lr=0.01):\n        self.hypothesis_values = np.array(hypothesis_values)\n        self.rand_state = rand_state\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.brief_epochs = brief_epochs\n        self.confidence_threshold = confidence_threshold\n        self.lr = lr\n        \n        # Initialize TabPFN\n        self.tabpfn_imputer = TabPFNConstrainedImputer(\n            hypothesis_values, rand_state, device\n        )\n        \n        # Stats tracking\n        self.stats = {\n            'total_imputed': 0,\n            'low_confidence': 0,\n            'refined': 0,\n            'changed': 0,\n        }\n        \n    def fit_transform(self, DO, batch_size=32):\n        \"\"\"Two-stage imputation with GGH refinement.\n        \n        Args:\n            DO: DataOperator with partial data info\n            batch_size: Batch size for brief training\n            \n        Returns:\n            imputed_matrix: Matrix with imputed values\n        \"\"\"\n        from GGH.imputation_methods import prep_imputation_input\n        \n        # Prepare imputation input (same as standard Imputer)\n        df_train_imp = prep_imputation_input(\n            DO, DO.df_train[DO.inpt_vars + DO.miss_vars + DO.target_vars], \n            DO.miss_vars, DO.partial_rows_id\n        )\n        matrix = df_train_imp.values\n        \n        # Stage 1: TabPFN initial imputation\n        print(\"  Stage 1: TabPFN initial imputation...\")\n        initial_values, tabpfn_probs, missing_mask = self.tabpfn_imputer.fit_transform(matrix)\n        \n        if tabpfn_probs is None:\n            # No missing values\n            return matrix\n        \n        self.stats['total_imputed'] = len(initial_values)\n        \n        # Create initial imputed matrix\n        matrix_imputed = matrix.copy()\n        column = np.argwhere(np.sum(np.isnan(matrix), axis=0) > 0)[0, 0]\n        matrix_imputed[missing_mask, column] = initial_values\n        \n        # Identify low-confidence samples\n        max_probs = np.max(tabpfn_probs, axis=1)\n        low_confidence_mask = max_probs < self.confidence_threshold\n        self.stats['low_confidence'] = np.sum(low_confidence_mask)\n        \n        print(f\"    Total imputed: {self.stats['total_imputed']}\")\n        print(f\"    Low confidence (<{self.confidence_threshold}): {self.stats['low_confidence']}\")\n        \n        if not low_confidence_mask.any():\n            print(\"    No low-confidence samples to refine.\")\n            return matrix_imputed\n        \n        # Stage 2: GGH refinement for low-confidence samples\n        print(f\"  Stage 2: GGH gradient refinement ({self.brief_epochs} epochs)...\")\n        \n        # Brief training to get gradients\n        anchors = self._compute_ggh_anchors(matrix_imputed, DO, batch_size)\n        \n        if anchors is None:\n            print(\"    Could not compute anchors, skipping refinement.\")\n            return matrix_imputed\n        \n        # Refine low-confidence samples\n        X_missing = np.delete(matrix, column, axis=1)[missing_mask]\n        refined_values = initial_values.copy()\n        \n        for i in np.where(low_confidence_mask)[0]:\n            tabpfn_score = tabpfn_probs[i]  # Shape: (num_hypotheses,)\n            ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n            \n            if ggh_score is None:\n                continue\n            \n            # Combine: multiply and renormalize\n            combined = tabpfn_score * ggh_score\n            combined_sum = combined.sum()\n            if combined_sum > 0:\n                combined /= combined_sum\n            else:\n                combined = tabpfn_score  # Fallback to TabPFN only\n            \n            new_value = self.hypothesis_values[np.argmax(combined)]\n            \n            if new_value != refined_values[i]:\n                self.stats['changed'] += 1\n            \n            refined_values[i] = new_value\n            self.stats['refined'] += 1\n        \n        print(f\"    Refined: {self.stats['refined']}, Changed: {self.stats['changed']}\")\n        \n        # Update matrix with refined values\n        matrix_imputed[missing_mask, column] = refined_values\n        \n        return matrix_imputed\n    \n    def _compute_ggh_anchors(self, matrix_imputed, DO, batch_size):\n        \"\"\"Brief training to compute GGH-style anchors per class.\"\"\"\n        set_to_deterministic(self.rand_state + 500)\n        \n        n_shared = len(DO.inpt_vars)\n        n_hyp = len(DO.miss_vars)\n        out_size = len(DO.target_vars)\n        hyp_per_sample = DO.num_hyp_comb\n        \n        # Get partial data info\n        partial_correct_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == True)\n        ].index.tolist())\n        blacklisted_gids = set(DO.df_train_hypothesis[\n            (DO.df_train_hypothesis['partial_full_info'] == 1) & \n            (DO.df_train_hypothesis['correct_hypothesis'] == False)\n        ].index.tolist())\n        \n        if not partial_correct_gids or not blacklisted_gids:\n            return None\n        \n        # Create dataloader with imputed data\n        input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n        n_samples = len(DO.df_train_hypothesis)\n        global_ids = torch.arange(n_samples)\n        \n        dataset = TensorDataset(\n            torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n            torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n            global_ids\n        )\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        \n        # Train model briefly\n        model = HypothesisAmplifyingModel(n_shared, n_hyp, 16, 32, 32, out_size)\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n        criterion = nn.MSELoss()\n        \n        model.train()\n        for epoch in range(self.brief_epochs):\n            for inputs, targets, gids in dataloader:\n                predictions = model(inputs)\n                loss = criterion(predictions, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        \n        # Compute gradients for partial data\n        model.eval()\n        gradient_data = {}\n        \n        for inputs, targets, gids in dataloader:\n            for i in range(len(inputs)):\n                gid = gids[i].item()\n                if gid not in partial_correct_gids and gid not in blacklisted_gids:\n                    continue\n                \n                inp = inputs[i:i+1].clone().requires_grad_(True)\n                pred = model(inp)\n                loss = nn.MSELoss()(pred, targets[i:i+1])\n                \n                params = list(model.parameters())\n                grad_param = grad(loss, params[-2], retain_graph=False)[0]\n                grad_vec = grad_param.flatten().detach().cpu().numpy()\n                \n                is_correct = gid in partial_correct_gids\n                class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n                features = DO.df_train_hypothesis.loc[gid, DO.inpt_vars].values.astype(np.float64)\n                \n                gradient_data[gid] = {\n                    'gradient': grad_vec,\n                    'features': features,\n                    'is_correct': is_correct,\n                    'class_id': class_id\n                }\n        \n        # Build per-class anchors\n        anchors = {\n            'correct_features': {},\n            'incorrect_features': {},\n            'correct_grads': {},\n            'incorrect_grads': {},\n        }\n        \n        for class_id in range(hyp_per_sample):\n            correct_feats = []\n            incorrect_feats = []\n            correct_grads = []\n            incorrect_grads = []\n            \n            for gid, data in gradient_data.items():\n                if data['class_id'] != class_id:\n                    continue\n                if data['is_correct']:\n                    correct_feats.append(data['features'])\n                    correct_grads.append(data['gradient'])\n                else:\n                    incorrect_feats.append(data['features'])\n                    incorrect_grads.append(data['gradient'])\n            \n            if correct_feats and incorrect_feats:\n                anchors['correct_features'][class_id] = np.mean(correct_feats, axis=0)\n                anchors['incorrect_features'][class_id] = np.mean(incorrect_feats, axis=0)\n                anchors['correct_grads'][class_id] = np.mean(correct_grads, axis=0)\n                anchors['incorrect_grads'][class_id] = np.mean(incorrect_grads, axis=0)\n        \n        return anchors\n    \n    def _compute_anchor_similarity(self, features, anchors):\n        \"\"\"Compute similarity scores for each hypothesis class.\n        \n        Returns array of shape (num_hypotheses,) with similarity scores.\n        Higher score = more similar to correct anchor.\n        \"\"\"\n        scores = np.zeros(len(self.hypothesis_values))\n        \n        for class_id in range(len(self.hypothesis_values)):\n            if class_id not in anchors['correct_features']:\n                scores[class_id] = 0.5  # Neutral if no anchor\n                continue\n            \n            correct_feat = anchors['correct_features'][class_id]\n            incorrect_feat = anchors['incorrect_features'][class_id]\n            \n            # Cosine similarity to correct vs incorrect anchors\n            sim_correct = np.dot(features, correct_feat) / (\n                np.linalg.norm(features) * np.linalg.norm(correct_feat) + 1e-8\n            )\n            sim_incorrect = np.dot(features, incorrect_feat) / (\n                np.linalg.norm(features) * np.linalg.norm(incorrect_feat) + 1e-8\n            )\n            \n            # Convert to 0-1 score (higher = more similar to correct)\n            raw_score = sim_correct - sim_incorrect\n            scores[class_id] = 1 / (1 + np.exp(-raw_score))  # Sigmoid\n        \n        return scores\n\nprint(\"TabPFNGGHHybridImputer defined.\")"
  },
  {
   "cell_type": "markdown",
   "id": "ggh_functions_header",
   "metadata": {},
   "source": [
    "## GGH Soft Refinement Functions\n",
    "\n",
    "Import the GGH soft refinement functions from Photocell benchmark for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ggh_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGH helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# GGH Configuration\n",
    "GGH_ITER1_EPOCHS = 60\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5\n",
    "GGH_ITER1_LR = 0.01\n",
    "GGH_ITER2_EPOCHS = 30\n",
    "GGH_ITER2_LR = 0.01\n",
    "GGH_SCORING_PASSES = 5\n",
    "GGH_FINAL_EPOCHS = 200\n",
    "GGH_MIN_WEIGHT = 0.1\n",
    "GGH_TEMPERATURE_ITER1 = 1.0\n",
    "GGH_TEMPERATURE_ITER3 = 0.8\n",
    "GGH_LOSS_INFLUENCE = 0.25\n",
    "GGH_PARTIAL_BASE_WEIGHT = 2.0\n",
    "GGH_BENCHMARK_LR = 0.01\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "# Import GGH functions from Photocell benchmark (copy essential ones)\n",
    "def sigmoid_stable(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def compute_soft_weights(scores, min_weight=0.1, temperature=1.0):\n",
    "    scores = np.array(scores, dtype=np.float64)\n",
    "    if len(scores) == 0:\n",
    "        return np.array([])\n",
    "    mean_s = np.mean(scores)\n",
    "    std_s = np.std(scores) + 1e-8\n",
    "    normalized = (scores - mean_s) / std_s\n",
    "    raw_weights = sigmoid_stable(normalized / temperature)\n",
    "    weights = min_weight + (1 - min_weight) * raw_weights\n",
    "    return weights\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "print(\"GGH helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## Benchmark: TabPFN vs TabPFN+GGH Hybrid\n",
    "\n",
    "Compare:\n",
    "1. **Full Info** (Oracle)\n",
    "2. **Partial** (Baseline)\n",
    "3. **TabPFN Constrained** (TabPFN with hypothesis constraints)\n",
    "4. **TabPFN+GGH Hybrid** (Our new method)\n",
    "5. **GGH Soft Refinement** (Original GGH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: TabPFN vs TabPFN+GGH Hybrid on Photocell\n",
      "================================================================================\n",
      "Methods compared:\n",
      "  - Full Info (oracle)\n",
      "  - Partial (baseline)\n",
      "  - TabPFN Constrained (hypothesis values only)\n",
      "  - TabPFN+GGH Hybrid (NEW)\n",
      "Training epochs: 600\n",
      "Number of runs: 15\n",
      "Hybrid confidence threshold: 0.6\n",
      "================================================================================\n",
      "Using r_states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16]\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=0)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n",
      "  Full Info R2: 0.8350\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.2697\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: -1.2025\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 736\n",
      "    Low confidence (<0.6): 736\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 1 Summary:\n",
      "    Full Info: 0.8350\n",
      "    Partial: 0.2697\n",
      "    TabPFN Constrained: -1.2025\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=1)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Full Info...\n",
      "  Full Info R2: 0.8697\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: -0.3155\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.2809\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 733\n",
      "    Low confidence (<0.6): 733\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 2 Summary:\n",
      "    Full Info: 0.8697\n",
      "    Partial: -0.3155\n",
      "    TabPFN Constrained: 0.2809\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=2)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Info R2: 0.8931\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: -0.2415\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.1489\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 730\n",
      "    Low confidence (<0.6): 730\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 3 Summary:\n",
      "    Full Info: 0.8931\n",
      "    Partial: -0.2415\n",
      "    TabPFN Constrained: 0.1489\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=3)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Info R2: 0.9169\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: -0.0361\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: -0.5406\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 735\n",
      "    Low confidence (<0.6): 452\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 4 Summary:\n",
      "    Full Info: 0.9169\n",
      "    Partial: -0.0361\n",
      "    TabPFN Constrained: -0.5406\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=4)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Full Info...\n",
      "  Full Info R2: 0.7224\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.7539\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.0850\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 730\n",
      "    Low confidence (<0.6): 718\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 5 Summary:\n",
      "    Full Info: 0.7224\n",
      "    Partial: 0.7539\n",
      "    TabPFN Constrained: 0.0850\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=5)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Info R2: 0.8583\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.4309\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.1245\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 735\n",
      "    Low confidence (<0.6): 735\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 6 Summary:\n",
      "    Full Info: 0.8583\n",
      "    Partial: 0.4309\n",
      "    TabPFN Constrained: 0.1245\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=6)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Full Info...\n",
      "  Full Info R2: 0.7352\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.2176\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.1856\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 734\n",
      "    Low confidence (<0.6): 734\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 7 Summary:\n",
      "    Full Info: 0.7352\n",
      "    Partial: 0.2176\n",
      "    TabPFN Constrained: 0.1856\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=7)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Info R2: 0.8879\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.5407\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.3633\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n",
      "    Total imputed: 735\n",
      "    Low confidence (<0.6): 666\n",
      "  Stage 2: GGH gradient refinement (30 epochs)...\n",
      "  TabPFN+GGH Hybrid FAILED: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n",
      "\n",
      ">>> Run 8 Summary:\n",
      "    Full Info: 0.8879\n",
      "    Partial: 0.5407\n",
      "    TabPFN Constrained: 0.3633\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=8)\n",
      "============================================================\n",
      "\n",
      "Training Full Info...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_495478/905720842.py\", line 143, in <module>\n",
      "    matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 130, in fit_transform\n",
      "    ggh_score = self._compute_anchor_similarity(X_missing[i], anchors)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_495478/343590846.py\", line 284, in _compute_anchor_similarity\n",
      "    sim_correct = np.dot(features, correct_feat) / (\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Full Info R2: 0.8793\n",
      "\n",
      "Training Partial...\n",
      "  Partial R2: 0.3586\n",
      "\n",
      "Running TabPFN Constrained...\n",
      "  TabPFN Constrained R2: 0.0169\n",
      "\n",
      "Running TabPFN+GGH Hybrid...\n",
      "  Stage 1: TabPFN initial imputation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": "def full_experiment(use_info, DO, INSPECT, batch_size, hidden_size, output_size, num_epochs, \n                    rand_state, results_path, dropout=0.05, lr=0.004, nu=0.1, final_analysis=False):\n    \"\"\"Standard full experiment for Full Info / Partial.\"\"\"\n    AM = AlgoModulators(DO, lr=lr, nu=nu)\n    dataloader = DO.prep_dataloader(use_info, batch_size)\n    model = initialize_model(DO, dataloader, hidden_size, rand_state, dropout=dropout)\n    TVM = TrainValidationManager(use_info, num_epochs, dataloader, batch_size, rand_state, \n                                 results_path, final_analysis=final_analysis)\n    TVM.train_model(DO, AM, model, final_analysis=final_analysis)\n    INSPECT.save_train_val_logs(DO, AM, TVM, model, final_analysis=final_analysis)\n    return DO, TVM, model\n\n\nprint(\"=\" * 80)\nprint(\"BENCHMARK: TabPFN vs TabPFN+GGH Hybrid on Photocell\")\nprint(\"=\" * 80)\nprint(f\"Methods compared:\")\nprint(f\"  - Full Info (oracle)\")\nprint(f\"  - Partial (baseline)\")\nprint(f\"  - TabPFN Constrained (hypothesis values only)\")\nprint(f\"  - TabPFN+GGH Hybrid (NEW)\")\nprint(f\"Training epochs: {BENCHMARK_EPOCHS}\")\nprint(f\"Number of runs: {BENCHMARK_N_RUNS}\")\nprint(f\"Hybrid confidence threshold: {HYBRID_CONFIDENCE_THRESHOLD}\")\nprint(\"=\" * 80)\n\n# Store results\nall_results = {\n    'Full Info': {'r2': [], 'mse': [], 'mae': []},\n    'Partial': {'r2': [], 'mse': [], 'mae': []},\n    'TabPFN Constrained': {'r2': [], 'mse': [], 'mae': []},\n    'TabPFN+GGH Hybrid': {'r2': [], 'mse': [], 'mae': [], 'refined': [], 'changed': []},\n}\n\n# Find valid r_states\nvalid_r_states = []\nfor r_state in range(2000):\n    if len(valid_r_states) >= BENCHMARK_N_RUNS:\n        break\n    set_to_deterministic(r_state)\n    DO_test = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                           partial_perc, r_state, device=DEVICE)\n    if not DO_test.lack_partial_coverage:\n        valid_r_states.append(r_state)\n\nprint(f\"Using r_states: {valid_r_states}\")\n\n# Run benchmark\nfor run_idx, r_state in enumerate(valid_r_states):\n    print(f\"\\n{'='*60}\")\n    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={r_state})\")\n    print(f\"{'='*60}\")\n    \n    set_to_deterministic(r_state)\n    DO = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                      partial_perc, r_state, device=DEVICE)\n    \n    # === Full Info ===\n    print(\"\\nTraining Full Info...\")\n    set_to_deterministic(r_state)\n    DO_full = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                           partial_perc, r_state, device=DEVICE)\n    INSPECT_full = Inspector(results_path, hidden_size)\n    DO_full, TVM_full, model_full = full_experiment(\n        \"full info\", DO_full, INSPECT_full, batch_size, hidden_size, output_size,\n        BENCHMARK_EPOCHS, r_state, results_path, dropout, lr, nu\n    )\n    full_r2 = INSPECT_full.calculate_val_r2score(DO_full, TVM_full, model_full, data=\"test\")\n    _, _, full_r2_direct = evaluate_on_test(DO, model_full)\n    all_results['Full Info']['r2'].append(full_r2)\n    print(f\"  Full Info R2: {full_r2:.4f}\")\n    \n    # === Partial ===\n    print(\"\\nTraining Partial...\")\n    set_to_deterministic(r_state)\n    DO_partial = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                              partial_perc, r_state, device=DEVICE)\n    INSPECT_partial = Inspector(results_path, hidden_size)\n    DO_partial, TVM_partial, model_partial = full_experiment(\n        \"partial info\", DO_partial, INSPECT_partial, batch_size, hidden_size, output_size,\n        BENCHMARK_EPOCHS, r_state, results_path, dropout, lr, nu\n    )\n    partial_r2 = INSPECT_partial.calculate_val_r2score(DO_partial, TVM_partial, model_partial, data=\"test\")\n    all_results['Partial']['r2'].append(partial_r2)\n    print(f\"  Partial R2: {partial_r2:.4f}\")\n    \n    # === TabPFN Constrained ===\n    print(\"\\nRunning TabPFN Constrained...\")\n    set_to_deterministic(r_state)\n    DO_tabpfn = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                             partial_perc, r_state, device=DEVICE)\n    \n    try:\n        from GGH.imputation_methods import prep_imputation_input\n        df_train_imp = prep_imputation_input(\n            DO_tabpfn, DO_tabpfn.df_train[DO_tabpfn.inpt_vars + DO_tabpfn.miss_vars + DO_tabpfn.target_vars], \n            DO_tabpfn.miss_vars, DO_tabpfn.partial_rows_id\n        )\n        matrix = df_train_imp.values\n        \n        tabpfn_imputer = TabPFNConstrainedImputer(HYPOTHESIS_VALUES, r_state)\n        imputed_values, _, missing_mask = tabpfn_imputer.fit_transform(matrix)\n        \n        # Create imputed matrix and train\n        column = np.argwhere(np.sum(np.isnan(matrix), axis=0) > 0)[0, 0]\n        matrix_imputed = matrix.copy()\n        matrix_imputed[missing_mask, column] = imputed_values\n        imput_input = matrix_imputed[:, :-1]  # Remove target column\n        \n        AM_tabpfn = AlgoModulators(DO_tabpfn, lr=lr)\n        dataloader_tabpfn = DO_tabpfn.prep_dataloader(\"use imputation\", batch_size, imputed_input=imput_input)\n        \n        set_to_deterministic(r_state)\n        model_tabpfn = initialize_model(DO_tabpfn, dataloader_tabpfn, hidden_size, r_state, dropout=dropout)\n        TVM_tabpfn = TrainValidationManager(\"use imputation\", BENCHMARK_EPOCHS, dataloader_tabpfn, \n                                            batch_size, r_state, results_path, \n                                            imput_method=\"TabPFN Constrained\", final_analysis=False)\n        TVM_tabpfn.train_model(DO_tabpfn, AM_tabpfn, model_tabpfn, final_analysis=False)\n        \n        INSPECT_tabpfn = Inspector(results_path, hidden_size)\n        tabpfn_r2 = INSPECT_tabpfn.calculate_val_r2score(DO_tabpfn, TVM_tabpfn, model_tabpfn, data=\"test\")\n        all_results['TabPFN Constrained']['r2'].append(tabpfn_r2)\n        print(f\"  TabPFN Constrained R2: {tabpfn_r2:.4f}\")\n        \n    except Exception as e:\n        print(f\"  TabPFN Constrained FAILED: {e}\")\n        all_results['TabPFN Constrained']['r2'].append(np.nan)\n    \n    # === TabPFN+GGH Hybrid ===\n    print(\"\\nRunning TabPFN+GGH Hybrid...\")\n    set_to_deterministic(r_state)\n    DO_hybrid = DataOperator(data_path, inpt_vars, target_vars, miss_vars, hypothesis, \n                             partial_perc, r_state, device=DEVICE)\n    \n    try:\n        hybrid_imputer = TabPFNGGHHybridImputer(\n            HYPOTHESIS_VALUES, r_state,\n            brief_epochs=HYBRID_BRIEF_EPOCHS,\n            confidence_threshold=HYBRID_CONFIDENCE_THRESHOLD,\n            lr=HYBRID_LR\n        )\n        \n        matrix_hybrid = hybrid_imputer.fit_transform(DO_hybrid, batch_size)\n        imput_input_hybrid = matrix_hybrid[:, :-1]\n        \n        AM_hybrid = AlgoModulators(DO_hybrid, lr=lr)\n        dataloader_hybrid = DO_hybrid.prep_dataloader(\"use imputation\", batch_size, imputed_input=imput_input_hybrid)\n        \n        set_to_deterministic(r_state)\n        model_hybrid = initialize_model(DO_hybrid, dataloader_hybrid, hidden_size, r_state, dropout=dropout)\n        TVM_hybrid = TrainValidationManager(\"use imputation\", BENCHMARK_EPOCHS, dataloader_hybrid, \n                                            batch_size, r_state, results_path, \n                                            imput_method=\"TabPFN+GGH Hybrid\", final_analysis=False)\n        TVM_hybrid.train_model(DO_hybrid, AM_hybrid, model_hybrid, final_analysis=False)\n        \n        INSPECT_hybrid = Inspector(results_path, hidden_size)\n        hybrid_r2 = INSPECT_hybrid.calculate_val_r2score(DO_hybrid, TVM_hybrid, model_hybrid, data=\"test\")\n        all_results['TabPFN+GGH Hybrid']['r2'].append(hybrid_r2)\n        all_results['TabPFN+GGH Hybrid']['refined'].append(hybrid_imputer.stats['refined'])\n        all_results['TabPFN+GGH Hybrid']['changed'].append(hybrid_imputer.stats['changed'])\n        print(f\"  TabPFN+GGH Hybrid R2: {hybrid_r2:.4f}\")\n        \n    except Exception as e:\n        print(f\"  TabPFN+GGH Hybrid FAILED: {e}\")\n        import traceback\n        traceback.print_exc()\n        all_results['TabPFN+GGH Hybrid']['r2'].append(np.nan)\n        all_results['TabPFN+GGH Hybrid']['refined'].append(0)\n        all_results['TabPFN+GGH Hybrid']['changed'].append(0)\n    \n    # Print run summary\n    print(f\"\\n>>> Run {run_idx + 1} Summary:\")\n    print(f\"    Full Info: {all_results['Full Info']['r2'][-1]:.4f}\")\n    print(f\"    Partial: {all_results['Partial']['r2'][-1]:.4f}\")\n    if not np.isnan(all_results['TabPFN Constrained']['r2'][-1]):\n        print(f\"    TabPFN Constrained: {all_results['TabPFN Constrained']['r2'][-1]:.4f}\")\n    if not np.isnan(all_results['TabPFN+GGH Hybrid']['r2'][-1]):\n        print(f\"    TabPFN+GGH Hybrid: {all_results['TabPFN+GGH Hybrid']['r2'][-1]:.4f}\")\n        if 'TabPFN Constrained' in all_results and not np.isnan(all_results['TabPFN Constrained']['r2'][-1]):\n            improvement = all_results['TabPFN+GGH Hybrid']['r2'][-1] - all_results['TabPFN Constrained']['r2'][-1]\n            print(f\"    >>> Hybrid improvement over TabPFN: {improvement:+.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK SUMMARY: TabPFN vs TabPFN+GGH Hybrid\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for method, data in all_results.items():\n",
    "    r2_vals = [v for v in data['r2'] if not np.isnan(v)]\n",
    "    if r2_vals:\n",
    "        summary_data.append({\n",
    "            'Method': method,\n",
    "            'R2 Mean': np.mean(r2_vals),\n",
    "            'R2 Std': np.std(r2_vals),\n",
    "            'N Runs': len(r2_vals)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Statistical tests\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# TabPFN+GGH Hybrid vs TabPFN Constrained\n",
    "hybrid_r2 = np.array(all_results['TabPFN+GGH Hybrid']['r2'])\n",
    "tabpfn_r2 = np.array(all_results['TabPFN Constrained']['r2'])\n",
    "valid_mask = ~(np.isnan(hybrid_r2) | np.isnan(tabpfn_r2))\n",
    "\n",
    "if valid_mask.sum() >= 2:\n",
    "    t_stat, p_val = stats.ttest_rel(hybrid_r2[valid_mask], tabpfn_r2[valid_mask])\n",
    "    diff = np.mean(hybrid_r2[valid_mask]) - np.mean(tabpfn_r2[valid_mask])\n",
    "    sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "    \n",
    "    print(f\"\\nTabPFN+GGH Hybrid vs TabPFN Constrained:\")\n",
    "    print(f\"  Hybrid R2: {np.mean(hybrid_r2[valid_mask]):.4f}\")\n",
    "    print(f\"  TabPFN R2: {np.mean(tabpfn_r2[valid_mask]):.4f}\")\n",
    "    print(f\"  Difference: {diff:+.4f}\")\n",
    "    print(f\"  t={t_stat:.3f}, p={p_val:.6f} {sig}\")\n",
    "    \n",
    "    if diff > 0:\n",
    "        print(f\"  >>> Hybrid OUTPERFORMS TabPFN\")\n",
    "    else:\n",
    "        print(f\"  >>> TabPFN outperforms Hybrid\")\n",
    "\n",
    "# Hybrid refinement stats\n",
    "if all_results['TabPFN+GGH Hybrid']['refined']:\n",
    "    avg_refined = np.mean([r for r in all_results['TabPFN+GGH Hybrid']['refined'] if r > 0])\n",
    "    avg_changed = np.mean([c for c in all_results['TabPFN+GGH Hybrid']['changed'] if c > 0])\n",
    "    print(f\"\\nHybrid Refinement Stats:\")\n",
    "    print(f\"  Avg samples refined: {avg_refined:.1f}\")\n",
    "    print(f\"  Avg samples changed: {avg_changed:.1f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: R2 comparison\n",
    "methods = list(all_results.keys())\n",
    "r2_means = [np.nanmean(all_results[m]['r2']) for m in methods]\n",
    "r2_stds = [np.nanstd(all_results[m]['r2']) for m in methods]\n",
    "colors = ['#2ecc71', '#95a5a6', '#9b59b6', '#3498db']\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(range(len(methods)), r2_means, yerr=r2_stds, capsize=5, \n",
    "               color=colors[:len(methods)], edgecolor='black', linewidth=1.2)\n",
    "ax1.set_xlabel('Method', fontsize=12)\n",
    "ax1.set_ylabel('Test R2 Score', fontsize=12)\n",
    "ax1.set_title('Test R2 by Method', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(range(len(methods)))\n",
    "ax1.set_xticklabels(methods, rotation=30, ha='right', fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, r2_means):\n",
    "    if not np.isnan(val):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.3f}', \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Per-run comparison (Hybrid vs TabPFN)\n",
    "ax2 = axes[1]\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "tabpfn_vals = [v if not np.isnan(v) else 0 for v in all_results['TabPFN Constrained']['r2']]\n",
    "hybrid_vals = [v if not np.isnan(v) else 0 for v in all_results['TabPFN+GGH Hybrid']['r2']]\n",
    "\n",
    "ax2.bar(x - width/2, tabpfn_vals, width, label='TabPFN Constrained', color='#9b59b6', alpha=0.7)\n",
    "ax2.bar(x + width/2, hybrid_vals, width, label='TabPFN+GGH Hybrid', color='#3498db', alpha=0.7)\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Test R2')\n",
    "ax2.set_title('Per-Run Comparison: TabPFN vs Hybrid')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/tabpfn_ggh_hybrid_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final conclusion\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "full_mean = np.nanmean(all_results['Full Info']['r2'])\n",
    "partial_mean = np.nanmean(all_results['Partial']['r2'])\n",
    "tabpfn_mean = np.nanmean(all_results['TabPFN Constrained']['r2'])\n",
    "hybrid_mean = np.nanmean(all_results['TabPFN+GGH Hybrid']['r2'])\n",
    "\n",
    "print(f\"\\nFull Info (Oracle): {full_mean:.4f}\")\n",
    "print(f\"TabPFN+GGH Hybrid: {hybrid_mean:.4f}\")\n",
    "print(f\"TabPFN Constrained: {tabpfn_mean:.4f}\")\n",
    "print(f\"Partial (Baseline): {partial_mean:.4f}\")\n",
    "\n",
    "if not np.isnan(hybrid_mean) and not np.isnan(tabpfn_mean):\n",
    "    improvement = hybrid_mean - tabpfn_mean\n",
    "    print(f\"\\n>>> Hybrid improvement over TabPFN: {improvement:+.4f}\")\n",
    "    \n",
    "    gap_to_full = full_mean - partial_mean\n",
    "    if gap_to_full > 0:\n",
    "        hybrid_closes = (hybrid_mean - partial_mean) / gap_to_full * 100\n",
    "        tabpfn_closes = (tabpfn_mean - partial_mean) / gap_to_full * 100\n",
    "        print(f\">>> Hybrid closes {hybrid_closes:.1f}% of gap to Full Info\")\n",
    "        print(f\">>> TabPFN closes {tabpfn_closes:.1f}% of gap to Full Info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf9a55-bbcf-4bb4-b42d-2d39826ca711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}