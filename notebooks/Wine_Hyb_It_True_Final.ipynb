{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Wine GGH Expansion Benchmark (Cleaned)\n",
    "\n",
    "This notebook contains only the necessary code to run the GGH expansion benchmark.\n",
    "\n",
    "**Results**: Partial R2 ≈ 0.18, GGH R2 ≈ 0.235\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "sys.path.insert(0, '../GGH')\n",
    "\n",
    "from GGH.data_ops import DataOperator\n",
    "from GGH.selection_algorithms import AlgoModulators, compute_individual_grads_nothread\n",
    "from GGH.models import initialize_model, load_model\n",
    "from GGH.train_val_loop import TrainValidationManager\n",
    "from GGH.inspector import Inspector, visualize_train_val_error, selection_histograms\n",
    "from GGH.custom_optimizer import CustomAdam\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_to_deterministic(rand_state):\n",
    "    import random\n",
    "    random.seed(rand_state)\n",
    "    np.random.seed(rand_state)\n",
    "    torch.manual_seed(rand_state)\n",
    "    torch.set_num_threads(1)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ../saved_results/Red Wine Hybrid Iterative\n",
      "Iteration 1: 60 epochs (track last 5)\n",
      "Iteration 2: 30 epochs on top 30% + weighted partial\n",
      "Iteration 3: Score remaining 70% with biased model\n",
      "Hypothesis values: [9.4, 10.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# Data configuration\n",
    "data_path = '../data/wine/red_wine.csv'\n",
    "results_path = \"../saved_results/Red Wine Hybrid Iterative\"\n",
    "inpt_vars = ['volatile acidity', 'total sulfur dioxide', 'citric acid'] \n",
    "target_vars = ['quality']\n",
    "miss_vars = ['alcohol']\n",
    "\n",
    "# Hypothesis values (3-class)\n",
    "hypothesis = [[9.4, 10.5, 12.0]]\n",
    "\n",
    "# Model parameters\n",
    "hidden_size = 32\n",
    "output_size = len(target_vars)\n",
    "hyp_per_sample = len(hypothesis[0])\n",
    "batch_size = 100 * hyp_per_sample\n",
    "\n",
    "# Training parameters\n",
    "partial_perc = 0.025  # 2.5% complete data\n",
    "rand_state = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Iteration 1 parameters\n",
    "iter1_epochs = 60\n",
    "iter1_analysis_epochs = 5  # Track last 5 epochs\n",
    "\n",
    "# Iteration 2 parameters\n",
    "iter2_epochs = 30  # Same training duration\n",
    "top_percentile = 30  # Use top 30% from Iteration 1\n",
    "partial_target_ratio = 0.25  # Partial should be ~25% of effective training\n",
    "\n",
    "# Iteration 3 parameters\n",
    "iter3_analysis_epochs = 5  # Track last 5 epochs for remaining data\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "for folder in ['iteration1', 'iteration2', 'iteration3']:\n",
    "    os.makedirs(f'{results_path}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {results_path}\")\n",
    "print(f\"Iteration 1: {iter1_epochs} epochs (track last {iter1_analysis_epochs})\")\n",
    "print(f\"Iteration 2: {iter2_epochs} epochs on top {top_percentile}% + weighted partial\")\n",
    "print(f\"Iteration 3: Score remaining {100-top_percentile}% with biased model\")\n",
    "print(f\"Hypothesis values: {hypothesis[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models_header",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "class HypothesisAmplifyingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that amplifies the impact of hypothesis feature on gradients.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared features (non-hypothesis): small embedding\n",
    "    - Hypothesis feature: separate, larger embedding path\n",
    "    - Concatenate and process through final layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_shared_features, n_hypothesis_features=1, \n",
    "                 shared_hidden=16, hypothesis_hidden=32, final_hidden=32, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared features path (smaller)\n",
    "        self.shared_path = nn.Sequential(\n",
    "            nn.Linear(n_shared_features, shared_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Hypothesis feature path (larger - amplifies its importance)\n",
    "        self.hypothesis_path = nn.Sequential(\n",
    "            nn.Linear(n_hypothesis_features, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hypothesis_hidden, hypothesis_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combined path\n",
    "        combined_size = shared_hidden + hypothesis_hidden\n",
    "        self.final_path = nn.Sequential(\n",
    "            nn.Linear(combined_size, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        self.n_shared = n_shared_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input: shared features vs hypothesis feature\n",
    "        shared_features = x[:, :self.n_shared]\n",
    "        hypothesis_feature = x[:, self.n_shared:]\n",
    "        \n",
    "        # Process separately\n",
    "        shared_emb = self.shared_path(shared_features)\n",
    "        hypothesis_emb = self.hypothesis_path(hypothesis_feature)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = torch.cat([shared_emb, hypothesis_emb], dim=1)\n",
    "        return self.final_path(combined)\n",
    "\n",
    "\n",
    "class StandardModel(nn.Module):\n",
    "    \"\"\"Standard MLP for comparison.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainers_header",
   "metadata": {},
   "source": [
    "## Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "phase1_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnbiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class UnbiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on ALL hypotheses equally (no selection).\n",
    "    Track per-hypothesis losses and gradients in the last N epochs.\n",
    "    Used for Iteration 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        # Tracking data\n",
    "        self.loss_history = {}  # global_id -> list of losses per epoch\n",
    "        self.gradient_history = {}  # global_id -> list of gradient vectors\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on ALL hypotheses equally.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Standard forward pass on ALL hypotheses\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            # Compute loss (mean over all hypotheses - no selection)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            batch_loss = individual_losses.mean()\n",
    "            \n",
    "            # Track per-hypothesis data if in analysis window\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Standard backprop on ALL hypotheses\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient for this hypothesis\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            # Get gradient w.r.t. last layer weights\n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results for each hypothesis.\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"UnbiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "biased_trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiasedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class BiasedTrainer:\n",
    "    \"\"\"\n",
    "    Train on selected hypotheses + weighted partial data.\n",
    "    Used for Iteration 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, selected_gids, partial_gids, partial_weight, lr=0.001, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss(reduction='none')\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        \n",
    "        self.selected_gids = set(selected_gids)  # Top N% from Iteration 1\n",
    "        self.partial_gids = set(partial_gids)    # Partial data (known correct)\n",
    "        self.partial_weight = partial_weight\n",
    "        \n",
    "        # Tracking data for analysis\n",
    "        self.loss_history = {}\n",
    "        self.gradient_history = {}\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch, track_data=False):\n",
    "        \"\"\"Train one epoch on selected + partial data.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, global_ids) in enumerate(dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            predictions = self.model(inputs)\n",
    "            individual_losses = self.criterion(predictions, targets).mean(dim=1)\n",
    "            \n",
    "            # Apply weights: selected gets weight 1, partial gets partial_weight\n",
    "            weights = torch.zeros(len(inputs), device=self.device)\n",
    "            included_indices = []\n",
    "            \n",
    "            for i, gid in enumerate(global_ids):\n",
    "                gid = gid.item()\n",
    "                if gid in self.partial_gids:\n",
    "                    weights[i] = self.partial_weight\n",
    "                    included_indices.append(i)\n",
    "                elif gid in self.selected_gids:\n",
    "                    weights[i] = 1.0\n",
    "                    included_indices.append(i)\n",
    "            \n",
    "            if len(included_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Weighted loss\n",
    "            weighted_loss = (individual_losses * weights).sum() / weights.sum()\n",
    "            \n",
    "            # Track data if requested\n",
    "            if track_data:\n",
    "                self._track_hypothesis_data(inputs, targets, global_ids, individual_losses)\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item() * weights.sum().item()\n",
    "            total_weight += weights.sum().item()\n",
    "        \n",
    "        return total_loss / total_weight if total_weight > 0 else 0\n",
    "    \n",
    "    def _track_hypothesis_data(self, inputs, targets, global_ids, losses):\n",
    "        \"\"\"Track loss and gradient for each hypothesis in the batch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            gid = global_ids[i].item()\n",
    "            \n",
    "            # Track loss\n",
    "            if gid not in self.loss_history:\n",
    "                self.loss_history[gid] = []\n",
    "            self.loss_history[gid].append(losses[i].item())\n",
    "            \n",
    "            # Compute and track gradient\n",
    "            inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "            pred = self.model(inp)\n",
    "            loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "            \n",
    "            params = list(self.model.parameters())\n",
    "            grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "            grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "            \n",
    "            if gid not in self.gradient_history:\n",
    "                self.gradient_history[gid] = []\n",
    "            self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "    def get_hypothesis_analysis(self):\n",
    "        \"\"\"Compile analysis results.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_history:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_history[gid]),\n",
    "                'loss_std': np.std(self.loss_history[gid]),\n",
    "                'loss_trajectory': self.loss_history[gid],\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"BiasedTrainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "remaining_scorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemainingDataScorer defined.\n"
     ]
    }
   ],
   "source": [
    "class RemainingDataScorer:\n",
    "    \"\"\"\n",
    "    Score remaining data (not used in Iteration 2) using a biased model.\n",
    "    Computes both loss and gradient signals.\n",
    "    Used for Iteration 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, DO, model, remaining_sample_indices, device='cpu'):\n",
    "        self.DO = DO\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hyp_per_sample = DO.num_hyp_comb\n",
    "        self.remaining_sample_indices = set(remaining_sample_indices)\n",
    "        \n",
    "        # Storage for scores\n",
    "        self.loss_scores = {}  # gid -> avg_loss\n",
    "        self.gradient_history = {}  # gid -> list of gradients\n",
    "        \n",
    "    def compute_scores(self, dataloader, n_passes=5):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient scores for remaining data.\n",
    "        Run multiple passes to get stable gradient estimates.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        for pass_idx in tqdm(range(n_passes), desc=\"Scoring passes\"):\n",
    "            for inputs, targets, global_ids in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                for i in range(len(inputs)):\n",
    "                    gid = global_ids[i].item()\n",
    "                    sample_idx = gid // self.hyp_per_sample\n",
    "                    \n",
    "                    # Only score remaining samples\n",
    "                    if sample_idx not in self.remaining_sample_indices:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    inp = inputs[i:i+1].clone().requires_grad_(True)\n",
    "                    pred = self.model(inp)\n",
    "                    loss = nn.MSELoss()(pred, targets[i:i+1])\n",
    "                    \n",
    "                    # Store loss\n",
    "                    if gid not in self.loss_scores:\n",
    "                        self.loss_scores[gid] = []\n",
    "                    self.loss_scores[gid].append(loss.item())\n",
    "                    \n",
    "                    # Compute gradient\n",
    "                    params = list(self.model.parameters())\n",
    "                    grad_param = grad(loss, params[-2], retain_graph=False)[0]\n",
    "                    grad_vec = grad_param.flatten().detach().cpu().numpy()\n",
    "                    \n",
    "                    if gid not in self.gradient_history:\n",
    "                        self.gradient_history[gid] = []\n",
    "                    self.gradient_history[gid].append(grad_vec)\n",
    "        \n",
    "        print(f\"Scored {len(self.loss_scores)} hypotheses from {len(self.remaining_sample_indices)} samples\")\n",
    "    \n",
    "    def get_analysis(self):\n",
    "        \"\"\"Get analysis for scored hypotheses.\"\"\"\n",
    "        analysis = {}\n",
    "        for gid in self.loss_scores:\n",
    "            analysis[gid] = {\n",
    "                'avg_loss': np.mean(self.loss_scores[gid]),\n",
    "                'loss_std': np.std(self.loss_scores[gid]),\n",
    "                'avg_gradient': np.mean(self.gradient_history[gid], axis=0) if gid in self.gradient_history else None,\n",
    "                'gradient_magnitude': np.mean([np.linalg.norm(g) for g in self.gradient_history.get(gid, [])]),\n",
    "            }\n",
    "        return analysis\n",
    "\n",
    "print(\"RemainingDataScorer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_header",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adaptive_context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive context utilities loaded.\n"
     ]
    }
   ],
   "source": [
    "def compute_anchor_data(trainer, DO):\n",
    "    \"\"\"\n",
    "    Compute gradient-only anchors AND enriched anchors for each class.\n",
    "    Also computes anchor_similarity to decide which method to use per class.\n",
    "    \"\"\"\n",
    "    analysis = trainer.get_hypothesis_analysis()\n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    input_cols = DO.inpt_vars\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    # Compute all anchors per class\n",
    "    anchor_correct_grad = {}\n",
    "    anchor_incorrect_grad = {}\n",
    "    anchor_correct_enriched = {}\n",
    "    anchor_incorrect_enriched = {}\n",
    "    anchor_similarity_grad = {}\n",
    "    anchor_similarity_enriched = {}\n",
    "    use_enriched = {}\n",
    "    \n",
    "    # For normalization: collect all gradients to get scale\n",
    "    all_grads = [analysis[gid]['avg_gradient'] for gid in analysis \n",
    "                 if analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = float(np.mean([np.linalg.norm(g) for g in all_grads])) if all_grads else 1.0\n",
    "    \n",
    "    # Store normalization params per class\n",
    "    feature_norm_params = {}\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        class_correct_gids = [gid for gid in partial_correct_gids \n",
    "                              if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        class_incorrect_gids = [gid for gid in blacklisted_gids \n",
    "                                if DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id]\n",
    "        \n",
    "        # Collect gradients and features for correct\n",
    "        correct_grads = []\n",
    "        correct_features = []\n",
    "        for gid in class_correct_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                correct_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                correct_features.append(feat)\n",
    "        \n",
    "        # Collect gradients and features for incorrect\n",
    "        incorrect_grads = []\n",
    "        incorrect_features = []\n",
    "        for gid in class_incorrect_gids:\n",
    "            if gid in analysis and analysis[gid]['avg_gradient'] is not None:\n",
    "                incorrect_grads.append(analysis[gid]['avg_gradient'])\n",
    "                feat = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "                incorrect_features.append(feat)\n",
    "        \n",
    "        if not correct_grads or not incorrect_grads:\n",
    "            continue\n",
    "            \n",
    "        # Gradient-only anchors\n",
    "        anchor_correct_grad[class_id] = np.mean(correct_grads, axis=0)\n",
    "        anchor_incorrect_grad[class_id] = np.mean(incorrect_grads, axis=0)\n",
    "        \n",
    "        # Compute gradient-only anchor similarity\n",
    "        sim_grad = float(np.dot(anchor_correct_grad[class_id], anchor_incorrect_grad[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_grad[class_id]) * np.linalg.norm(anchor_incorrect_grad[class_id]) + 1e-8))\n",
    "        anchor_similarity_grad[class_id] = sim_grad\n",
    "        \n",
    "        # Decide: use enriched if gradient anchor_similarity > 0\n",
    "        use_enriched[class_id] = sim_grad > 0\n",
    "        \n",
    "        # Enriched anchors (gradient + normalized features)\n",
    "        correct_grads = np.array(correct_grads, dtype=np.float64)\n",
    "        incorrect_grads = np.array(incorrect_grads, dtype=np.float64)\n",
    "        correct_features = np.array(correct_features, dtype=np.float64)\n",
    "        incorrect_features = np.array(incorrect_features, dtype=np.float64)\n",
    "        \n",
    "        # Normalize features to gradient scale\n",
    "        all_features = np.vstack([correct_features, incorrect_features])\n",
    "        feat_mean = np.mean(all_features, axis=0)\n",
    "        feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "        \n",
    "        feature_norm_params[class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "        \n",
    "        correct_features_norm = (correct_features - feat_mean) / feat_std * grad_scale\n",
    "        incorrect_features_norm = (incorrect_features - feat_mean) / feat_std * grad_scale\n",
    "        \n",
    "        # Enriched = gradient + normalized features\n",
    "        correct_enriched = np.hstack([correct_grads, correct_features_norm])\n",
    "        incorrect_enriched = np.hstack([incorrect_grads, incorrect_features_norm])\n",
    "        \n",
    "        anchor_correct_enriched[class_id] = np.mean(correct_enriched, axis=0)\n",
    "        anchor_incorrect_enriched[class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "        \n",
    "        # Compute enriched anchor similarity\n",
    "        sim_enriched = float(np.dot(anchor_correct_enriched[class_id], anchor_incorrect_enriched[class_id]) / (\n",
    "            np.linalg.norm(anchor_correct_enriched[class_id]) * np.linalg.norm(anchor_incorrect_enriched[class_id]) + 1e-8))\n",
    "        anchor_similarity_enriched[class_id] = sim_enriched\n",
    "    \n",
    "    return {\n",
    "        'anchor_correct_grad': anchor_correct_grad,\n",
    "        'anchor_incorrect_grad': anchor_incorrect_grad,\n",
    "        'anchor_correct_enriched': anchor_correct_enriched,\n",
    "        'anchor_incorrect_enriched': anchor_incorrect_enriched,\n",
    "        'anchor_similarity_grad': anchor_similarity_grad,\n",
    "        'anchor_similarity_enriched': anchor_similarity_enriched,\n",
    "        'use_enriched': use_enriched,\n",
    "        'grad_scale': grad_scale,\n",
    "        'feature_norm_params': feature_norm_params,\n",
    "        'partial_correct_gids': partial_correct_gids,\n",
    "        'blacklisted_gids': blacklisted_gids,\n",
    "        'partial_sample_indices': partial_sample_indices,\n",
    "        'input_cols': input_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_adaptive_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute score using adaptive method:\n",
    "    - Gradient-only for classes with good gradient separation (anchor_sim < 0)\n",
    "    - Enriched (gradient + features) for classes with poor gradient separation (anchor_sim > 0)\n",
    "    \"\"\"\n",
    "    use_enriched = anchor_data['use_enriched'].get(class_id, False)\n",
    "    \n",
    "    if use_enriched:\n",
    "        # Use enriched vectors\n",
    "        norm_params = anchor_data['feature_norm_params'].get(class_id)\n",
    "        if norm_params:\n",
    "            features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "        else:\n",
    "            features_norm = features\n",
    "        enriched = np.concatenate([gradient, features_norm])\n",
    "        \n",
    "        anchor_c = anchor_data['anchor_correct_enriched'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_enriched'].get(class_id)\n",
    "    else:\n",
    "        # Use gradient-only\n",
    "        enriched = gradient\n",
    "        anchor_c = anchor_data['anchor_correct_grad'].get(class_id)\n",
    "        anchor_i = anchor_data['anchor_incorrect_grad'].get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    \n",
    "    if anchor_i is not None:\n",
    "        sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8))\n",
    "    else:\n",
    "        sim_i = 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def print_adaptive_method_summary(anchor_data, hyp_per_sample):\n",
    "    \"\"\"Print summary of adaptive method selection per class.\"\"\"\n",
    "    print(\"Per-class method selection:\")\n",
    "    for class_id in range(hyp_per_sample):\n",
    "        use_enr = anchor_data['use_enriched'].get(class_id, False)\n",
    "        sim_grad = anchor_data['anchor_similarity_grad'].get(class_id, None)\n",
    "        sim_enr = anchor_data['anchor_similarity_enriched'].get(class_id, None)\n",
    "        \n",
    "        if use_enr:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (poor) -> ENRICHED (enriched_sim={sim_enr:+.3f})\")\n",
    "        else:\n",
    "            print(f\"  Class {class_id}: grad_sim={sim_grad:+.3f} (good) -> GRADIENT-ONLY\")\n",
    "\n",
    "print(\"Adaptive context utilities loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark_header",
   "metadata": {},
   "source": [
    "## GGH Expansion Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ggh_expansion_enriched",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "GGH Method (Expansion Strategy):\n",
      "  Iter1: 60 epochs unbiased (lr=0.01), last 5 tracked\n",
      "  Iter1: Enriched selection (gradient + features) -> top 30%\n",
      "  Iter2: 30 epochs biased (lr=0.01, pw=2.0) on top 30% + partial\n",
      "  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\n",
      "  Final: Train on expansion selection + partial (pw=2.0)\n",
      "Partial: Train only on partial data (~2.5%)\n",
      "Both: 200 epochs, validation-based epoch selection, same architecture\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "RUN 1/15 (rand_state=42)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 33.9%, class dist: {0: 51, 1: 105, 2: 180}\n",
      "  Iter3 top 20% (of remaining) precision: 63.1%, class dist: {0: 101, 1: 56, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 43.2%, class dist: {0: 152, 1: 161, 2: 180}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 40.4%, class dist: {0: 106, 1: 112, 2: 126}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=15, val_loss=0.0177, test_loss=0.0167, test_mae=0.0949, R2=0.3602\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0176, test_loss=0.0172, test_mae=0.0982, R2=0.3394\n",
      "\n",
      ">>> Improvement: Loss=+0.0005, MAE=+0.0034, R2=+0.0208\n",
      "\n",
      "============================================================\n",
      "RUN 2/15 (rand_state=142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 56.2%, class dist: {0: 120, 1: 17, 2: 199}\n",
      "  Iter3 top 20% (of remaining) precision: 52.2%, class dist: {0: 91, 1: 66, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 55.0%, class dist: {0: 211, 1: 83, 2: 199}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 58.1%, class dist: {0: 147, 1: 58, 2: 139}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=190, val_loss=0.0195, test_loss=0.0208, test_mae=0.1085, R2=0.1471\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=4, val_loss=0.0179, test_loss=0.0194, test_mae=0.1102, R2=0.2048\n",
      "\n",
      ">>> Improvement: Loss=-0.0014, MAE=+0.0018, R2=-0.0576\n",
      "\n",
      "============================================================\n",
      "RUN 3/15 (rand_state=242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 50.9%, class dist: {0: 68, 1: 0, 2: 268}\n",
      "  Iter3 top 20% (of remaining) precision: 36.9%, class dist: {0: 46, 1: 111, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 46.5%, class dist: {0: 114, 1: 111, 2: 268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 45.8%, class dist: {0: 79, 1: 77, 2: 187}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=1, val_loss=0.0201, test_loss=0.0171, test_mae=0.1037, R2=0.2844\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=88, val_loss=0.0196, test_loss=0.0171, test_mae=0.1036, R2=0.2850\n",
      "\n",
      ">>> Improvement: Loss=-0.0000, MAE=-0.0000, R2=-0.0006\n",
      "\n",
      "============================================================\n",
      "RUN 4/15 (rand_state=342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 47.0%, class dist: {0: 85, 1: 183, 2: 68}\n",
      "  Iter3 top 20% (of remaining) precision: 51.0%, class dist: {0: 43, 1: 114, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.3%, class dist: {0: 128, 1: 297, 2: 68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 47.8%, class dist: {0: 89, 1: 207, 2: 47}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=5, val_loss=0.0159, test_loss=0.0165, test_mae=0.0998, R2=0.2928\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=97, val_loss=0.0210, test_loss=0.0187, test_mae=0.1101, R2=0.1972\n",
      "\n",
      ">>> Improvement: Loss=+0.0022, MAE=+0.0103, R2=+0.0956\n",
      "\n",
      "============================================================\n",
      "RUN 5/15 (rand_state=442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 42.0%, class dist: {0: 71, 1: 161, 2: 104}\n",
      "  Iter3 top 20% (of remaining) precision: 63.1%, class dist: {0: 137, 1: 5, 2: 15}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.7%, class dist: {0: 208, 1: 166, 2: 119}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 48.8%, class dist: {0: 145, 1: 116, 2: 83}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=6, val_loss=0.0222, test_loss=0.0178, test_mae=0.1040, R2=0.2865\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=10, val_loss=0.0244, test_loss=0.0203, test_mae=0.1061, R2=0.1838\n",
      "\n",
      ">>> Improvement: Loss=+0.0026, MAE=+0.0021, R2=+0.1028\n",
      "\n",
      "============================================================\n",
      "RUN 6/15 (rand_state=542)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%, class dist: {0: 141, 1: 31, 2: 164}\n",
      "  Iter3 top 20% (of remaining) precision: 70.1%, class dist: {0: 130, 1: 27, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 64.3%, class dist: {0: 271, 1: 58, 2: 164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 63.8%, class dist: {0: 189, 1: 40, 2: 114}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=1, val_loss=0.0157, test_loss=0.0192, test_mae=0.1052, R2=0.1957\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=52, val_loss=0.0174, test_loss=0.0194, test_mae=0.1114, R2=0.1901\n",
      "\n",
      ">>> Improvement: Loss=+0.0001, MAE=+0.0062, R2=+0.0056\n",
      "\n",
      "============================================================\n",
      "RUN 7/15 (rand_state=642)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 40.8%, class dist: {0: 82, 1: 147, 2: 107}\n",
      "  Iter3 top 20% (of remaining) precision: 59.9%, class dist: {0: 71, 1: 58, 2: 28}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 46.9%, class dist: {0: 153, 1: 205, 2: 135}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 46.8%, class dist: {0: 107, 1: 143, 2: 94}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=7, val_loss=0.0204, test_loss=0.0176, test_mae=0.1067, R2=0.2557\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=5, val_loss=0.0233, test_loss=0.0210, test_mae=0.1154, R2=0.1099\n",
      "\n",
      ">>> Improvement: Loss=+0.0034, MAE=+0.0088, R2=+0.1458\n",
      "\n",
      "============================================================\n",
      "RUN 8/15 (rand_state=742)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%, class dist: {0: 121, 1: 31, 2: 184}\n",
      "  Iter3 top 20% (of remaining) precision: 53.5%, class dist: {0: 80, 1: 77, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 54.4%, class dist: {0: 201, 1: 108, 2: 184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 54.5%, class dist: {0: 140, 1: 75, 2: 128}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=18, val_loss=0.0191, test_loss=0.0197, test_mae=0.1068, R2=0.2445\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=41, val_loss=0.0192, test_loss=0.0185, test_mae=0.1102, R2=0.2889\n",
      "\n",
      ">>> Improvement: Loss=-0.0012, MAE=+0.0034, R2=-0.0444\n",
      "\n",
      "============================================================\n",
      "RUN 9/15 (rand_state=842)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 46.7%, class dist: {0: 66, 1: 156, 2: 114}\n",
      "  Iter3 top 20% (of remaining) precision: 65.0%, class dist: {0: 81, 1: 28, 2: 48}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 52.5%, class dist: {0: 147, 1: 184, 2: 162}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 52.2%, class dist: {0: 102, 1: 128, 2: 113}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=12, val_loss=0.0155, test_loss=0.0239, test_mae=0.1131, R2=0.2224\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=14, val_loss=0.0159, test_loss=0.0244, test_mae=0.1217, R2=0.2050\n",
      "\n",
      ">>> Improvement: Loss=+0.0005, MAE=+0.0087, R2=+0.0174\n",
      "\n",
      "============================================================\n",
      "RUN 10/15 (rand_state=942)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 59.5%, class dist: {0: 246, 1: 90, 2: 0}\n",
      "  Iter3 top 20% (of remaining) precision: 31.8%, class dist: {0: 19, 1: 132, 2: 6}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 50.7%, class dist: {0: 265, 1: 222, 2: 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 53.8%, class dist: {0: 185, 1: 155, 2: 4}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=0, val_loss=0.0149, test_loss=0.0225, test_mae=0.1207, R2=0.2174\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=6, val_loss=0.0169, test_loss=0.0240, test_mae=0.1214, R2=0.1681\n",
      "\n",
      ">>> Improvement: Loss=+0.0014, MAE=+0.0008, R2=+0.0493\n",
      "\n",
      "============================================================\n",
      "RUN 11/15 (rand_state=1042)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.2%, class dist: {0: 170, 1: 12, 2: 154}\n",
      "  Iter3 top 20% (of remaining) precision: 35.7%, class dist: {0: 0, 1: 109, 2: 48}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 48.3%, class dist: {0: 170, 1: 121, 2: 202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 50.3%, class dist: {0: 119, 1: 84, 2: 141}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=107, val_loss=0.0201, test_loss=0.0203, test_mae=0.1149, R2=0.1578\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=3, val_loss=0.0224, test_loss=0.0229, test_mae=0.1148, R2=0.0496\n",
      "\n",
      ">>> Improvement: Loss=+0.0026, MAE=-0.0000, R2=+0.1082\n",
      "\n",
      "============================================================\n",
      "RUN 12/15 (rand_state=1142)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 54.8%, class dist: {0: 167, 1: 88, 2: 81}\n",
      "  Iter3 top 20% (of remaining) precision: 53.5%, class dist: {0: 36, 1: 121, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 54.4%, class dist: {0: 203, 1: 209, 2: 81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 59.3%, class dist: {0: 142, 1: 146, 2: 56}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=10, val_loss=0.0186, test_loss=0.0225, test_mae=0.1203, R2=0.0351\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=86, val_loss=0.0199, test_loss=0.0228, test_mae=0.1204, R2=0.0234\n",
      "\n",
      ">>> Improvement: Loss=+0.0003, MAE=+0.0001, R2=+0.0117\n",
      "\n",
      "============================================================\n",
      "RUN 13/15 (rand_state=1242)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 52.4%, class dist: {0: 66, 1: 142, 2: 128}\n",
      "  Iter3 top 20% (of remaining) precision: 54.1%, class dist: {0: 46, 1: 111, 2: 0}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 52.9%, class dist: {0: 112, 1: 253, 2: 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 52.6%, class dist: {0: 78, 1: 177, 2: 89}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=33, val_loss=0.0190, test_loss=0.0179, test_mae=0.1047, R2=0.2739\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=32, val_loss=0.0223, test_loss=0.0196, test_mae=0.1125, R2=0.2037\n",
      "\n",
      ">>> Improvement: Loss=+0.0017, MAE=+0.0078, R2=+0.0702\n",
      "\n",
      "============================================================\n",
      "RUN 14/15 (rand_state=1342)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 44.3%, class dist: {0: 174, 1: 121, 2: 41}\n",
      "  Iter3 top 20% (of remaining) precision: 59.9%, class dist: {0: 71, 1: 25, 2: 61}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 49.3%, class dist: {0: 245, 1: 146, 2: 102}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 344 samples, precision: 52.3%, class dist: {0: 171, 1: 102, 2: 71}\n",
      "  Dynamic partial_weight: 4.10\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=3, val_loss=0.0181, test_loss=0.0160, test_mae=0.1027, R2=0.3021\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=192, val_loss=0.0241, test_loss=0.0240, test_mae=0.1330, R2=-0.0483\n",
      "\n",
      ">>> Improvement: Loss=+0.0080, MAE=+0.0303, R2=+0.3504\n",
      "\n",
      "============================================================\n",
      "RUN 15/15 (rand_state=1442)\n",
      "============================================================\n",
      "Running GGH Expansion selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 84 hypotheses from 28 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 2361 hypotheses from 787 samples\n",
      "  Iter1 top 30% precision: 61.6%, class dist: {0: 211, 1: 125, 2: 0}\n",
      "  Iter3 top 20% (of remaining) precision: 42.7%, class dist: {0: 3, 1: 72, 2: 82}\n",
      "  Combined: 493 samples (336 iter1 + 157 iter3), precision: 55.6%, class dist: {0: 214, 1: 197, 2: 82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring passes: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 1479 hypotheses from 493 samples\n",
      "  Iter4 pruned (bottom 30% per class): 343 samples, precision: 57.7%, class dist: {0: 149, 1: 137, 2: 57}\n",
      "  Dynamic partial_weight: 4.08\n",
      "Training GGH model (200 epochs)...\n",
      "GGH: best_epoch=2, val_loss=0.0182, test_loss=0.0173, test_mae=0.1000, R2=0.3343\n",
      "Training Partial model (200 epochs)...\n",
      "Partial: best_epoch=8, val_loss=0.0175, test_loss=0.0176, test_mae=0.1060, R2=0.3227\n",
      "\n",
      ">>> Improvement: Loss=+0.0003, MAE=+0.0060, R2=+0.0116\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\n",
      "================================================================================\n",
      "\n",
      "Detailed Results:\n",
      "Run   GGH Prec   GGH Loss     Part Loss    Δ Loss     GGH R2     Part R2    Δ R2      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     40.4      % 0.0167       0.0172          +0.0005 0.3602     0.3394        +0.0208\n",
      "2     58.1      % 0.0208       0.0194          -0.0014 0.1471     0.2048        -0.0576\n",
      "3     45.8      % 0.0171       0.0171          -0.0000 0.2844     0.2850        -0.0006\n",
      "4     47.8      % 0.0165       0.0187          +0.0022 0.2928     0.1972        +0.0956\n",
      "5     48.8      % 0.0178       0.0203          +0.0026 0.2865     0.1838        +0.1028\n",
      "6     63.8      % 0.0192       0.0194          +0.0001 0.1957     0.1901        +0.0056\n",
      "7     46.8      % 0.0176       0.0210          +0.0034 0.2557     0.1099        +0.1458\n",
      "8     54.5      % 0.0197       0.0185          -0.0012 0.2445     0.2889        -0.0444\n",
      "9     52.2      % 0.0239       0.0244          +0.0005 0.2224     0.2050        +0.0174\n",
      "10    53.8      % 0.0225       0.0240          +0.0014 0.2174     0.1681        +0.0493\n",
      "11    50.3      % 0.0203       0.0229          +0.0026 0.1578     0.0496        +0.1082\n",
      "12    59.3      % 0.0225       0.0228          +0.0003 0.0351     0.0234        +0.0117\n",
      "13    52.6      % 0.0179       0.0196          +0.0017 0.2739     0.2037        +0.0702\n",
      "14    52.3      % 0.0160       0.0240          +0.0080 0.3021     -0.0483       +0.3504\n",
      "15    57.7      % 0.0173       0.0176          +0.0003 0.3343     0.3227        +0.0116\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "GGH Expansion Precision: 52.3% ± 5.8%\n",
      "\n",
      "Test Loss (MSE):\n",
      "  GGH:     0.0190 ± 0.0024\n",
      "  Partial: 0.0205 ± 0.0025\n",
      "\n",
      "Test R2 Score:\n",
      "  GGH:     0.2407 ± 0.0799\n",
      "  Partial: 0.1815 ± 0.1063\n",
      "\n",
      "Statistical Tests (paired t-test):\n",
      "  Loss: t=-2.391, p=0.0314 *\n",
      "  R2:   t=2.317, p=0.0362 *\n",
      "\n",
      "Win Rate:\n",
      "  GGH wins (Loss): 12/15 (80.0%)\n",
      "  GGH wins (R2):   12/15 (80.0%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "GGH Expansion significantly OUTPERFORMS Partial-only on Loss (p=0.0314)\n",
      "Average loss improvement: 0.0014 MSE\n",
      "GGH Expansion significantly OUTPERFORMS Partial-only on R2 (p=0.0362)\n",
      "Average R2 improvement: 0.0591\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK: GGH Expansion Strategy (Like Wine_Hybrid_Iterative) + Enriched Scoring\n",
    "# =============================================================================\n",
    "# GGH Method:\n",
    "#   1. Unbiased training (60 epochs, last 5 tracked) + Enriched selection (top 30%)\n",
    "#   2. Biased training (30 epochs, lr=0.01) on top 30% + partial\n",
    "#   3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select top from remaining\n",
    "#   4. Final model trained on expansion selection + partial\n",
    "# Partial: Only partial data (~2.5%)\n",
    "# Both use same final model architecture, validation-based epoch selection\n",
    "# =============================================================================\n",
    "from scipy import stats\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# === CONFIGURATION (Matching Wine_Hybrid_Iterative) ===\n",
    "BENCHMARK_N_RUNS = 15\n",
    "BENCHMARK_RAND_STATES = [42 + i * 100 for i in range(15)]\n",
    "BENCHMARK_FINAL_EPOCHS = 200\n",
    "BENCHMARK_LR = 0.01\n",
    "BENCHMARK_PARTIAL_WEIGHT = 2.0  # Final model partial weight\n",
    "\n",
    "# GGH Parameters (Matching Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_EPOCHS = 60              # 60 epochs (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_ANALYSIS_EPOCHS = 5      # Last 5 tracked (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER1_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_TOP_PERCENTILE = 30            # Top 30%\n",
    "GGH_ITER2_EPOCHS = 30              # 30 epochs biased\n",
    "GGH_ITER2_LR = 0.01                # lr=0.01 (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER2_PARTIAL_WEIGHT = 2.0     # partial_weight=2.0\n",
    "GGH_SCORING_PASSES = 5             # 5 passes (like Wine_Hybrid_Iterative)\n",
    "GGH_ITER3_TOP_PERCENTILE = 20      # Top 20% from remaining (unseen) samples\n",
    "GGH_ITER4_PRUNE_PERCENTILE = 30    # Remove bottom 30% from each class\n",
    "GGH_ITER4_EPOCHS = 30              # Epochs to train Iter4 model\n",
    "\n",
    "# Model architecture\n",
    "MODEL_SHARED_HIDDEN = 16\n",
    "MODEL_HYPOTHESIS_HIDDEN = 32\n",
    "MODEL_FINAL_HIDDEN = 32\n",
    "\n",
    "\n",
    "def create_dataloader_with_gids(DO, batch_size=32):\n",
    "    \"\"\"Create dataloader that includes global_ids.\"\"\"\n",
    "    input_cols = DO.inpt_vars + [var + '_hypothesis' for var in DO.miss_vars]\n",
    "    n_samples = len(DO.df_train_hypothesis)\n",
    "    global_ids = torch.arange(n_samples)\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(DO.df_train_hypothesis[input_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(DO.df_train_hypothesis[DO.target_vars].values, dtype=torch.float32),\n",
    "        global_ids\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_with_validation(DO, model, trainer_class, selected_gids, partial_gids, \n",
    "                          partial_weight, lr, n_epochs=200, batch_size=32):\n",
    "    \"\"\"Train model with validation-based epoch selection.\"\"\"\n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size)\n",
    "    \n",
    "    trainer = trainer_class(DO, model, selected_gids=selected_gids, \n",
    "                           partial_gids=partial_gids, partial_weight=partial_weight, lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainer.train_epoch(dataloader, epoch, track_data=False)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs, val_targets = DO.get_validation_tensors(use_info=\"full info\")\n",
    "            val_preds = model(val_inputs)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_preds, val_targets).item()\n",
    "        model.train()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_on_test(DO, model):\n",
    "    \"\"\"Evaluate model on test set. Returns loss, MAE, and R2 score.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_inputs, test_targets = DO.get_test_tensors(use_info=\"full info\")\n",
    "        test_preds = model(test_inputs)\n",
    "        test_loss = torch.nn.functional.mse_loss(test_preds, test_targets).item()\n",
    "        test_mae = torch.nn.functional.l1_loss(test_preds, test_targets).item()\n",
    "        \n",
    "        ss_res = torch.sum((test_targets - test_preds) ** 2).item()\n",
    "        ss_tot = torch.sum((test_targets - test_targets.mean()) ** 2).item()\n",
    "        r2_score = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "    return test_loss, test_mae, r2_score\n",
    "\n",
    "\n",
    "def compute_enriched_score(gradient, features, class_id, anchor_data):\n",
    "    \"\"\"Compute enriched score (gradient + normalized features).\"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    enriched = np.concatenate([gradient, features_norm])\n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        anchor_c = anchor_data.get('anchor_correct_grad', {}).get(class_id)\n",
    "        anchor_i = anchor_data.get('anchor_incorrect_grad', {}).get(class_id)\n",
    "        enriched = gradient\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data):\n",
    "    \"\"\"\n",
    "    Compute enriched score with loss included (gradient + features + loss).\n",
    "    For expansion scoring on remaining samples.\n",
    "    \"\"\"\n",
    "    norm_params = anchor_data.get('feature_norm_params', {}).get(class_id)\n",
    "    loss_params = anchor_data.get('loss_norm_params', {}).get(class_id)\n",
    "    grad_scale = anchor_data.get('grad_scale', 1.0)\n",
    "    \n",
    "    # Normalize features\n",
    "    if norm_params:\n",
    "        features_norm = (features - norm_params['mean']) / norm_params['std'] * norm_params['scale']\n",
    "    else:\n",
    "        features_norm = features * grad_scale / (np.linalg.norm(features) + 1e-8)\n",
    "    \n",
    "    # Normalize loss (negated: lower loss = higher value)\n",
    "    if loss_params:\n",
    "        loss_norm = -((loss - loss_params['mean']) / loss_params['std']) * loss_params['scale']\n",
    "    else:\n",
    "        loss_norm = -loss * grad_scale\n",
    "    \n",
    "    # Enriched = gradient + features + loss\n",
    "    enriched = np.concatenate([gradient, features_norm, [loss_norm]])\n",
    "    \n",
    "    anchor_c = anchor_data.get('anchor_correct_enriched', {}).get(class_id)\n",
    "    anchor_i = anchor_data.get('anchor_incorrect_enriched', {}).get(class_id)\n",
    "    \n",
    "    if anchor_c is None:\n",
    "        return 0.0\n",
    "    \n",
    "    sim_c = float(np.dot(enriched, anchor_c) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_c) + 1e-8))\n",
    "    sim_i = float(np.dot(enriched, anchor_i) / (np.linalg.norm(enriched) * np.linalg.norm(anchor_i) + 1e-8)) if anchor_i is not None else 0.0\n",
    "    \n",
    "    return sim_c - sim_i\n",
    "\n",
    "\n",
    "def prune_class_balanced(scored_samples, prune_percentile, DO):\n",
    "    \"\"\"\n",
    "    Prune bottom X% from EACH class separately.\n",
    "    Returns pruned list maintaining original class distribution.\n",
    "    \"\"\"\n",
    "    # Group by class\n",
    "    by_class = {}\n",
    "    for s in scored_samples:\n",
    "        class_id = DO.df_train_hypothesis.iloc[s['gid']]['hyp_class_id']\n",
    "        if class_id not in by_class:\n",
    "            by_class[class_id] = []\n",
    "        by_class[class_id].append(s)\n",
    "\n",
    "    # Sort each class by score and keep top (100 - prune_percentile)%\n",
    "    kept = []\n",
    "    for class_id, samples in by_class.items():\n",
    "        samples.sort(key=lambda x: x['score'], reverse=True)\n",
    "        n_keep = int(len(samples) * (100 - prune_percentile) / 100)\n",
    "        n_keep = max(1, n_keep)  # Keep at least 1 per class\n",
    "        kept.extend(samples[:n_keep])\n",
    "\n",
    "    return kept\n",
    "\n",
    "\n",
    "def run_ggh_expansion(DO, rand_state):\n",
    "    \"\"\"\n",
    "    Run GGH method with EXPANSION strategy (like Wine_Hybrid_Iterative) but with Enriched scoring.\n",
    "    \n",
    "    1. Unbiased training (60 epochs, track last 5) + Enriched selection -> top 30%\n",
    "    2. Biased training on top 30% + partial (30 epochs)\n",
    "    3. EXPANSION: Score REMAINING 70% with Enriched+Loss, select best from remaining\n",
    "    \n",
    "    Returns selected_gids, precision, partial_correct_gids\n",
    "    \"\"\"\n",
    "    set_to_deterministic(rand_state)\n",
    "    \n",
    "    hyp_per_sample = DO.num_hyp_comb\n",
    "    n_samples = len(DO.df_train_hypothesis) // hyp_per_sample\n",
    "    n_shared = len(DO.inpt_vars)\n",
    "    n_hyp = len(DO.miss_vars)\n",
    "    out_size = len(DO.target_vars)\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_correct_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    blacklisted_gids = set(DO.df_train_hypothesis[\n",
    "        (DO.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO.df_train_hypothesis['correct_hypothesis'] == False)\n",
    "    ].index.tolist())\n",
    "    partial_sample_indices = set(gid // hyp_per_sample for gid in partial_correct_gids)\n",
    "    \n",
    "    dataloader = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    \n",
    "    # === ITERATION 1: Unbiased training (60 epochs, track last 5) + Enriched selection ===\n",
    "    model_unbiased = HypothesisAmplifyingModel(n_shared, n_hyp, \n",
    "                                               MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN, \n",
    "                                               MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_unbiased = UnbiasedTrainer(DO, model_unbiased, lr=GGH_ITER1_LR)\n",
    "    \n",
    "    # Train without tracking for first (60-5)=55 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # Track last 5 epochs\n",
    "    for epoch in range(GGH_ITER1_EPOCHS - GGH_ITER1_ANALYSIS_EPOCHS, GGH_ITER1_EPOCHS):\n",
    "        trainer_unbiased.train_epoch(dataloader, epoch, track_data=True)\n",
    "    \n",
    "    # Compute anchors for ENRICHED selection\n",
    "    anchor_data = compute_anchor_data(trainer_unbiased, DO)\n",
    "    analysis = trainer_unbiased.get_hypothesis_analysis()\n",
    "    input_cols = anchor_data['input_cols']\n",
    "    \n",
    "    # Select using ENRICHED method (gradient + features)\n",
    "    all_selections = []\n",
    "    for sample_idx in range(n_samples):\n",
    "        if sample_idx in partial_sample_indices:\n",
    "            continue\n",
    "        \n",
    "        start = sample_idx * hyp_per_sample\n",
    "        best_score, best_is_correct, best_gid, best_class = -np.inf, False, None, None\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start + hyp_idx\n",
    "            if gid in blacklisted_gids or gid not in analysis or analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = analysis[gid]['avg_gradient']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, input_cols].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED scoring (gradient + features)\n",
    "            score = compute_enriched_score(gradient, features, class_id, anchor_data)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "                best_gid = gid\n",
    "                best_class = class_id\n",
    "        \n",
    "        if best_score > -np.inf:\n",
    "            all_selections.append((best_score, best_is_correct, sample_idx, best_gid, best_class))\n",
    "    \n",
    "    # Sort and get top 30%\n",
    "    all_selections.sort(key=lambda x: x[0], reverse=True)\n",
    "    n_top = int(len(all_selections) * GGH_TOP_PERCENTILE / 100)\n",
    "    top_selections = all_selections[:n_top]\n",
    "    top_sample_indices = set(s[2] for s in top_selections)\n",
    "    sample_to_gid = {s[2]: s[3] for s in all_selections}\n",
    "    \n",
    "    # Iter1 precision\n",
    "    iter1_correct = sum(1 for s in top_selections if s[1])\n",
    "    iter1_precision = iter1_correct / len(top_selections) * 100 if top_selections else 0\n",
    "    \n",
    "    # Track iter1 class distribution\n",
    "    iter1_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_selections:\n",
    "        gid = s[3]\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        iter1_class_counts[class_id] += 1\n",
    "    \n",
    "    # === ITERATION 2: Biased training on top 30% + partial ===\n",
    "    set_to_deterministic(rand_state + 100)\n",
    "    model_biased = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                             MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                             MODEL_FINAL_HIDDEN, out_size)\n",
    "    \n",
    "    top_gids_set = set(sample_to_gid[idx] for idx in top_sample_indices if idx in sample_to_gid)\n",
    "    trainer_biased = BiasedTrainer(DO, model_biased, selected_gids=top_gids_set,\n",
    "                                   partial_gids=partial_correct_gids, \n",
    "                                   partial_weight=GGH_ITER2_PARTIAL_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    for epoch in range(GGH_ITER2_EPOCHS):\n",
    "        trainer_biased.train_epoch(dataloader, epoch, track_data=False)\n",
    "    \n",
    "    # === ITERATION 3: EXPANSION - Score REMAINING 70% with Enriched+Loss ===\n",
    "    all_sample_indices = set(range(n_samples))\n",
    "    remaining_sample_indices = all_sample_indices - top_sample_indices - partial_sample_indices\n",
    "    \n",
    "    # First, score partial data to build anchors with loss\n",
    "    partial_scorer = RemainingDataScorer(DO, model_biased, partial_sample_indices)\n",
    "    partial_scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    partial_analysis = partial_scorer.get_analysis()\n",
    "    \n",
    "    # Build anchors with loss from partial data\n",
    "    anchor_data_biased = {\n",
    "        'anchor_correct_grad': {},\n",
    "        'anchor_incorrect_grad': {},\n",
    "        'anchor_correct_enriched': {},  # Will include loss\n",
    "        'anchor_incorrect_enriched': {},\n",
    "        'feature_norm_params': {},\n",
    "        'loss_norm_params': {},\n",
    "    }\n",
    "    \n",
    "    # Compute normalization parameters\n",
    "    all_grads = [partial_analysis[gid]['avg_gradient'] for gid in partial_correct_gids | blacklisted_gids\n",
    "                 if gid in partial_analysis and partial_analysis[gid]['avg_gradient'] is not None]\n",
    "    grad_scale = np.mean([np.linalg.norm(g) for g in all_grads]) if all_grads else 1.0\n",
    "    anchor_data_biased['grad_scale'] = grad_scale\n",
    "    \n",
    "    inpt_vars_list = DO.inpt_vars\n",
    "    \n",
    "    for class_id in range(hyp_per_sample):\n",
    "        correct_grads, incorrect_grads = [], []\n",
    "        correct_features, incorrect_features = [], []\n",
    "        correct_losses, incorrect_losses = [], []\n",
    "        \n",
    "        for gid in partial_correct_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    correct_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    correct_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    correct_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        for gid in blacklisted_gids:\n",
    "            if gid in partial_analysis and DO.df_train_hypothesis.iloc[gid]['hyp_class_id'] == class_id:\n",
    "                if partial_analysis[gid]['avg_gradient'] is not None:\n",
    "                    incorrect_grads.append(partial_analysis[gid]['avg_gradient'])\n",
    "                    incorrect_features.append(DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64))\n",
    "                    incorrect_losses.append(partial_analysis[gid]['avg_loss'])\n",
    "        \n",
    "        if correct_grads and incorrect_grads:\n",
    "            # Gradient anchors\n",
    "            anchor_data_biased['anchor_correct_grad'][class_id] = np.mean(correct_grads, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_grad'][class_id] = np.mean(incorrect_grads, axis=0)\n",
    "            \n",
    "            # Feature normalization\n",
    "            all_features = correct_features + incorrect_features\n",
    "            feat_mean = np.mean(all_features, axis=0)\n",
    "            feat_std = np.std(all_features, axis=0) + 1e-8\n",
    "            anchor_data_biased['feature_norm_params'][class_id] = {'mean': feat_mean, 'std': feat_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in correct_features]\n",
    "            incorrect_features_norm = [(f - feat_mean) / feat_std * grad_scale for f in incorrect_features]\n",
    "            \n",
    "            # Loss normalization\n",
    "            all_losses = correct_losses + incorrect_losses\n",
    "            loss_mean = np.mean(all_losses)\n",
    "            loss_std = np.std(all_losses) + 1e-8\n",
    "            anchor_data_biased['loss_norm_params'][class_id] = {'mean': loss_mean, 'std': loss_std, 'scale': grad_scale}\n",
    "            \n",
    "            correct_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in correct_losses]\n",
    "            incorrect_losses_norm = [-(l - loss_mean) / loss_std * grad_scale for l in incorrect_losses]\n",
    "            \n",
    "            # Enriched anchors = gradient + features + loss\n",
    "            correct_enriched = [np.concatenate([g, f, [l]]) \n",
    "                               for g, f, l in zip(correct_grads, correct_features_norm, correct_losses_norm)]\n",
    "            incorrect_enriched = [np.concatenate([g, f, [l]]) \n",
    "                                 for g, f, l in zip(incorrect_grads, incorrect_features_norm, incorrect_losses_norm)]\n",
    "            \n",
    "            anchor_data_biased['anchor_correct_enriched'][class_id] = np.mean(correct_enriched, axis=0)\n",
    "            anchor_data_biased['anchor_incorrect_enriched'][class_id] = np.mean(incorrect_enriched, axis=0)\n",
    "    \n",
    "    # Score REMAINING 70%\n",
    "    scorer = RemainingDataScorer(DO, model_biased, remaining_sample_indices)\n",
    "    scorer.compute_scores(dataloader, n_passes=GGH_SCORING_PASSES)\n",
    "    remaining_analysis = scorer.get_analysis()\n",
    "    \n",
    "    # Score each hypothesis in remaining samples using Enriched+Loss\n",
    "    remaining_scored = []\n",
    "    for sample_idx in remaining_sample_indices:\n",
    "        start_gid = sample_idx * hyp_per_sample\n",
    "        best_score = -np.inf\n",
    "        best_gid = None\n",
    "        best_is_correct = False\n",
    "        \n",
    "        for hyp_idx in range(hyp_per_sample):\n",
    "            gid = start_gid + hyp_idx\n",
    "            if gid in blacklisted_gids:\n",
    "                continue\n",
    "            if gid not in remaining_analysis or remaining_analysis[gid]['avg_gradient'] is None:\n",
    "                continue\n",
    "            \n",
    "            gradient = remaining_analysis[gid]['avg_gradient']\n",
    "            loss = remaining_analysis[gid]['avg_loss']\n",
    "            class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "            features = DO.df_train_hypothesis.loc[gid, inpt_vars_list].values.astype(np.float64)\n",
    "            \n",
    "            # ENRICHED + LOSS scoring\n",
    "            score = compute_enriched_score_with_loss(gradient, features, loss, class_id, anchor_data_biased)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_gid = gid\n",
    "                best_is_correct = DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "        \n",
    "        if best_gid is not None:\n",
    "            remaining_scored.append({\n",
    "                'sample_idx': sample_idx,\n",
    "                'gid': best_gid,\n",
    "                'score': best_score,\n",
    "                'is_correct': best_is_correct\n",
    "            })\n",
    "    \n",
    "    # Sort remaining by score and take top 20% (unseen from Iter1)\n",
    "    remaining_scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "    n_take_from_remaining = int(len(remaining_scored) * GGH_ITER3_TOP_PERCENTILE / 100)\n",
    "    top_remaining = remaining_scored[:n_take_from_remaining]\n",
    "    \n",
    "    # Calculate precision on top of remaining\n",
    "    iter3_correct = sum(1 for s in top_remaining if s['is_correct'])\n",
    "    iter3_precision = iter3_correct / len(top_remaining) * 100 if top_remaining else 0\n",
    "    \n",
    "    # Track iter3 class distribution\n",
    "    iter3_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in top_remaining:\n",
    "        class_id = DO.df_train_hypothesis.iloc[s['gid']]['hyp_class_id']\n",
    "        iter3_class_counts[class_id] += 1\n",
    "    \n",
    "    # COMBINE Iter1 + Iter3 selections\n",
    "    iter1_gids = set(s[3] for s in top_selections)\n",
    "    iter3_gids = set(s['gid'] for s in top_remaining)\n",
    "    selected_gids = iter1_gids | iter3_gids\n",
    "    \n",
    "    # Calculate combined precision\n",
    "    combined_correct = sum(1 for s in top_selections if s[1]) + iter3_correct\n",
    "    combined_precision = combined_correct / len(selected_gids) * 100 if selected_gids else 0\n",
    "    \n",
    "    # Combined class distribution\n",
    "    combined_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for gid in selected_gids:\n",
    "        class_id = DO.df_train_hypothesis.iloc[gid]['hyp_class_id']\n",
    "        combined_class_counts[class_id] += 1\n",
    "    \n",
    "    print(f\"  Iter1 top {GGH_TOP_PERCENTILE}% precision: {iter1_precision:.1f}%, class dist: {iter1_class_counts}\")\n",
    "    print(f\"  Iter3 top {GGH_ITER3_TOP_PERCENTILE}% (of remaining) precision: {iter3_precision:.1f}%, class dist: {iter3_class_counts}\")\n",
    "    print(f\"  Combined: {len(selected_gids)} samples ({len(iter1_gids)} iter1 + {len(iter3_gids)} iter3), precision: {combined_precision:.1f}%, class dist: {combined_class_counts}\")\n",
    "    \n",
    "    # === ITERATION 4: Class-balanced pruning ===\n",
    "    # Train model on combined + partial\n",
    "    set_to_deterministic(rand_state + 150)\n",
    "    model_iter4 = HypothesisAmplifyingModel(n_shared, n_hyp,\n",
    "                                            MODEL_SHARED_HIDDEN, MODEL_HYPOTHESIS_HIDDEN,\n",
    "                                            MODEL_FINAL_HIDDEN, out_size)\n",
    "    trainer_iter4 = BiasedTrainer(DO, model_iter4, selected_gids=selected_gids,\n",
    "                                  partial_gids=partial_correct_gids,\n",
    "                                  partial_weight=GGH_ITER2_PARTIAL_WEIGHT, lr=GGH_ITER2_LR)\n",
    "    \n",
    "    dataloader_iter4 = create_dataloader_with_gids(DO, batch_size=32)\n",
    "    for epoch in range(GGH_ITER4_EPOCHS):\n",
    "        trainer_iter4.train_epoch(dataloader_iter4, epoch, track_data=False)\n",
    "    \n",
    "    # Score combined selection with Iter4 model\n",
    "    combined_sample_indices = set(gid // hyp_per_sample for gid in selected_gids)\n",
    "    combined_scorer = RemainingDataScorer(DO, model_iter4, combined_sample_indices)\n",
    "    combined_scorer.compute_scores(dataloader_iter4, n_passes=GGH_SCORING_PASSES)\n",
    "    combined_analysis = combined_scorer.get_analysis()\n",
    "    \n",
    "    # Build scored list for pruning\n",
    "    combined_scored = []\n",
    "    for gid in selected_gids:\n",
    "        if gid in combined_analysis and combined_analysis[gid]['avg_gradient'] is not None:\n",
    "            combined_scored.append({\n",
    "                'gid': gid,\n",
    "                'score': -combined_analysis[gid]['avg_loss'],  # Lower loss = higher score\n",
    "                'is_correct': DO.df_train_hypothesis.iloc[gid]['correct_hypothesis']\n",
    "            })\n",
    "    \n",
    "    # Class-balanced pruning\n",
    "    pruned = prune_class_balanced(combined_scored, GGH_ITER4_PRUNE_PERCENTILE, DO)\n",
    "    selected_gids = set(s['gid'] for s in pruned)\n",
    "    \n",
    "    # Track pruned class distribution\n",
    "    pruned_class_counts = {c: 0 for c in range(hyp_per_sample)}\n",
    "    for s in pruned:\n",
    "        class_id = DO.df_train_hypothesis.iloc[s['gid']]['hyp_class_id']\n",
    "        pruned_class_counts[class_id] += 1\n",
    "    \n",
    "    pruned_correct = sum(1 for s in pruned if s['is_correct'])\n",
    "    pruned_precision = pruned_correct / len(pruned) * 100 if pruned else 0\n",
    "    \n",
    "    print(f\"  Iter4 pruned (bottom {GGH_ITER4_PRUNE_PERCENTILE}% per class): {len(pruned)} samples, precision: {pruned_precision:.1f}%, class dist: {pruned_class_counts}\")\n",
    "    \n",
    "    return selected_gids, pruned_precision, partial_correct_gids, iter1_class_counts, iter3_class_counts, combined_class_counts, pruned_class_counts\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON LOOP\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(\"=\" * 80)\n",
    "print(\"GGH Method (Expansion Strategy):\")\n",
    "print(f\"  Iter1: {GGH_ITER1_EPOCHS} epochs unbiased (lr={GGH_ITER1_LR}), last {GGH_ITER1_ANALYSIS_EPOCHS} tracked\")\n",
    "print(f\"  Iter1: Enriched selection (gradient + features) -> top {GGH_TOP_PERCENTILE}%\")\n",
    "print(f\"  Iter2: {GGH_ITER2_EPOCHS} epochs biased (lr={GGH_ITER2_LR}, pw={GGH_ITER2_PARTIAL_WEIGHT}) on top 30% + partial\")\n",
    "print(f\"  Iter3: EXPANSION - Score REMAINING 70% with Enriched+Loss, select best\")\n",
    "print(f\"  Final: Train on expansion selection + partial (pw={BENCHMARK_PARTIAL_WEIGHT})\")\n",
    "print(f\"Partial: Train only on partial data (~2.5%)\")\n",
    "print(f\"Both: {BENCHMARK_FINAL_EPOCHS} epochs, validation-based epoch selection, same architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for run_idx, run_rand_state in enumerate(BENCHMARK_RAND_STATES):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUN {run_idx + 1}/{BENCHMARK_N_RUNS} (rand_state={run_rand_state})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup DataOperator\n",
    "    set_to_deterministic(run_rand_state)\n",
    "    DO_run = DataOperator(\n",
    "        data_path, inpt_vars, target_vars, miss_vars, hypothesis,\n",
    "        partial_perc, run_rand_state, device='cpu',\n",
    "        data_split={\"train\": 0.72, \"val\": 0.88}\n",
    "    )\n",
    "    DO_run.problem_type = 'regression'\n",
    "    \n",
    "    # Get partial data\n",
    "    partial_gids = set(DO_run.df_train_hypothesis[\n",
    "        (DO_run.df_train_hypothesis['partial_full_info'] == 1) & \n",
    "        (DO_run.df_train_hypothesis['correct_hypothesis'] == True)\n",
    "    ].index.tolist())\n",
    "    \n",
    "    n_shared = len(DO_run.inpt_vars)\n",
    "    n_hyp = len(DO_run.miss_vars)\n",
    "    out_size = len(DO_run.target_vars)\n",
    "    \n",
    "    # === Run GGH Expansion selection ===\n",
    "    print(\"Running GGH Expansion selection...\")\n",
    "    ggh_selected_gids, ggh_precision, _, iter1_cls, iter3_cls, combined_cls, pruned_cls = run_ggh_expansion(DO_run, run_rand_state)\n",
    "    \n",
    "    # Calculate dynamic partial weight to maintain ~25% partial in training\n",
    "    n_combined = len(ggh_selected_gids)\n",
    "    partial_weight_dynamic = 0.25 * n_combined / (0.75 * len(partial_gids))\n",
    "    print(f\"  Dynamic partial_weight: {partial_weight_dynamic:.2f}\")\n",
    "    \n",
    "    # === Train GGH final model ===\n",
    "    print(f\"Training GGH model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 200)\n",
    "    model_ggh = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN, \n",
    "                                          MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_ggh, ggh_best_epoch, ggh_best_val_loss = train_with_validation(\n",
    "        DO_run, model_ggh, BiasedTrainer, \n",
    "        selected_gids=ggh_selected_gids, partial_gids=partial_gids,\n",
    "        partial_weight=partial_weight_dynamic, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    ggh_test_loss, ggh_test_mae, ggh_test_r2 = evaluate_on_test(DO_run, model_ggh)\n",
    "    print(f\"GGH: best_epoch={ggh_best_epoch}, val_loss={ggh_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={ggh_test_loss:.4f}, test_mae={ggh_test_mae:.4f}, R2={ggh_test_r2:.4f}\")\n",
    "    \n",
    "    # === Train Partial-only model ===\n",
    "    print(f\"Training Partial model ({BENCHMARK_FINAL_EPOCHS} epochs)...\")\n",
    "    set_to_deterministic(run_rand_state + 300)\n",
    "    model_partial = HypothesisAmplifyingModel(n_shared, n_hyp, MODEL_SHARED_HIDDEN,\n",
    "                                              MODEL_HYPOTHESIS_HIDDEN, MODEL_FINAL_HIDDEN, out_size)\n",
    "    model_partial, partial_best_epoch, partial_best_val_loss = train_with_validation(\n",
    "        DO_run, model_partial, BiasedTrainer,\n",
    "        selected_gids=set(), partial_gids=partial_gids,\n",
    "        partial_weight=1.0, lr=BENCHMARK_LR, n_epochs=BENCHMARK_FINAL_EPOCHS\n",
    "    )\n",
    "    partial_test_loss, partial_test_mae, partial_test_r2 = evaluate_on_test(DO_run, model_partial)\n",
    "    print(f\"Partial: best_epoch={partial_best_epoch}, val_loss={partial_best_val_loss:.4f}, \"\n",
    "          f\"test_loss={partial_test_loss:.4f}, test_mae={partial_test_mae:.4f}, R2={partial_test_r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'rand_state': run_rand_state,\n",
    "        'ggh_precision': ggh_precision,\n",
    "        'ggh_test_loss': ggh_test_loss,\n",
    "        'ggh_test_mae': ggh_test_mae,\n",
    "        'ggh_test_r2': ggh_test_r2,\n",
    "        'ggh_best_epoch': ggh_best_epoch,\n",
    "        'partial_test_loss': partial_test_loss,\n",
    "        'partial_test_mae': partial_test_mae,\n",
    "        'partial_test_r2': partial_test_r2,\n",
    "        'partial_best_epoch': partial_best_epoch,\n",
    "        'improvement_loss': partial_test_loss - ggh_test_loss,  # Positive = GGH better\n",
    "        'improvement_mae': partial_test_mae - ggh_test_mae,\n",
    "        'improvement_r2': ggh_test_r2 - partial_test_r2,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n>>> Improvement: Loss={results[-1]['improvement_loss']:+.4f}, \"\n",
    "          f\"MAE={results[-1]['improvement_mae']:+.4f}, R2={results[-1]['improvement_r2']:+.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BENCHMARK RESULTS: GGH Expansion (Enriched) vs Partial-Only\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(f\"{'Run':<5} {'GGH Prec':<10} {'GGH Loss':<12} {'Part Loss':<12} {'Δ Loss':<10} {'GGH R2':<10} {'Part R2':<10} {'Δ R2':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{i+1:<5} {r['ggh_precision']:<10.1f}% {r['ggh_test_loss']:<12.4f} {r['partial_test_loss']:<12.4f} \"\n",
    "          f\"{r['improvement_loss']:+10.4f} {r['ggh_test_r2']:<10.4f} {r['partial_test_r2']:<10.4f} {r['improvement_r2']:+10.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "ggh_losses = [r['ggh_test_loss'] for r in results]\n",
    "partial_losses = [r['partial_test_loss'] for r in results]\n",
    "ggh_r2s = [r['ggh_test_r2'] for r in results]\n",
    "partial_r2s = [r['partial_test_r2'] for r in results]\n",
    "ggh_precisions = [r['ggh_precision'] for r in results]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGGH Expansion Precision: {np.mean(ggh_precisions):.1f}% ± {np.std(ggh_precisions):.1f}%\")\n",
    "print(f\"\\nTest Loss (MSE):\")\n",
    "print(f\"  GGH:     {np.mean(ggh_losses):.4f} ± {np.std(ggh_losses):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_losses):.4f} ± {np.std(partial_losses):.4f}\")\n",
    "print(f\"\\nTest R2 Score:\")\n",
    "print(f\"  GGH:     {np.mean(ggh_r2s):.4f} ± {np.std(ggh_r2s):.4f}\")\n",
    "print(f\"  Partial: {np.mean(partial_r2s):.4f} ± {np.std(partial_r2s):.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "t_stat_loss, p_value_loss = stats.ttest_rel(ggh_losses, partial_losses)\n",
    "t_stat_r2, p_value_r2 = stats.ttest_rel(ggh_r2s, partial_r2s)\n",
    "\n",
    "print(f\"\\nStatistical Tests (paired t-test):\")\n",
    "print(f\"  Loss: t={t_stat_loss:.3f}, p={p_value_loss:.4f} {'*' if p_value_loss < 0.05 else ''}\")\n",
    "print(f\"  R2:   t={t_stat_r2:.3f}, p={p_value_r2:.4f} {'*' if p_value_r2 < 0.05 else ''}\")\n",
    "\n",
    "# Win/Loss count\n",
    "n_ggh_wins_loss = sum(1 for r in results if r['ggh_test_loss'] < r['partial_test_loss'])\n",
    "n_ggh_wins_r2 = sum(1 for r in results if r['ggh_test_r2'] > r['partial_test_r2'])\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  GGH wins (Loss): {n_ggh_wins_loss}/{BENCHMARK_N_RUNS} ({n_ggh_wins_loss/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "print(f\"  GGH wins (R2):   {n_ggh_wins_r2}/{BENCHMARK_N_RUNS} ({n_ggh_wins_r2/BENCHMARK_N_RUNS*100:.1f}%)\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "x = np.arange(BENCHMARK_N_RUNS)\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Test Loss comparison\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, ggh_losses, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax1.bar(x + width/2, partial_losses, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax1.set_xlabel('Run')\n",
    "ax1.set_ylabel('Test Loss (MSE)')\n",
    "ax1.set_title('Test Loss: GGH Expansion vs Partial')\n",
    "ax1.legend()\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 2: R2 comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, ggh_r2s, width, label='GGH Expansion', color='blue', alpha=0.7)\n",
    "ax2.bar(x + width/2, partial_r2s, width, label='Partial', color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Run')\n",
    "ax2.set_ylabel('Test R2')\n",
    "ax2.set_title('Test R2: GGH Expansion vs Partial')\n",
    "ax2.legend()\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(i+1) for i in range(BENCHMARK_N_RUNS)])\n",
    "\n",
    "# Plot 3: GGH Precision across runs\n",
    "ax3 = axes[2]\n",
    "ax3.bar(range(1, BENCHMARK_N_RUNS+1), ggh_precisions, color='green', alpha=0.7)\n",
    "ax3.axhline(y=np.mean(ggh_precisions), color='red', linestyle='--', label=f'Mean: {np.mean(ggh_precisions):.1f}%')\n",
    "ax3.set_xlabel('Run')\n",
    "ax3.set_ylabel('GGH Expansion Precision (%)')\n",
    "ax3.set_title('GGH Hypothesis Precision (from Remaining 70%)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/ggh_expansion_enriched_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "avg_improvement_loss = np.mean([r['improvement_loss'] for r in results])\n",
    "avg_improvement_r2 = np.mean([r['improvement_r2'] for r in results])\n",
    "if avg_improvement_loss > 0 and p_value_loss < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on Loss (p={p_value_loss:.4f})\")\n",
    "    print(f\"Average loss improvement: {avg_improvement_loss:.4f} MSE\")\n",
    "elif avg_improvement_loss < 0 and p_value_loss < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on Loss (p={p_value_loss:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in Loss (p={p_value_loss:.4f})\")\n",
    "\n",
    "if avg_improvement_r2 > 0 and p_value_r2 < 0.05:\n",
    "    print(f\"GGH Expansion significantly OUTPERFORMS Partial-only on R2 (p={p_value_r2:.4f})\")\n",
    "    print(f\"Average R2 improvement: {avg_improvement_r2:.4f}\")\n",
    "elif avg_improvement_r2 < 0 and p_value_r2 < 0.05:\n",
    "    print(f\"Partial-only significantly OUTPERFORMS GGH Expansion on R2 (p={p_value_r2:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant difference in R2 (p={p_value_r2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01483701-5176-4433-9e91-7d159ae81b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-michael_20250605]",
   "language": "python",
   "name": "conda-env-.conda-michael_20250605-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
